## Preparing for the CKA and CKAD Certifications
> Philippe Martin

### Front Matter

The target reader for this book is an application developer or a system administrator having a good knowledge inmicroservice development and deployment.

you have some experience installing and managing Linux servers on virtual machines (VMs).

This book includes 16 chapters. The first three chapters cover the installation of a freshnew cluster, the exploration of its components, and the installation of its CLI. 

how topartition them using namespaces, and how to decorate them using labels and annotations.

[插图]has been working with Kubernetes for three years, first bycreating an operator to deploy video CDNs into the cloud andlater helping companies deploy their applications into Kubernetes.

has edited two reference books about Kubernetes API and kubectl, and is responsible forFrench translation of the Kubernetes Dashboard.

### 1. Creating a Cluster with kubeadm

In this chapter, you will deploy a Kubernetes cluster on virtual machines (VMs) in GoogleCloud.Provisioning Compute ResourcesYou will install a single control plane cluster. For this, you will need one virtual machine forthe controller and several (here two) virtual machines for the workers.

from a localterminal, log in and set the current region, zone, and project 

Create a dedicated Virtual Private Cloud (VPC):

Create a subnet in the kubernetes-cluster VPC:

Create firewall rules for internal communications:$ gcloud compute firewall-rules create \  kubernetes-cluster-allow-internal \  --allow tcp,udp,icmp \  --network kubernetes-cluster \  --source-ranges 10.240.0.0/24,10.244.0.0/16

Create firewall rules for external communications:$ gcloud compute firewall-rules create \  kubernetes-cluster-allow-external \  --allow tcp:22,tcp:6443,icmp \  --network kubernetes-cluster \  --source-ranges 0.0.0.0/0

Reserve a public IP address for the controller:$ gcloud compute addresses create kubernetes-controller \  --region $(gcloud config get-value compute/region)

Create a VM for the controller:$ gcloud compute instances create controller \    --async \    --boot-disk-size 200GB \    --can-ip-forward \    --image-family ubuntu-1804-lts \    --image-project ubuntu-os-cloud \    --machine-type n1-standard-1 \    --private-network-ip 10.240.0.10 \    --scopes compute-rw,storage-ro,service-management,service-control,logging-write, monitoring \    --subnet kubernetes \    --address $PUBLIC_IP

Connect to the host (here the controller):$ gcloud compute ssh controller

Install kubeadm, kubelet, and kubectl on the HostsRepeat these steps for the controller and each worker.

sudo apt-get install -y kubelet=1.18.6-00 kubeadm=1.18.6-00kubectl=1.18.6-00

Initialize the cluster (that should take several minutes):

sudo kubeadm init \  --pod-network-cidr=10.244.0.0/16 \  --ignore-preflight-errors=NumCPU \  --apiserver-cert-extra-sans=$KUBERNETES_PUBLIC_ADDRESSAt the end of the initialization, a message gives you a command to join the workers to thecluster (a command starting with kubeadm join). Please copy this command for later use.

Save the kubeconfig file generated by the installation in your home directory. It will giveyou admin access to the cluster:$ mkdir -p $HOME/.kube$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config$ sudo chown $(id -u):$(id -g) $HOME/.kube/config$ kubectl get nodes

### 2. Control Plane Components

The Kubernetes control plane is composed ofThe API server kube-apiserver, the front end for the Kubernetes control planeThe key-value store etcd, the backing store of all cluster dataThe scheduler kube-scheduler, which selects nodes for new Pods to run onThe Controller Manager kube-controller-manager, which embeds all the controllers,including the Node controller, Replication controller, Endpoints controller, and ServiceAccount and Token controllers.

On each node, the components running arekubelet, which makes sure the Pods affected to the node are running correctly

kube-proxy, which maintains network rules on nodes to satisfy the Service demands

$ systemctl status kubelet[...]$ journalctl -u kubelet

You can find the manifests of the Pods in thefollowing directory of the controller:$ gcloud compute ssh controllerWelcome to controller$ ls /etc/kubernetes/manifests/etcd.yamlkube-apiserver.yamlkube-controller-manager.yamlkube-scheduler.yaml

### 3. Accessing the Cluster

 Accessing the Cluster

In this chapter, you will see how to install kubectl on your machine and how to configure itto access the cluster you installed in Chapter 1.

gcloud compute scp controller:~/.kube/config kubeconfig

Finally, you can switch the current context to cka-admin@kubernetes:$ kubectl config use-context cka-admin@kubernetes

### 4. Kubernetes Resources

Kubernetes works in a declarative way: you create resources with the help of the KubernetesAPI, these objects are stored in the etcd store, and controllers work to ensure that what youdeclared in these objects is correctly deployed in your infrastructure.

Most of the resources are composed of three parts: the metadata, the spec, and the status.The spec is the specification you provide to the cluster. This is what the controllers willexamine to know what to do.

The status represents the current status of the resource in the infrastructure, as observed bycontrollers. This is what you will examine to know how the resource is deployed in theinfrastructure.

The metadata contains other information like the name of the resource, the namespace itbelongs to, labels, annotations, and so on.

Lots of kubectl commands accept a --selector (-l for short) flag which permits to select resources by labels:
$ kubectl get pods -l app=nginx -A


Services, which route traffic to Pods, select the Pods with a label selector:

A Deployment, which is responsible for maintaining alive a given number of Pods, uses alabel selector to find the Pods it is responsible for

Annotations are metadata attached to a resource, generally intended for tools and Kubernetes extensions, but not yet integrated in the spec part. 

### 5. The Workloads

The fundamental goal of Kubernetes is to help you manage your containers. The Pod is the minimal piece deployable in a Kubernetes cluster, containing one or several containers.

By adding --dry-run=client -o yaml to the command, you can see the YAML template you would have to write to create the same Pod:$ kubectl run nginx --image=nginx --dry-run=client -o yaml

•  Containers fields will define and parameterize more precisely each container of the Pod, whether it is a normal container (containers) or an init container (initContainers). 

•  Volumes field (volumes) will define a list of volumes that containers will be able to mount and share.

add hosts in the /etc/hosts files of the containers (hostAliases), fine-tune the /etc/resolv.conf files of the containers (dnsConfig), and define a policy for the DNS configuration (dnsPolicy).

The fields related to container runtime are as follows:

•  Entrypoint fields define the command (command) and arguments (args) of the entrypoint and its working directory (workingDir).

•  Environment variables fields help define the environment variables that will be exported in the container, either directly (env) or by referencing ConfigMap or Secret values (envFrom).

•  Volumes fields define the volumes to mount into the container, whether they are a filesystem volume (volumeMounts) or a raw block volume (volumeDevices).


define probes to check liveness (livenessProbe) and readiness (readinessProbe) of the container.


•  ReplicaSet: Ensures that a specified number of Pod replicas are running at any given time.
•  Deployment: Enables declarative updates for Pods and ReplicaSets.
•  StatefulSet: Manages updates of Pods and ReplicaSets, taking care of stateful resources.


•  Job: Starts Pods and ensures they complete.
•  CronJob: Creates a Job on a time-based schedule.

•  replicas indicates how many replicas of selected Pods you want.
•  selector defines the Pods you want the ReplicaSet controller to manage.
•  template is the template used to create new Pods when insufficient replicas are detected by the controller.
•  minReadySeconds indicates the number of seconds the controller should wait after a Pod starts without failing to consider the Pod is ready.


Note that the terminated Pods are not necessarily Pods that werecreated by the ReplicaSet controller.

Note that to avoid the ReplicaSet controller tocreate Pods in a loop, the specified template must create a Pod selectable by thespecified selector (this is the reason why you must set the same labels in theselector.matchLabels and template.metadata.labels fields).

Changing the replicas field will immediately trigger the creation or termination of Pods.

strategy is the strategy to apply when changing the replicas of the previously andcurrently active ReplicaSets.

The command kubectl rolloutprovides several subcommands to work with Deployments.The subcommand status gives us the status of the Deployment:

We will now update the image of nginx to use the 1.11 revision. One way is to use thekubectl set image command:$ kubectl set image deployment nginx nginx=nginx:1.11

The change-cause is empty by default. It can contain the command used to make therollout either by using the --record flag$ kubectl set image deployment nginx nginx=nginx:1.12 --record

Now let’s roll back the last rollout with the undo subcommand:$ kubectl rollout undo deployment nginx

The simplest strategy is the Recreate strategy: in this case, the old ReplicaSet will bedownsized to zero, and when all Pods of this ReplicaSet are stopped, the new ReplicaSet willbe upsized to the number of requested replicas.

There will be a small downtime, the time the old Pods stop and the new Pods start.

The goal of this strategy is to update from previous to new version without downtime.This strategy will combine the possibility to downsize and upsize ReplicaSets and thepossibility to expose Pods through Services.

 Pods are removed from endpoints of Services when they are not ready to serverequests and are added when they become ready to serve requests.

The readiness of a Pod is determined by the state of the readiness probes declared for itscontainers. If you do not declare readiness probes for your containers, the risk is that thePods are detected ready before they really are and traffic is sent to them while they are stillin their startup phase.

Job ControllerThe Job controller runs one or more Pods in parallel and waits for a specific number of thesePods to terminate successfully.

If, after a while, the Pod terminates with success (this means that all containers exit with aZero status), the job is marked as completed.

But if the Pod exits in error, a new Pod will be restarted until the Pod succeeds or until agiven number of errors occur. The number of retries is determined by the value ofspec.backoffLimit.

You can specify with spec.completions how many Pods you want to succeed and withspec.parallelism the maximum number of Pods to run in parallel.

jobTemplate is the template of the Job you want to run on a time-based schedule.

schedule is the time specification at which to run the Job, in Cron format.

The concurrencyPolicyindicates how to treat a new Job when a previous job is stillrunning. The possible values are Allow to allow several concurrent Jobs, Forbid to skip thenew Job if the previous one is still running, and Replace to first cancel the previous Jobbefore running the new one.

### 6. Configuring Applications

Environment Variables
It is possible to define environment variables for a container either by declaring their values directly or by referencing their values from ConfigMaps, Secrets, or fields of the object created (Deployment and others)

You can instead use the kubectl set env command to add environment variables after you create the Deployment

In Declarative Form
Declaratively, when declaring an environment variable, you can indicate that the values should be extracted from a ConfigMap or Secret, one by one

you can use the optional field:- name: PASS2
  valueFrom:
    secretKeyRef:
        key: pass2
        name: passwords
        optional: true

   envFrom:
        - configMapRef:
            name: vars
        - secretRef:
            name: passwords
        - secretRef:
            name: notfound
            optional: true

Declaratively, it is possible to reference the value of some fields of the Pod:
•  metadata.name

  env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
                fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
                fieldPath: metadata.namespace

Configuration File from ConfigMap
It is possible to mount ConfigMap contents in the container filesystem. Each key/value of the mounted ConfigMap will be a filename and its content in the mount directory.

Finally, the file /etc/nginx/conf.d/nginx.conf in the container will contain the value of the nginx.conf key of the ConfigMap.


Configuration File from Secret
Similarly, it is possible to mount the contents of Secrets. 

volume with files containing Pod values:
•  metadata.name
•  metadata.namespace
•  metadata.uid
•  metadata.labels
•  metadata.annotations

It is possible to mount volumes with files containing information from a mix of ConfigMaps, Secrets, Pod fields, and container resources fields.

### 7. Scaling an Application

Manual ScalingIn declarative form, it is possible to edit the spec of the Deployment to change the value ofthis field:

Auto-scalingThe HorizontalPodAutoscaler resource(commonly called HPA) can be used to scaleDeployments automatically depending on the CPU usage of the current replicas.

And you can examine the events created by the HPA:$ kubectl describe hpa nginx

### 8. Application Self-Healing

First, run a Pod; then examine on which node it has been scheduled:

Let’s put this node in maintenance mode, to see what happens to the Pod:$ kubectl drain worker-0 --force

using Pod controllers ensuresyour Pod is scheduled in another node if one node stops to work.

This time, we can see that a Pod has been recreated in another node of the cluster – ourapp now survives a node eviction.

It is possible to define a liveness probe for each container of a Pod. If the kubelet is not ableto execute the probe successfully a given number of times, the container is considered nothealthy and is restarted into the same Pod.

This probe should be used to detect that the container is not responsive.There are three possibilities for the liveness probe:Make an HTTP request.

Most server applications have an associate CLI application. You can use this CLI toexecute a very simple operation on the server. If the server is not healthy, it is probable itwill not respond to this simple request either.

The main role of thereadiness probe is to indicate if a Pod is ready to serve network requests. The Pod will beadded to the list of backends of matching Services when the readiness probe succeeds.Later, during the container execution, if a readiness probe fails, the Pod will be removedfrom the list of backends of Services. This can be useful to detect that a container is notable to handle more connections (e.g., if it is already treating a lot of connections) and stopsending new ones.

Here, we definea probe that queries the /healthz endpoint. As nginx is not configured bydefault to reply to this path, it will reply with a 404 response code, and the probe will fail.This is not a real case, but that simulates an nginx server that would reply in error to asimple request.

You can see in the Pod events that after three failed probes, the container is restarted:

As this user does not exist, thequery will fail.

TCP Connection Liveness Probe

tcpSocket:        port: 5433

Here, the liveness probe tries to connect to the container on the 5433 port. As postgreslistens on the port 5432, the connection will fail.

If you do not declare limits, each container will still have access to all the resources of thenode; in this case, if some Pods are not using all their requested resources at a given time,some other containers will be able to use them and vice versa.

If a node runs out of an incompressible resource (memory), the associated kubelet candecide to eject one or more Pods, to prevent total starvation of the resource.

### 9. Scheduling Pods

The Pod specs contain a nodeName fieldindicating on which node the Pod is scheduled.

The scheduler perpetually watches for Pods; when it finds a Pod with an empty nodeNamefield, the scheduler determines the best node on which to schedule this Pod and thenmodifies the Pod spec to write the nodeName field with the selected node.

In parallel, the kubelet components, affected to specific nodes, watch the Pods; when a Podmarked with a nodeName matches the node of a kubelet, the Pod is affected to the kubelet,which deploys it to its node.

Using Label Selectors to Schedule Pods on Specific NodesThe Pod spec contains a nodeSelector field, as a map of key-value pairs. When set, thePod is deployable only on nodes having each key-value pair as label.

Adding Labels to NodesThe first step is to add labels to nodes. Consider you have four nodes, two with SSD disksand two with HDD disks. You can label the nodes with the commands:$ kubectl label node worker-0 disk=ssd

spec:      nodeSelector:        disk: ssd

Manual Scheduling

If you create a Pod and specify yourself the nodeName in its spec, the scheduler will neversee it, and the associated kubelet will adopt it immediately. The effect is that the Pod will bescheduled on the specified node, without the help of the scheduler.

DaemonSetsThe DaemonSet Kubernetes resource guarantees that all (or a given subset of) nodes run acopy of a given Pod.The typical use of a DaemonSet is to deploy daemons (storage daemons, logs daemons,monitoring daemons) on every node of a cluster.

Because of this particularity, the Pods created by DaemonSets are not scheduled by theKubernetes scheduler, but by the DaemonSet controller itself.

The spec of a DaemonSet is similar to a Deployment spec, with these differences:The DaemonSet spec does not contain a replicas field, as this quantity is given by thenumber of selected nodes.

By default, the DaemonSet will deploy Pods on every node of the cluster. If you want toselect a subset of nodes only, you can use the nodeSelector field of the Pod spec to selectthe nodes by label, as you would do for a Deployment (see Chapter 9, section “Using labelselectors to schedule Pods on specific nodes”).

here is a DaemonSet that would deploy a hypothetical GPU daemon onnodes labeled with compute=gpu:

Static PodsStatic Podsare directly attached to a kubelet daemon. They are declared in files located onthe host of the node running the kubelet daemon, in a specific directory.You can find the directory in the kubelet configuration file, under the staticPodPath field.

The configuration of a kubelet is accessible via the Kubernetes API, at the following path:/api/v1/nodes/<node-name>/proxy/configz.

To access the API easily, you can run the kubectl proxy command:$ kubectl proxyStarting to serve on 127.0.0.1:8001You now have access to the API on http://127.0.0.1:8001, with the same rights you havewith kubectl.

 "kubeletconfig":  {        "staticPodPath": "/etc/kubernetes/manifests",[...]    }}You can now create a manifest to declare a Pod on this directory, on the worker-0 host:

If you delete the Pod, it will be immediately recreated by kubelet:

Resource RequestsEach node has a maximum capacity of CPU and memory. Each time a Pod is scheduled on anode, the CPU and memory amounts requested by this Pod are removed from the CPU andmemory available on this node.

The kubectl set resources commandis used to set resource requests on an object (herea Deployment):

Examine Scheduler EventsIf you look at the events attached to a deployed Pod, you can see that the Pod has beenscheduled by the scheduler-round-robin scheduler:

### 10. Discovery and Load Balancing

The Service Kubernetes resource is used to make a Pod accessible via the network in areproducible way.

a Service is a frontend for a list of backend Pods. This list of backend Pods isdetermined by the selector field of the Service resource; all the Pods with keys and valuesas labels matching this selector are eligible to be part of the list of backends.

Once the list of Pods eligible to be a backend of a Service is determined by the selectorfield, the readiness of the Pods is also taken into account. The Pod is effectively inserted inthe list of backends when the Pod is in the Ready state.

The Pod is considered ready when all its containers are ready, and a container is ready eitherwhen it does not define a readinessProbe or when its readinessProbe succeeds.

TheEndpoints controller is in charge of creating and deleting endpoints, depending on thereadiness of Pods and selectors of Services.You can see the list of endpoints for a Service (here nginx) with the command$ kubectl get endpoints nginx

Service TypesClusterIPBy default, the Service is created with the ClusterIPtype. With this type, the Service isaccessible from inside the cluster only.

An IP address local to the cluster will be reserved for this Service, and a DNS entry will becreated that points to this address, under the form <name>.<namespace>.svc.cluster.local, in our example webapp.default.svc.cluster.local.

If you examine the resolv.conf file inside a container, you see that the search entryindicates:$ kubectl exec -it box cat /etc/resolv.confnameserver 10.96.0.10search default.svc.cluster.local svc.cluster.local cluster.local

Thanks to this, from inside a Pod, you can access a Service defined in the namespace withits name only (here webapp) or its name.namespace (here webapp.default) or itsname.namespace.svc (here webapp.default.svc) or its complete name(webapp.default.svc.cluster.local).

If you want to access a Service from outside the cloud environment, you can use theLoadBalancer type. In addition to creating a NodePort, it will create an external loadbalancer (if you use a managed Kubernetes cluster like Google GKE, Azure AKS, AmazonEKS, etc.), which routes to the ClusterIP via the NodePort.

ExternalNameThis is a specific type of Service, where the selector fieldis not used and the Serviceredirects instead to an external DNS name, using a DNS CNAME record.

 If you haveseveral applications, each with several access points (at least frontend and API), you willneed to reserve lots of load balancers, which can be very costly.

The selection is done on thehostname and path of the request.You have to install an Ingress controller in your cluster in order to use Ingress resources.

Install nginx Ingress ControllerYou can follow the installation instructions.1 In summary, you have to execute

Note that the ingress-nginx Service is of type NodePort; the Service will be accessible oneach worker of the cluster, on ports 32351 (for HTTP connections) and 31296 (for HTTPSconnections).

We have to add a firewall rule that enables the traffic on these ports on the worker VMs:$ gcloud compute firewall-rules create \  kubernetes-cluster-allow-external-ingress \  --allow tcp:$HTTP_PORT,tcp:$HTTPS_PORT \  --network kubernetes-cluster \  --source-ranges 0.0.0.0/0

If you can see these responses, the Ingress controller is running correctly, and you areredirected to the default backend that returns 404 errors.

With this Ingress configuration, all the requests to the Ingress controller will be routed tothe webapp Service:

 First, generate the certificate, and then create a tlssecret containing the certificate:

### 11. Security

Kubernetes is a secured system: you first need to be authenticated, as a normal user or as aservice account; then, an authorization system validates that you have the rights to performthe requested operations.

AuthenticationKubernetes defines two kinds of users: normal users and service accounts.

For a new user, the first step for the user is to create a certificate signing request (CSR):# Create a private key$ openssl genrsa -out user.key 4096[...]Create the csr.cnf configuration file to generate the CSR:

There are three parts in the kubeconfig file: server information, user information, and contextinformation.The command kubectl config set-cluster is used to write server information.

kubectl --kubeconfig=userconfig get nodes

Service Account AuthenticationIn contrast to normal users, service accounts are managed by the Kubernetes API. Theauthentication is handled by JSON Web Tokens (JWTs).

When a namespace is created, the Service Account controller creates a default serviceaccount in this namespace. This creation in turn leads to the creation of the associatedSecret.

A serviceAccountName field of the Pod spec indicates which service account is attached tothe Pod. By default, if you do not specify any service account name, its value is default.You can set the automountServiceAccountToken field of the Pod spec to false to indicatethat no service account should be used.

The Secret associated with the Pod’s service account is automatically mounted into the Podfilesystem, in a well-known directory. The Kubernetes clients inside the Pod are aware ofthis path and use these credentials to connect to the API server.

Service Account Outside the ClusterNote that the token associated with a service account is also usable from outside thecluster. You can, for example, create a kubeconfig file containing this token and use it fromyour dev machine:

TOKEN=$(kubectl get secrets $SECRET_NAME -o jsonpath='{.data.token}' |base64 -d)$ kubectl --kubeconfig=saconfig config set-credentials $USER_NAME \   --token=$TOKENUser "kubectl" set.

The user has to be granted access to all parts of the request against allpolicies to be authorized for this request.

RBAC: Role-based access controlWebhook: HTTP callback modeNode: Special-purpose module for kubelets

To select the modules to use, you have to specify their names in the --authorization-mode flag of the apiserver service. For a cluster installed with kubeadm, the default valueof the flag is --authorization-mode=Node,RBAC.

The endpoints of resource requests are of the form/api/v1/... for the resources of the core group/apis/<group>/<version>/... for resources of other APIs

Endpoints for namespaced resources are of the form/api/v1/namespaces/<namespace>/<resource>/apis/<group>/<version>/namespaces/<namespace>/<resource>Endpoints for non-namespaced resources are of the form/api/v1/<resource>/apis/<group>/<version>/<resource>

list to retrieve all resource objects within one namespace or across all namespaceswatch to stream events (create, update, delete) on object(s)

apiGroups: List of allowed API groups, or APIGroupAll (or “*” in YAML) to bypassverification on this field

Each PolicyRule represents a set of permissions to grant for this role.

Security ContextsYou can configure security contexts at Pod and container levels.At Pod LevelThe Pod spec defines several fields in its PodSecurityContext structure, accessible in thesecurityContext field.

By default, the processes inside the containers of a Pod run with the root rights. Thanks tothe container isolation, the root rights inside the container are limited.

But in some circumstances, for example, when mounting external filesystems inside thecontainer, you would like that the processes run with specific user and group rights.

With the runAsUser, runAsGroup, and supplementalGroups fields, you can affect the firstprocess of the containers of the Pod to a specific user and a specific group and add theseprocesses to supplemental groups.

 securityContext:        runAsNonRoot: true        runAsUser: 1000        runAsGroup: 1001        supplementalGroups:        - 1002        - 1003

sysctls:        - name: kernel.shm_rmid_forced          value: "1"

By default, traffic between Pods of a cluster is unrestricted. You can fine-tune the trafficauthorization between the Pods by declaring network policies, using the NetworkPolicyresource.

In this case, kubelet will need to get the necessary credentials to be able to download theimages stored in the private registry.Using imagePullSecretsThis is the recommended way to give access to the registry when you do not have access tothe nodes or when nodes are created automatically.The first step is to create a Secret containing the registry credentials.

An important thing to know is that once the image is downloaded by a node, all Podsexecuting on this same node will be allowed to use this image, even if they do not specify animagePullSecrets.To test it, first deploy the following Pod on a multi-worker cluster, and see on which nodeit is deployed:

In the previous step, you logged in the private registry to manually download the image.During login, Docker created a ∼/.docker/config.json to store the credentials usedduring login.

You can copy this file in a directory recognized by kubelet, so kubelet will try to downloadimages using these credentials:

### 12. Persistent Volumes

A persistent volume (PV) is a storage resource provisioned by the cluster administrators. Theprovisioning can be manual or automatic.

These storage resources are intended to be used by Pods, through the use ofPersistentVolumeClaims: a Pod will claim a persistent volume with specific capabilities,and Kubernetes will try to find a persistent volume matching these capabilities(independently of the implementation details).

On the workers, install the NFS drivers to be able to mount NFS filesystems:Repeat these steps for each worker:$ gcloud compute ssh worker-0Welcome to worker-0$ sudo apt-get -y update$ sudo apt-get -y install nfs-common

You can test that the worker can mount the filesystem:

You can now define the NFS persistent volume:

# capabilities  accessModes:  - ReadWriteOnce  - ReadOnlyMany  - ReadWriteMany

# implementation    nfs:      path: /vol1      server: 172.25.52.106

The PersistentVolume accessModes field indicates what the capabilities of theunderlying storage system are in terms of simultaneous access on read-only or read/writemode. Three values are defined:

ReadWriteOnce (RWO)The storage is accessible for read and write operations by a single client.

ReadOnlyMany (ROX)The storage is accessible for read-only operations, by several clients.ReadWriteMany (RWX)The storage is accessible for read and write operations, by several clients.

If the PV has a Many capability, several Pods will be able to use this PV at the same time.Note that for a PV to be used by several Pods, the access mode claimed must be the sameby all the Pods; it is not possible that one Pod uses a ReadOnlyMany modeand another Poduses ReadWriteMany on the same PV.

When a Pod needs a persistent volume, it must claim one. It does not claim a particularpersistent volume, but rather claims a list of capabilities. The persistent volume controller willaffect the best possible persistent volume depending on the capabilities matching.

selectorA label selector to match specific persistent volumes based on labels.

Remove the previously deployed Kubernetes cluster:

### 13. Multi-container Pod Design Patterns

When a Pod contains several containers, the containers share network and storageresources.

The Kubernetes community has described these following design patterns using multi-container Pods.

Init ContainerThe Init container patterncan be used when you want to initialize some resources before themain container runs or wait for a specific external state.The spec of the Pod resource contains an initContainers field. This field contains an arrayof container definitions.

If an init container fails, the Pod immediately fails.

Initialize a StorageIn this first example, an Init container loads files from a Google Cloud Bucket and storesthese files in a volatile volume. The main container, an nginx server, mounts the same volatilevolume and serves these files:

initContainers:      - name: copy-static-files        image: gcr.io/cloud-builders/gcloud        command:        - "bash"        - "-c"        - "gsutil cp -R $(SOURCE)/* /mnt/"        env:        - name: SOURCE          value: gs://my-gcp-project/my-bucket        volumeMounts:        - mountPath: /mnt          name: static-files          readOnly: false

Wait for Availability of Another ServiceIn this second example, an Init container tests if a backend service is available andterminates successfully when the service becomes available, so the main container canconsider the backend is running at startup:

Sidecar ContainerA sidecar container is an additional container running in the same Pod as the main container,intended to assist the main container during its execution.

Adapter ContainerAn adapter container is a sidecar container, whose purpose is to intercept the inbound trafficto the Pod and adapt it to the protocol expected by the main container.

### 14. Observability

When working with Kubernetes, observability is crucial.

First, you have to note that there are two types of Kubernetes resources: managed andunmanaged resources. A managed resource is recognizable because its definition contains aspec section and a status section. Controllers are responsible for managing suchresources; they will read the spec section, will do their best to change the world to reflectthese specifications, and then will report the state in the status section.

Other resources, like ConfigMap, Secret, Volume, ServiceAccount, Role, RoleBinding,and others, are unmanaged resources, and their purpose is to contain specific data used byother elements of the system.

 Using this command, you can observe thestate of your managed Kubernetes resources.

An Eventis a Kubernetes resource, used by the controllers and other Kubernetes elements,to log information. An event is of type Normal or Warning, indicates the time at which ithappened, the action taken, the reason why the action was taken, which controller took theaction and emitted the event, the resource the event is about, and a human-readabledescription of the event.

Reading these events, you will generally find the root cause of some problems in yourcluster, including the following:A Pod is not schedulable, because the nodes do not have enough available resources.An image cannot be pulled.A volume, ConfigMap, or Secret is not available.A readiness or liveness probe of a container failed.

kubectl get events. You can add the -w option that will make the command wait for newevents (you can terminate the command with Ctrl-C):

Debugging Inside ContainersIt is possible to execute commands from inside containers with the command kubectl exec. This implies that containers contain debug utilities.

Debugging ServicesAn important responsibility of Kubernetes is to expose Pods using Service and Ingressresources.

LoggingContainers have to output logs to standard output (stdout) or standard error (stderr) streamsfor the logs to be available within the Kubernetes infrastructure.You can use the kubectl logs commandto display the logs of a particular pod:

or a set of Pods, selected by their labels (use --prefix to differentiate the Pods):$ kubectl logs -l app=nginx --prefix

--previous (-p for short) allows to view the logs of the previous containers, which isuseful when a container crashed and you want to see the error that made it crash

containers shows the logs of all the containers.--timestampsdisplays timestamps of logs at the beginning of lines.

Logging at the Node LevelBy default, the logs of a Pod are stored in the node running the Pod. When you are deployingthe cluster with kubeadm, the logs can be found in the /var/log/pods directory. It is theresponsibility of the cluster administrator to install some log rotation for these logs.

Using a Sidecar to Redirect Logs to stdoutIf your application is not able to output logs to stdout or stderr, but only to files, you canrun a sidecar container that will read these log files and stream them to its own stdout. Thisway, the logs become available at the node level and can be explored with kubectl logsand exported with a logging agent.

MonitoringYou can use the kubectl top commandto monitor the nodes of the clusters and the Pods.To use this command, you first have to install the Metrics Serveron your cluster. You canfollow the instructions in Chapter 7 to install it.

Then, you can run the following commands to get the CPU and memory usage for eachnode and Pod:$ kubectl top nodes

The Metrics Server and the kubectl top commandonly give you a limited set of short-termmetrics. It is recommended to install a complete monitoring solution.

Monitoring with PrometheusPrometheus(prometheus.io) is a complete monitoring and alerting solution graduated by theCNCF. The Prometheus system is mainly composed ofA server which scrapes and stores time series dataClient libraries for instrumenting application codeA node exporter to export host metricsAn alert manager to handle alerts

The server, a stateful application, will regularly pull nodes and application components toscrape time series data containing metrics to monitor and save this data to its database.

Application developers are responsible for exposing metrics to monitor with the help of clientlibraries provided by Prometheus. The metrics are generally exposed on the /metricsendpoint.

The Grafanadashboard(www.grafana.com) is the Prometheus companion that will help yougraphically expose and explore your metrics.

### 15. Upgrading the Cluster

Upgrading the Kubernetes cluster is done in two phases. You first upgrade the control planenodes and then the worker nodes. It is possible to upgrade to the next minor release or toany other next patch release of the same minor release.

You will see how to prepare the cluster to make these operations withoutinterrupting your applications.Finally, you will see how to back up and restore your cluster certificates and data.

Check the possible upgrade plans:$ sudo kubeadm upgrade plan

Upgrade kubelet and kubectl on the controller:$ sudo apt-get update && \   sudo apt-get install \   -y --allow-change-held-packages \   kubelet=1.19.0-00 kubectl=1.19.0-00$ sudo apt-mark hold kubelet kubectlAt this point, the controller node should show the latest version:

Upgrade the WorkersRepeat these steps for each worker.First, drain the node, from your machine:$ kubectl drain worker-0 --ignore-daemonsets

Then, connect to the node and install the desired version of kubeadm:

After upgrading all worker nodes, you should obtain the latest release on each node ofthe cluster:

Upgrading the Operating SystemIf you need to reboot the host of a cluster node for maintenance (e.g., for a kernel orhardware upgrade), you first need to drain the node:$ kubectl drain $NODENAME --ignore-daemonsets

Draining the node will have two effects:Evict all Pods from this node, all Pods controlled by a ReplicaSet being rescheduled onanother node.Make this node unschedulable, so that no new Pod is scheduled on this node during themaintenance.

You can now safely make maintenance operations on the operating system or hardware.Once the maintenance is over, you can uncordon the node, to make the node schedulableagain:$ kubectl uncordon $NODENAME

Back Up a ClusterBack up the files /etc/kubernetes/pki/ca.crt and /etc/kubernetes/pki/ca.key aftercluster installation.Periodically create a snapshot of the etcd database with the command

kubectl exec -it -n kube-system etcd-controller \   sh -- -c "ETCDCTL_API=3 etcdctl snapshot save snapshot.db \   --cacert /etc/kubernetes/pki/etcd/server.crt \   --cert /etc/kubernetes/pki/etcd/ca.crt \   --key /etc/kubernetes/pki/etcd/ca.key"Snapshot saved at snapshot.db

Place snapshot.db in /mnt and then run:

"etcdctl snapshot restore /backup/snapshot.db ; mv /default.etcd/member//var/lib/etcd/"

### 16. Command-Line Tools

kubectl is the command-line tool used to work on Kubernetes clusters. You can use it tocreate application resources and cluster resources, interact with running containers, andmanage the cluster.

 source <(kubectl completion bash)or, if using the zsh shell:$ source <(kubectl completion zsh)

Run kubectl completion --help to get the instructions on how to install the completionin a permanent way.

This file is by default searched at $HOME/.kube/config. It is possible to use another file either by using the --kubeconfig flag or bydefining the KUBECONFIG environment variable.

kubectl configThis command provides subcommands to edit the list of clusters, users, and contexts andto switch the current context:get-clusters, set-cluster, delete-cluster to edit cluster information.set-credentials to edit user credentials.

Generic Commandskubectl apply

kubectl getGet a tabular list of resources or their complete definitions with the -o yaml flag.

Create a cronjob, given an image and a schedule. You can also specify the command andargs to pass to the container, useful when using a generic image like busybox:kubectl create cronjob pinghost --image=busybox \--schedule="10 * * * *" \-- sh -c 'ping -c 1 myhost'

The dry-run=client -o yaml is useful to output the declarative form of the cronjob thatcan be edited and applied later with kubectl apply.

kubectl autoscaleCreate an auto-scaler for applicable resources (Deployment, ReplicaSet, StatefulSet). Theselection of the resources to auto-scale is done by type/name or file. The important flagsare min, max, and cpu-percent to indicate the limits in number of replicas and theaverage CPU percentage at which the resource will be scaled.

Configuring Workloadskubectl create configmapCreate a ConfigMap resource, getting key/value pairs:From env-style files (--from-env-file): The keys and values will be extracted from thefile# .envkey1=value1key2=value2

Annotate and LabelkubectlannotateAttach metadata to any kind of resource, not used by Kubernetes but intended to be usedby tools or system extensions.kubectllabelEdit labels on any kind of resource, used as selectors.

kubectl cpCopy files from/to an existing container to/from your local computer.The syntax of the source and destination files are/path/to/file for a file in your local computerpod:/path/to/file for a file in the container of pod

kubectl describeDisplay the details of a resource.kubectl logsPrint the logs for a container in a Pod.kubectl port-forwardForward local ports to a Pod:kubectl port-forward type/name local-port:remote-porttype can be a Pod, a workload managing a Pod (ReplicaSet, Deployment, etc.), or aService exposing a Pod.

The --address ip flag specifies the local address to listen on, 127.0.0.1 by default.

kubectl proxyRun a proxy on your local machine to the Kubernetes API server.

kubectl topDisplay resource (CPU/memory/storage) usage for nodes or Pods.

kubectl drainPrepare a node for maintenance by marking it as unschedulable and evicting Pods fromthis node.

kubectl api-versionsDisplay the supported API versions on the server.kubectl api-resourcesDisplay the supported API resources on the server.

kubectl explainList and document fields of a resource.

Helm is a package manager for Kubernetes.Helm charts are packages that help you define, install, and upgrade Kubernetes applications,and these charts are stored in Helm repositories.You can use the helm command to search, install, upgrade, roll back, and uninstall charts.

Install ChartsThe charts are stored in Helm repositories. You can use the Helm Hub(https://hub.helm.sh/) to discover new repositories.

The helm repo command helps you manage the repositories you have access to. Afterinstallation, you can see that you do not have access to any repository:

Create Your Own ChartsThe command helm createis used to create your own package. When you run thiscommand, it will create for you a new directory structure with a default application

The Chart.yaml file contains metadata about the package.The templates directorycontains the Kubernetes manifests that will be used to deploy thedifferent resources that compose your application (a Deployment, a Service, an Ingress, aServiceAccount, and a Horizontal Pod Autoscaler). If you look at one of these files, you cansee that these manifests are templated.

Helm uses the Go templates engine, and you can find all the details about the associatedlanguage at https://helm.sh/docs/chart_template_guide/.

When using kubectl apply with the -k flag, the command will look at akustomization.yaml file into the directory specified in the command line (here the currentdirectory) and will apply it.A kustomization.yaml filecontains directives of different types:The resources directive that points to manifest files

### Back Matter

Perform a version upgrade on a Kubernetes cluster using kubeadm.Chapter 15Implement etcd backup and restore.

Workloads and Scheduling (15%!)(MISSING)Understand deployments and how to perform rolling update and rollbacks.“ReplicaSet Controller,” “Deployment Controller,” “Update and Rollback,” and “DeploymentStrategies,” Chapter 5Use ConfigMaps and Secrets to configure applications.Chapter 6

Understand how resource limits can affect Pod scheduling.“Resource Requests,” Chapter 9

Awareness of manifest management and common templating tools.“Helm,” “Kustomize,” Chapter 16

Services and Networking (20%!)(MISSING)Understand host networking configuration on the cluster nodes.Chapter 1Understand connectivity between Pods.Chapter 1Understand ClusterIP, NodePort, and LoadBalancer service types and endpoints.

Know how to configure and use CoreDNS.

Observability (18%!)(MISSING)Understand LivenessProbes and ReadinessProbes.

Understand container logging.Basic loggingUnderstand how to monitor applications in Kubernetes.“Auto-scaling,” Chapter 7Understand debugging in Kubernetes“Interacting with the Application,” Chapter 16

Services and Networking (13%!)(MISSING)
Understand Services .
Chapter 10
Demonstrate basic understanding of NetworkPolicies.
“Network Policies,” Chapter 11


State Persistence (8%!)(MISSING)
Understand PersistentVolumeClaims for storage .
“Persistent Volume,” Chapter 12
