## 企业私有云建设指南
> 孙杰等

### 序1

本书的四位作者长期参与企业私有云的理论研究、架构设计、技术选型、部署实施和日常管理，十分了解企业私有云建设维护方面的需求和难点。本书不只是讨论私有云技术，而且对私有云进行了全景式论述。本书分享了云计算如何为企业信息化赋能、企业私有云建设的需求分析和技术架构规划。在设计方面，本书讨论了私有云计算、网络、存储的规划和建设，以及企业私有云设计的通用原则。从产品角度，本书分享了目前主流的开源私有云方案OpenStack、商业的私有云方案VMware在私有云建设方面的实践。本书第10章还深入讨论了私有云的管理及成熟度评估。

### 序2

造成软硬件资源无法共享、系统动态可扩展性差、资源调度缺乏灵活性、信息数据“孤岛”严重、运营管理成本高，很难满足业务发展与激烈的市场竞争需求，也很难适应新数字经济环境下的技术创新和新型商业模式。而基于网络和虚拟化技术的云计算能够根据需求灵活敏捷地将计算资源切换到所需的应用上，满足弹性需求；同时为解决跨平台资源共享与服务，以及实现快速自动部署、安全且高效地交付应用、工作负载均衡和协同管理，提供了很好的解决方案。

### 序3

第一个建设目标，是使用虚拟化技术，抽象所有IT资源，使之成为应用在其之上可以任意部署的资源池。当下绝大多数企业私有云建设，都能很好地实现这个目标，使得IT资源的使用效率大幅度提升；

### 序4

2006年，亚马逊推出了AWS，正式拉开了全球云计算产业的大幕。发展到2018年，云计算业已成为企业IT转型的核心驱动力。

### 前言

2006年8月，Google在业界首次提出“云计算”（Cloud Computing）的概念；同年亚马逊相继推出在线存储服务S3和弹性计算云EC2等云服务。

云计算的核心可以用五大基本特征、三种服务模式以及四类部署模式来概括：五大基本特征是按需的自助服务、广泛的网络访问、资源池化、快捷的弹性伸缩以及可度量的服务；三种服务模式为基础架构即服务（IaaS）、平台即服务（PaaS）、软件即服务（SaaS）；四类部署模式可以划分为私有云（专有云）、行业云、公有云以及混合云。

以AWS、Google、阿里等为代表的公有云大力发展的同时，很多大型企业出于数据安全性、系统稳定性、软硬件自主权，以及对自主可控和TCO低的考虑，更加倾向于建设企业私有云来承载内部业务信息系统的运行。然而构建企业私有云并非一蹴而就，正如Gartner的副总裁Tom Bittman所述：“部署私有云并不是简单地对硬件进行采购，而是一场革新。”

### 第1章 企业信息化与云计算架构和关键技术

￼从信息化内容看，重点是企业级套装软件的实施和开发，大部分企业引入了ERP、CRM、PDM及行业特性管理软件，并通过集成平台实现系统的整合与集成，实现了系统间的互联、互通、互操作。
￼从信息化范围看，信息化首先是跨部门实现了企业内部的整合，而后是跨过企业边界，部分实现了供应链上合作伙伴之间的整合。

未来几年，随着PaaS层和SaaS层的逐步成熟，越来越多的集团企业会将系统部署到云端，混合云将在较长一段时间内成为IT基础设施的常态，如何处理好私有云和公有云的关系是考验IT建设和运维的一道难题。

### 1.2 云计算参考架构

云计算是一种按使用量付费的模式，这种模式提供可用的、便捷的、按需的网络访问，并进入可配置的计算资源共享池（资源包括网络、服务器、存储、应用软件、服务），这些资源能够被快速提供，只须投入很少的管理工作或与服务供应商进行很少的交互。

### 1.4 云计算涉及的关键技术

虚拟化技术主要用来解决高性能物理硬件产能过剩和老旧硬件产能过低的重组和重用，透明化底层物理硬件，从而最大化利用物理硬件。

分布式处理则是将不同地点或具有不同功能，或拥有不同数据的多台计算机通过通信网络连接起来，在控制系统的统一管理控制下，协调完成大规模信息处理任务的计算机系统。
分布式系统是建立在网络之上的软件系统，具有高度的内聚性和透明性。以分布式数据库为例，内聚性是指每一个数据库分布节点高度自治，有本地的数据库管理系统；透明性是指每一个数据库分布节点对用户的应用来说都是透明的，即看不出是本地还是远程。在分布式数据库系统中，用户感觉不到数据是分布的，即用户无须知道关系是否分割、有无副本、数据存放于哪个站点以及事务在哪个站点上执行等。

CAP理论是由Eric Brewer提出的分布式系统中最为重要的理论之一：
￼Consistency:（强）一致性，事务保障，ACID模型。
￼Availiablity:（高）可用性，冗余以避免单点，至少做到柔性可用（服务降级）。
￼Partitiontolerance:（高）可扩展性（分区容错性），一般要求系统能够自动按需扩展，比如HBase。

### 2.2 企业私有云技术路线选择

VMware是商业软件，其成熟度和稳定性经受住了大量实际环境的考验，但使用成本高，体现在其授权费用和服务费用上。相对VMware的昂贵价格，OpenStack免费、开放的优势还是很明显的。

虽然OpenStack是免费使用的，但是它需要有专业的开发人员和此领域的专家才行，并且需要完成很多架构和搭建方面的工作，因为它支持很多部署场景，并且安装过程都不尽相同。VMware则需要花费一些经费购买授权和服务，但相对来说更加容易安装和运行，另外VMware的学习成本更低一些。

大型企业使用VMware则更稳定和可靠。而OpenStack入门门槛较高，如果企业没有足够的技术能力储备则无法解决大面积部署OpenStack所遇到的问题。


### 2.3 计算资源管理

在企业IT基础设施云架构下，计算资源、存储资源、网络资源在统一的云平台管理下被封装整合成不同的资源池，以云服务的方式提供给服务使用者。

开发者可以通过OpenStack API访问计算资源从而创建云应用，管理员与用户则可以通过Web访问这些资源。在OpenStack中，计算服务是OpenStack的核心服务，它由Nova项目提供实现。Nova项目不包括任何虚拟化软件；相反地，它通过各种插件与运行在主机上的底层虚拟化软件进行对接，然后再向外提供API。

当Nova-api服务接收到创建虚拟机的请求后，它会通过消息队列将请求转交给Nova-scheduler模块，后者会根据在数据库中保存的整个环境中计算资源池的情况，按照请求中所要申请的资源，选择一个最合适的主机来创建该虚拟机。

### 2.6 监控管理

指标1：每秒I/O数（IOPS或TPS）
对于磁盘来说，一次磁盘的连续读或者连续写称为一次磁盘I/O，磁盘的IOPS就是每秒磁盘连续读次数和连续写次数之和

指标2：吞吐量
吞吐量即硬盘传输数据流的速度，传输数据为读出数据和写入数据的和。其单位一般为kbit/s、MB/s等。当传输大块不连续数据时，该指标有重要参考作用。

### 2.7 运维管理

￼缺乏统一的服务台，用户请求随意性大，他们直接找有经验的信息人员，导致能干的人员成天处理无价值的琐碎事情，价值无法有效体现。

￼缺乏规范的运维制度和流程。在处理问题时，没有对问题进行记录和分类，导致无法跟踪和监控问题的处理情况。
￼IT运维的相关经验没有积累和共享。由于缺乏对运维过程的记录，使得问题的处理方法只有当时的维护人员掌握，相关经验难以积累和共享。
￼运维人员绩效无法量化。在运维工作中没有建立量化的考核指标，IT运维质量和运维人员的绩效无法量化，使得运维人员的工作积极性得不到提高。

Ansible:Ansible是一款基于Python的自动化运维工具，集合了众多运维工具（Puppet、Chef）的优点，实现了批量系统配置、批量程序部署、批量运行命令等功能。管理节点上的Ansible将命令通过SSH协议（或者Kerberos、LDAP）推送到被管理节点上并执行命令，通过这种方式能够在管理节点上控制一台或多台被管理节点，以执行安装软件、重启服务等命令。

### 4.1 计算资源

存储节点主要为OpenStack私有云提供块存储、对象存储或文件存储服务。在OpenStack私有云中，使用最普遍的还是Cinder块存储和Ceph分布式存储。如果仅使用OpenStack原生的块存储服务，则存储节点主要运行Cinder-volume服务；如果使用Ceph分布式存储集群，则存储节点运行Ceph-osd进程，通常Ceph-mon进程会部署在控制节点。就主流的OpenStack私有云建设而言，Ceph是推荐的最佳实践。关于Ceph存储节点物理服务器的选型配置，Ceph官网有更详细的配置参考

考虑到网络瓶颈和服务高可用问题，在实际的OpenStack高可用集群部署中，通常把网络服务部署在三个控制节点上，结合负载均衡（HAProxy）和虚拟路由冗余协议（Keepalived/VRRP）实现网络服务的高可用。因

Hypervisor是运行在物理服务器和操作系统之间的软件层，其主要作用在于调度虚拟客户机系统对共享物理资源的使用请求。Hypervisor是虚拟化技术的基石，同时也是云计算的核心基础。当通过Hypervisor为虚拟客户机分配指定资源后，一旦服务器启动并执行Hypervisor时，其便会给已创建的每一台虚拟机分配适量的内存、CPU、网络和磁盘资源，并加载所有虚拟客户机的操作系统，开始对硬件资源进行调度响应以供虚拟客户机使用，同时对客户机系统进行隔离。

在云计算领域，目前较为成熟、使用最为广泛的Hypervisor主要有KVM、ESXi、HyperV、Xen和XenServer等，

KVM、HyperV和ESXi则是CPU虚拟化时代全虚拟化Hypervisor中的代表。在上述Hypervisor中，Xen和KVM都是基于开源社区的虚拟化项目，并且随着硬件对虚拟化的加速支持，为了提升性能和支持各种发行版本的操作系统，越来越多的用户正转向全虚拟化Hypervisor，如AWS和阿里云都在将底层Hypervisor由Xen迁移至KVM。

为了解决OpenStack的自动化部署问题，众多基于Puppet、Chef、SaltStack、Ansible的项目层出不穷，但是均没有从根本上解决OpenStack部署复杂和升级维护困难的问题，直至Docker容器技术在OpenStack部署中的应用。由于Docker容器对OpenStack服务进程进行了隔离封装，问题才得到根本性改观。

### 4.2 存储资源

使用对象存储，用户可以通过REST API来访问二进制存储对象，而无须如块存储一样需要将存储设备挂载到实例后才能与其进行交互。例如，亚马逊AWS的S3存储便是最为成熟的商业对象存储。OpenStack中实现对象存储的项目是Swift，这也是OpenStack最早的项目之一，如果用户需要存储或管理大量无序、非结构化的数据集，则可以考虑部署对象存储Swift。

共享文件系统服务为多租户云环境提供了一系列管理共享文件系统的服务集，用户通过挂载远程文件系统到本地实例以进行文件存储和交互的形式，实现与共享文件系统服务的交互。共享文件系统服务为用户提供了“共享”，这里的“共享”通常指一个远程可挂载的文件系统，在同一时刻不同用户的多个主机实例可以同时挂载并访问同一个“共享”。通过OpenStack中的共享文件系统服务，用户可以创建一个“共享”，指定共享的大小、访问协议和可见级别。根据选取的后端模式和有无共享网络，用户可以在共享或独立服务器上创建“共享”，同时还可以将不同的“共享”集合到共享组中以保持多个共享数据的一致性。

### 4.3 网络资源

Neutron的众多插件可以被归为两类，即Core插件和Service插件。Core插件实现的是Neutron中的Core API，其主要负责L2网络通信和IP地址管理，如网络、子网、子网池和端口的创建与删除。Service插件实现了Neutron中的扩展API，并负责提供三层、四层和七层的高级网络服务，如L3路由、防火墙、VPN与负载均衡等。

### 4.4 高可用部署与备份容灾

在集群服务方面，可将OpenStack高可用集群划分为基础架构服务高可用（包括消息队列服务高可用、数据库服务高可用和缓存服务高可用）、控制服务高可用（如Nova-api、Glance-api和Neutron-server等）、网络服务高可用（如L2和L3服务）、存储服务高可用（如Cinder-volume、Ceph RBD）和计算服务高可用（如Nova-compute和虚拟机）等。

基于Keepalived和HAProxy的解决方案。在这两种方案中，OpenStack控制服务和基础架构服务通常都部署在三台控制节点上，OpenStack控制服务以Active/Active或Active/Passive高可用模式运行在三个控制节点上，并且OpenStack基础架构服务的高可用实现在两种解决方案中是类似的，如通过消息队列镜像形式实现RabbitMQ服务的高可用，通过Galera集群实现MySQL或MariaDB数据库服务的高可用，通过列表形式实现Memcache缓存服务高可用。两种高可用解决方案的不同之处在于OpenStack服务的运行管理模式和服务IP地址高可用的实现方式。

### 5.2 vSphere体系架构

vSphere可加快现有数据中心向云计算的转变，同时还支持兼容的公有云服务，从而为支持混合云模式奠定了基础。

### 6.2 架构设计通用原则

 集群横向扩展须包括计算节点、存储、网络资源的扩展以及扩展后的聚合；
■ 计算节点新加入集群后，私有云上的现有业务不受影响，且均能在新节点上正常运行；
■ 新节点的加入对用户来说是透明的，即用户不需要进行额外操作就可以正常使用。

■ 具备完善的监控能力，可及时发现异常；
■ 具备自我修复能力，包括在最小化影响业务的条件下在线迁移虚拟机、新建虚拟机等。

■ API支持Restful；
■ API升级具备良好的兼容性，不影响现有对接系统；
■ API的信息访问支持加密和安全访问控制，具备详细的接口访问日志，便于审计。

### 第7章 企业私有云平台建设和管理

关键设备之间的物理链路采用双路冗余连接，按照负载均衡方式或active-active方式工作。关键主机可采用双路网卡来增加可靠性。全冗余的方式使系统达到电信级可靠性。要求网络具有设备/链中故障的毫秒级保护切换能力。

虚拟资源池化是网络发展的重要趋势，可大大提高资源利用率，降低运营成本。应有效开展服务器、存储的虚拟资源池技术建设，网络设备的虚拟化也应进行设计实现。服务器、存储器、网络及安全设备应具备虚拟化功能。

### 7.2 架构设计

公有云和私有云都会使用云管理平台，但是企业私有云具有更多的个性化特性和自定义的功能。不同的云平台在功能特性、易用性、灵活性和成本上差异巨大，一个企业可以开发自己的云管理平台，也可以从第三方云计算厂商采购，当然最重要的是需要满足自己业务和管理的需求和特性。

在企业私有云中，企业可以通过云管理平台实现对物理资源、虚拟资源的统一管理和自动化配置，同时还可以提供基础设施服务、平台服务、软件服务（含通用软件和专业软件），结合云的多租户管理、流程服务编排、资源的计费计量管理，从而可以监控整个云基础架构的使用情况并对其进行综合优化管控，所以云平台对于企业来说是一个必需品。相反，如果没有云管理平台，你就需要自己配置网络、服务器、虚拟机模板、应用等所有的一切，因此会致使企业运营和维护成本居高不下。

一般来说，目前大家比较公认的云架构是划分为基础设施层（IaaS）、平台层（PaaS）和软件服务层（SaaS）三个层次的，各层有不同的侧重和服务。搭建企业云平台时还需构建基于云的运营管理体系和信息安全体系

基础架构即服务：包括基础硬件设施层、虚拟化/资源池化层、资源调度层。

### 7.3 部署和管理

￼云管理平台统一管理所有资源池，包括生产资源池和灾备资源池。
￼为了提高部署性能，在每个资源池中保存适应该类资源池的镜像和脚本文件、配置文件副本。
￼业务数据在存储底层通过异步复制实现一致。

统一的云管理平台打破了业务应用对资源的独占方式，实现硬件资源和软件资源的统一认证、统一管理、统一分配、统一部署、统一监控和统一备份，如图7-4所示。

### 7.4 容器与容器云建设

（3）企业规模大、业务复杂、应用粒度更细
这时Kubernetes就更适合了，因为Kubernetes模块划分得更细、更多，比Marathon和Mesos功能丰富，而且模块之间完全松耦合，可以非常方便地实现定制化。另外Kubernetes提供了强大的自动化机制，从而使后期的系统运维难度和运维成本大幅降低，而且Kubernetes社区的流行使得使用Kubernetes的公司能够很快地得到帮助，方便Bug的解决。

跨主机容器间的网络互通已经成为最基本的要求。跨主机的容器网络解决方案不外乎三大类。

比如Flannel的VXLAN，特点是对底层的网络没有过高的要求，一般来说只要是三层可达即可——只要是在一个三层可达网络里，就能构建出一个基于隧道的容器网络。不过问题也很明显，一个得到大家共识的是随着节点规模的增长，复杂度会提升，而且出现网络问题后跟踪起来也比较麻烦。在大规模集群情况下，这是需要考虑的。

路由技术从三层实现跨主机容器互通，没有NAT，效率比较高，与目前的网络能够融合在一起，每一个容器都可以像虚拟机一样分配一个业务的IP。但路由网络也有问题，路由网络对现有网络设备影响比较大，路由器的路由表应该有空间限制，一般是两三万条，而容器的大部分应用场景是运行微服务，数量集很大。如果几万个新的容器IP冲击到路由表中，会导致下层的物理设备无法承受；而且每一个容器都分配一个业务IP，业务IP会很快消耗完。

基于主机的端口冲突和网络资源竞争比较麻烦，相对来说Calico的是纯三层的SDN实现，它基于BPG协议和Linux自己的路由转发机制，不依赖特殊硬件，没有使用NAT或Tunnel等技术。它能够方便地部署在物理服务器、虚拟机（如OpenStack）或者容器环境下，同时它自带的基于Iptables的ACL管理组件非常灵活，能够满足比较复杂的企业安全隔离需求。关于容器应用的网络隔离还有多种解决方案，基本上就是把不同的应用容器放置在不同的VLAN/VXLAN中，通过让不同的VLAN/VXLAN不能互访而实现隔离。可以尝试用Docker自带的Overlay来简单地解决，首先创建不同的网络，然后在启动不同应用的容器时，使用--net参数指定容器所在的对应的VXLAN即可。

典型ELK架构的优点是搭建简单、易于上手，缺点是Logstash耗费资源较大，运行占用CPU和内存高，另外没有消息队列缓存，存在数据丢失隐患，建议小规模集群使用。如果大规模集群使用该架构，则可以引入Kafka（或者Redis），增加消息队列缓存，均衡网络传输，再把Logstash和Elasticsearch配置为集群模式，以减轻负荷压力。Logstash占用系统资源过多，后来Fluentd替代了Logstash，被称作社区方案中的EFK，相比Logstash等传统日志收集工具，Fluentd的日志收集功能对容器的支持更加完备。

通过标准化日志，如带上唯一的ID信息等，可以实现把同一个业务在不同微服务中的处理过程关联起来。通过标准化的应用日志，我们可以对交易率、成功率、响应时间等关键业务指标进行统一关联分析，作为问题预警、诊断分析、容量扩缩的科学依据。

在虚拟机运维时代，Nagios和Zabbix等都是十分经典的性能监控软件。但在容器时代，这些曾经耳熟能详的软件大多不能提供方便的容器化服务的监控体验，

说到容器应用的监控设计，在这里要注意监控是分层的，具体可以分为系统层面、应用层面和服务层面，每个层面都有自己的监控重点。
（1）系统层面
系统层面主要是针对资源使用情况、网络连通性、节点健康情况的监控。

### 7.7 运维与效益分析

AIOps不依赖于人为指定规则，主张由机器学习算法自动地从海量运维数据（包括事件本身以及运维人员的人工处理日志）中不断地学习，自动提炼并总结规则，做出分析、决策，从而达到运维系统的整体目标。

AIOps是运维的发展必然，是自动化运维的下一个发展阶段。Gartner相关报告预测AIOps的全球部署率将从2017年的10%!增(MISSING)加到2020年的50%!。(MISSING)其应用行业除了互联网以外，还包括高性能计算、电信、金融、电力网络、物联网、医疗网络和设备、航空航天、军用设备及网络等领域。

一体化智能运维管理平台分为三个层次，即数据采集层、数据分析和处理层和数据展示层。平台采用模块化设计，模块之间松耦合。新模块可以直接整合到平台中，模块之间通过接口、消息队列等方式进行通信。

智能运维平台的人才体系建设强调培养知识型人才、服务型人才和全栈型人才。
￼知识型人才。智能运维实现了运维真正的自动化、可量化和自服务体系，减少了运维人员机械的重复劳动，要求运维人员注重工作中自我引导和自我管理，向创造性人才转变，依靠企业知识管理平台进行创造性思维，不断形成创造性的智慧成果。
￼服务型人才。智能运维是面向服务的运维模式，强调以业务为导向，更关注用户体验。人才培养注重服务型，突出集约化管理和主动性维护，人员的价值评价和考核体系也相应以服务质量为依据。
￼全栈型人才。智能运维弱化研发和运维界限，倡导IT融合，要求运维人员在兼顾横向专业领域，如Web层、中间件层、数据库层等的同时打破技术个性的壁垒，提升整体业务运营能力，向开发和运维全栈发展。

### 8.1 自动化与DevOps

Docker容器的出现将DevOps的落地实现和普及推广又推上了一个新的台阶。就现阶段开源私有云集群管理而言，基于Docker容器的微服务架构和容器化私有云，以及Ansible自动化运维工具的组合使用，是目前主流的开源云管理最佳实践手段，通过类似的微服务架构和自动化运维方案，包括二次开发、部署、运维和升级在内的整个私有云集群生命周期管理都可通过DevOps理念来实现。

### 8.2 中心化日志监控与可视化

以开源OpenStack私有云为例，在OpenStack部署和运行过程中，位于不同节点上的分布式服务将会产生大量日志文件，如何从分布式集群上收集、存储、检索和可视化日志，是OpenStack私有云部署和运行管理中非常重要的一个部分。

中心化日志监控体系的最佳实践为Elasticsearch、Fluentd和Kibana组合（EFK架构）。其中，Fluentd负责日志收集与传输，Elasticsearch负责中心化日志的存储、组织和使其具有更简易的访问性，Kibana负责中心化日志的可视化检索和展示。

在Zabbix与Grafana的组合中，Zabbix主要负责分布式集群中各个节点的监控数据的采集，Grafana主要负责监控数据的可视化显示，为了将Zabbix与Grafana对接，通常需要在Grafana中安装并启用Zabbix插件，之后Grafana方可显示Zabbix中的数据源

在OpenStack私有云集群中，要实现网络可视化监控，可以部署RedHat开源的Skydive项目。Skydive是一个开源的网络实时分析工具，可以分析网络拓扑和协议，有助于理解网络基础设施，以及了解网络中发生的事件。其中，运行在各个节点上的Skydive agent负责收集网络拓扑信息和流量信息，并将收集到的信息存储到Elasticsearch中，之后通过Skydive analyzer进行分析和可视化，用户通过Web界面即可监视集群内部的网络流量和拓扑信息。

### 8.6 OpenStack容器化私有云扩容升级最佳实践

确保欲加入集群的计算节点系统环境与现有计算节点一致，包括Docker引擎安装、防火墙、安全规则等，将新节点的主机名加入集群的/etc/hosts文件中，在集群中同步最新的/etc/hosts文件。

（3）修改inventory文件
因为是在多节点环境中，因此只须修改multinode文件即可。这里主要是扩容新增计算节点，因此将新节点的主机名加入multinode的计算节点组中

### 第9章 企业私有云VMware最佳实践

NSX共提供四种负载均衡算法：IP散列（基于客户机地址）、最小连接（基于会话数量）、交替（基于服务器权重即处理能力）和URI（基于访问的Web地址）。一般情况下，我们建议采用交替（Round_Robin）算法，并根据池中各服务器的处理能力高低设置合理的权重。

NSX提供三种健康检测方法：HTTP、SSL和TCP。对于Web服务来说，可选择HTTP或SSL（用于HTTPS协议）；对于非Web服务器，应采用基于TCP会话的可用性检测方法。
NSX提供三种会话保持机制选项：无、Cookie或Session ID