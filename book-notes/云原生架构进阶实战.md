## 云原生架构进阶实战
> 王玉平编著

### 前言

随着数字经济的发展和企业数字化转型的深入，企业业务部门需要越来越快的应用开发，传统的开发模式和运维模式节奏缓慢，这两者之间的矛盾越来越突出。为了解决这种矛盾，云原生概念于2013年首次提出，云原生计算基金会在2015年成立。2018年云原生生态不断壮大，主流云计算供应商纷纷加入该基金会。业界更是称2019年为云原生技术的商业化元年。云原生架构为传统云计算领域的人才和传统信息系统开发者带来了新挑战，对企业IT更是极大的挑战。

5章讲述云原生架构下的日志解决方案。日志是解决业务系统问题的溯源依据。本章介绍了在云原生架构下如何收集日志、存储日志，并介绍了伴生模式和DaemonSet模式。

第6章讲述如何监控云原生架构。通过介绍云原生架构下Prometheus监控指标收集系统，讲解了如何通过各类Exporter来收集指标，以及通过Grafana来图形化展示各类指标。

第7章重点描述了服务网格。云原生架构下的应用主要是以微服务形式提供的。随着微服务的发展与兴起，其规模也越来越庞大，服务治理的需求也越来越迫切。本章通过目前排名第一的Istio架构来阐述服务治理的各项要素，并介绍了分布式调用追踪、遥测度量收集、灰度发布应用、服务熔断和故障注入等常用场景。

### 第1章 云原生架构

利用虚拟化技术，也可以使得虚拟资源不受物理资源的限制，例如一台物理服务器可以虚拟成多台服务器，多台物理服务器也可以虚拟成一台服务器，从而整合硬件资源，提高资源利用率。

根据云计算服务提供的内容，业界把云计算分成三层：基础架构即服务（IaaS）、平台即服务（PaaS）和软件即服务（SaaS）。根据云计算服务提供的来源和服务对象，云计算分为公有云和私有云。

2006年Amazon（亚马逊）开始提供S3存储和EC2虚拟服务器的云服务；2009年Heroku提供了第一款PaaS云服务。2011年诞生了开源的IaaS实现——OpenStack，同年Pivotal开源了PaaS的实现——Cloud Foundry。2014年诞生了第一款商用函数即服务（FaaS），更加推动了云服务的深度应用。

为此，LXC（Linux Container）技术和Docker技术开始出现。它摒弃了启动完整系统的弊端，在现有操作系统上对任务进行隔离，并实现资源按需分配。它允许多个容器共享一个操作系统内核，容器内存储的仅仅是与某个应用紧密相关的资源，其空间占用往往只有几十到几百MB。单独容器化如同虚拟PC一样会面临高可用性不足、管理低级等问题。为此，业界推出了容器编排技术，其发展如图1-3所示。2014年Google开源了Kubernetes（简称K8S），它是一种容器编排工具。

随着虚拟化技术、云计算服务以及容器化技术的发展，越来越多的开发者和IT设施运维人员开始团队协作，改变过往独立运作的传统，开始对现有基础架构、运维人员的职责、开发团队的文化以及软件开发模式进行思考和研究，逐渐形成了云原生的概念。

### 1.2 什么是云原生

云原生（Cloud Native）概念是由Pivotal的Matt Stine在2013年首次提出的。这个概念得到了社区的不断完善，内容越来越丰富，目前已经包括了DevOps（Development和Operations的组合）、持续交付（Continuous Delivery，CD）、微服务（MicroServices）、敏捷基础设施（Agile Infrastructure）和十二要素（The Twelve-Factor App）等几大主题。这个概念不但包括根据业务能力对企业（高校）进行文化、组织架构的重组与建设，也包括方法论和原则，以及具体的操作工具。采用基于云原生的技术和管理方法，可以更好地从云中诞生业务，也可以把业务迁移到不同的云中，从而享受云的高效与持续服务的能力。

2015年云原生计算基金会（CNCF）成立，对云原生定义进行了修改，认为云原生需要包含应用容器化、面向微服务架构以及支持容器编排调度等方面的内容。

容器、服务网格、微服务、敏捷基础结构和声明性API都是这些方法的例证。这些技术能够构建可弹性扩展的、可管理的、可观察的松耦合系统。结合自动化手段，云原生技术可以使得开发者以最低的成本对系统进行频繁并可预测的重大变更。

### 1.3 云原生基础架构

例如Prometheus抓取监控信息的动作主要在配置文件中描述，但是如果Prometheus监控的目标是动态的，则需要部署人员每次去修改Prometheus配置文件，这是非常麻烦的，而且人工操作易出错。为此，Prometheus实现了一种服务发现方式，主动感知系统监控目标的变化，自动添加、删除和更新服务。以下是一个Kubernetes上的服务允许Prometheus自动发现的代码，其中的prometheus.io/scrape:"true"就是为了让Prometheus感知到该服务。该服务暴露了HTTP协议访问，端口为8080，端点为/metrics。


### 1.4 云原生应用

云原生应用程序的关键在于提供弹性、敏捷性、可操作性和可观察性。弹性的概念隐含了允许应用程序失败而不是试图阻止程序失败的意思。敏捷性允许应用快速部署和快速迭代，这就需要引入DevOps文化。可操作性是指从应用程序内部控制应用程序的生命周期，而不是依赖外部进程和监视器。可观察性是指应用程序需要提供信息以反映应用程序的状态。

单体应用程序的好处是显而易见的，但是它无法解决面向大量互联网用户提供服务的并发量问题，且使得开发过程变得臃肿，开发进程变得缓慢，维护也越来越困难。

分解单体应用为众多小的服务模块。如图1-5所示，这些服务模块相互独立，使得开发人员可以独立维护这些小系统，而且开发和维护过程也变得敏捷。分解成微服务后，各服务的编写语言也可以自行确定，只需要遵守总体的API优先和通信要求即可。

微服务更像是UNIX哲学的实践和改造。UNIX哲学是“程序应该只关注一个目标，并尽可能把它做好。让程序能够互相协同工作。”

1.4.2 健康状况报告
为了能够由软件控制一切，应用程序必须提供可供管理软件监测的度量指标。而一个应用程序的度量指标只有创建应用程序的作者最清楚，因此在应用程序中内置度量指标是最好的设计方式。这要求各应用程序提供必要的端点，供管理软件访问以判断应用程序状态。例如Kubernetes、ETCD都通过HTTP提供了大量的度量指标。

自动测量数据可以解决以下问题：
● 应用程序每分钟收到的请求数。
● 是否有任何错误。
● 应用程序延迟多久。
● 业务处理需要多长时间。

原生应用还需要设计一种方法应对过载，这也是互联网应用面临的常见问题，例如12306售票、淘宝双11活动时的并发访问过载。处理过载的一种常见方法是适度降级。

所以云原生应用要求具备服务优雅降级的能力。最现实的处理方式是服务降级，返回部分回应或使用本地缓存中的旧信息进行回应。

### 1.5 十二要素应用

这些要素代表了针对云环境中蓬勃发展的可移植弹性应用程序（特别是“软件即服务”应用程序）的一组准则或最佳实践。下面列出了这十二要素。

十二要素应用推荐将应用的配置存储于环境变量中，如图1-7所示。这允许应用程序非常方便地在不同的部署间修改，而不需要改动一行代码。例如wget用的环境变量HTTP_PROXY。还有一种做法是使用配置文件但把该配置文件从版本控制中排除。例如Drupal站点的settings.php文件，里面含有数据库连接信息，为了保证安全，必须将该文件排除在版本控制外。但是有时仍然难免将该文件加入版本控制，从而造成了数据库连接信息泄露。

图1-7 在环境中存储配置

构建阶段是指将代码进行编译、打包等操作，生成可执行文件。而发布则是将构建的结果和相关配置发布到运行环境中投入使用。运行阶段则是在执行环境中启动一系列应用程序。例如Node.js应用程序，其构建步骤较为简单，只需要复制相关文件即可。而发布到运行环境时，则通过npm install安装相关依赖项；在运行阶段可以通过Node.js进程管理工具pm2安装或者重启服务。

符合十二要素的应用程序的进程必须是无状态且无共享的。任何需要持久化的数据都需要存储在后端服务内。例如ApereoCAS，所有认证的Ticket均保存在后端数据库中，如memcached集群。而需要处理的是session状态，这个也是需要通过后端memcached或者redis进行统一存储，或者通过前端负载均衡粘性路由到同一个应用进程中。

7）通过端口绑定（Port binding）来提供服务
符合十二要素的应用程序可以自我加载而不依赖于任何网络服务器。例如Java代码可以直接使用JVM的Jetty，而不依赖于Tomcat。这一点就像Node.js不需要Apache一样。

如果查看常用软件的Dockerfile，会发现这些Docker的执行命令不再是以后端程序的方式运行，而是直接以前端运行。例如PHP的Docker镜像，命令为apache2-foreground。

（8）通过进程模型进行扩展
在十二要素应用中的进程主要借鉴了UNIX守护进程模型，不同的工作分配给不同类型的进程处理。尤其是无共享、水平分区的特性让并发处理更加简单。十

进程应当追求最短的启动时间，一旦接收到终止信号则会优雅地终止。进程还应当合理处理意外终止，例如可以在客户端断开或者超时连接后自动退回任务。

每个Docker中的应用不应该自己进行日志的管理，而应该直接提交给标准输出和标准错误事件流。

这段代码改造了Apache2的日志，默认情况下，Apache2将访问日志写入access.log文件，错误日志写入error.log，而通过ln软链接命令，实现了将这些日志流直接重定向为标准事件流。这样，这些应用的日志流将被Docker或者Kubernetes捕获。

要实现高质量的微服务环境，可以不用严格遵循这些要素。但是，通过牢记这些要素，用户可以在持续交付环境中构建和维护可移植应用程序或服务

### 1.6 实现云原生模式

如果一个应用程序比较复杂，则应该采用微服务模式，将复杂功能拆分为细微的服务，然后通过集成这些细微服务来组装成一个应用系统。但由微服务构成的如此复杂的系统，势必无法通过人工管理，应该采用自动化管理，这也是云原生应用的一个基本特征。

原生应用并不限定语言，它只对诸如弹性、服务发现、配置、日志记录、健康检查和度量检查等模式有要求。针对两种不同情况，目前常用的做法主要有两类。

1）单一语言情况下，可以通过导入标准库的形式在程序代码中声明云原生特征。例如Java语言系的Spring Cloud，可以通过类声明来实现配置、服务发现、熔断机制等功能。

（2）多语言情况下，则可以通过伴生（sidecar）模式来解决。该模式将实现各种功能的微服务应用通过容器化部属捆绑在一起。常见例子如Envoy proxy为服务增加弹性和指标

### 1.7 何时采用云原生

（1）企业的现有基础架构是否敏捷？所谓敏捷是指可以通过API来自动分配、销毁资源，也就是说，现有IaaS层是不是API可控制的。

换句话说，当应用不再关注底层系统的情况时，就可以尝试云原生架构。

（2）企业的开发团队、部署团队、运维团队是否独立？技能是否相通？如果多个团队各自独立运作，互相之间技术不了解，这种情况下采用云原生架构，则势必带来磨合成本和高昂的学习成本。如果采用云原生架构，将打破这种独立运作的组织架构，由传统纵向分割团队改为横向分割团队，每个小组可以自己负责纵向的所有事宜，从而更好地协作。

采用云原生架构的应用，就是为了快速响应业务变化，并快速部署应用，只有业务变化快的业务才能充分发挥云原生架构的作用。

适合云原生架构的应用必须职能单一、与系统无绑定，能够动态扩展实例数量。这就要求能够以无人值守的方式扩展应用实例，也要求状态持久化的存储服务独立于应用所在的服务器。符合十二要素是最好的实践。

### 1.9 服务网格（Service Mesh）

实际上，服务网格通常实现为多个轻量级网络代理，这些代理与应用程序代码一起部署，但不需要知道应用程序。

● 解耦应用程序的重试/超时、监控、追踪和服务发现。

Istio是一个源代码开放的平台，用于管理和保护微服务。Istio与编排器（如Kubernetes）配合使用，可管理和控制服务之间的通信。

Istio提供了一种机制，以集中方式配置路由和安全策略，以分散方式通过伴生来应用这些策略。

Istio从最高层上扩展了Kubernetes平台，提供了额外的管理概念、可视性和安全性。Istio的功能可以细分为以下四个类别：● 流量管理：控制微服务之间的流量，以执行流量分割、故障恢复和金丝雀发布。● 安全性：在微服务之间提供基于身份的强认证、授权和加密。● 可观察性：收集度量值和日志，以更好地了解集群中运行的应用程序。● 策略：强制实施访问控制、速率限制和配额，以保护应用程序。

### 1.10 云原生的未来

2019年5月，著名分析机构Gartner发布了一份有关云原生基础设施的报告。该报告指出容器和Kubernetes是构建云原生基础设施的关键，有助于提升软件效能和开发生产率。该报告预计2020年所有领先的容器管理软件均内置服务融合技术，到2022年，全球化企业在生产中使用容器化的应用比例将从当前不足30%!发(MISSING)展到75%!，(MISSING)应用软件采用容器化技术适应多云环境的比例从现在的少于20%!发(MISSING)展到50%!。(MISSING)

（2）在边缘计算中引入云原生技术，简化管理。（3）服务网格继续发展，并以Istio领先。

### 第2章 Kubernetes核心对象

Kubernetes主要提供了以下功能：
● 服务发现和负载均衡。Kubernetes可以使用DNS名称或使用自己的IP地址公开容器。如果容器的流量很高，Kubernetes也支持将这些流量均衡分配，以确保系统稳定。
● 存储编排。Kubernetes允许用户自动加载自选的存储系统，例如本地存储、NFS存储等。
● 自动部署和回滚。Kubernetes可以描述已部署容器的所需状态，并且可以以受控的方式更新现有状态到预期状态。

● 管理资源。Kubernetes允许用户指定每个容器请求的资源，以便更好地管理容器的资源。
● 自我修复。Kubernetes可以重新启动失败的容器、替换容器，也可以终止不响应用户自定义的运行状况检查的容器。
● 密钥和配置管理。Kubernetes允许用户存储和管理敏感信息，例如密码、TLS证书等。用户可以部署和更新机密信息而无须重建容器，也不会泄露机密信息。

### 2.1 Kubernetes架构

Kubernetes分控制节点（Master Node）和工作节点（Worker Node），如图2-1所示。其对外提供的操作接口全部通过API Server对接。无论是图形界面还是命令行工具，都是通过访问API Server与Kubernetes交互，这样就能符合云原生架构的要求——通过API来操作基础架构。

### 2.2 命名空间

4．命名空间和DNS
创建服务时，也会创建相应的DNS条目<service-name>.<namespace-name>.svc.cluster.local，这在跨命名空间中使用Service时就比较有用了，此时必须使用完全限定的域名（FQDN）。

### 2.3 Pod

● Kubernetes直接管理Pod，而不是容器。
● Pod可以封装多个协作的容器，类似于Docker Stack。


图2-2演示了一个Pod内有两个容器，分别是Drupal站点和Drupal后端数据库。但在这种模式下无法横向扩展，只有Drupal站点系统和MySQL数据库分离后在不同的Pod中才能进行扩展。

在本书所提及的Elasticsearch部署案例中，使用了三个初始化容器，按照以下顺序执行：
● 修正存储卷的权限。
● 修改系统参数vm.max_map_count。
● 修改系统参数ulimit的值。
描述初始化容器：

[插图]

### 2.4 部署

Kubernetes中的部署是指如何控制一组Pod的运行状态使其满足生产需要。部署分为无状态部署和有状态部署。无论哪种部署，其内部都有副本集控制功能。首先了解ReplicaSet是如何控制副本数量的。

部署（Deployment）是一个拥有ReplicaSet并通过声明来控制Pod滚动、更新的对象。虽然ReplicaSet可以独立使用，但是它主要被用作协调Pod创建、删除和更新的机制。使用部署时，用户不必担心副本集，因为部署通过ReplicaSet进行管理，所以，本书建议使用部署而不是ReplicaSet。

.spec.strategy.rollingUpdate.maxSurge也是一个可选字段。它指定了Pod副本的最大副本数量。例如取默认值为25%!，(MISSING)则在更新部署时可创建的Pod副本数量是期望副本数量的125%!。(MISSING)

下面是影响升级过程的几个重要参数：
（1）新的ReplicaSet创建maxSurge所指定数量的Pod副本，此时新旧Pod副本数量为期望副本数量和maxSurge个副本数量之和。

2.4.3 有状态部署有状态部署（StatefulSets）是用于管理有状态应用程序工作负载的一种部署方法。区别于无状态部署（Deployment），StatefulSets可以对Pod副本进行排序并保证每个Pod的唯一性。在无状态部署中，删除一个Pod，新建一个Pod，对于所有Pod来讲都是一致的。但是在StatefulSets中，每个Pod都有唯一的持久标识符，即使在重建后也会继续保留该标识符。

2.4.4 DaemonSet一个DaemonSet确保每个节点上只运行一个副本。若有节点添加到集群中，将在新增的节点上运行唯一一个Pod，若节点删除，则从该节点上终止Pod。

DaemonSet适合以下场景：● 每个节点上运行集群存储后台驻留程序，例如glusterd、ceph。● 每个节点上运行日志守护程序，例如fluentd、logstash。● 每个节点上运行Prometheus node exporter、sisdig、collectd等代理。

### 2.5 服务

Kubernetes服务（Service）定义了这样一种抽象：在逻辑上通过标签来选定一组Pod，提供一种策略，让外网和前端客户能够访问这组Pod。

在Headless Service中，.spec.clusterIP需要设置为“None”。这也就决定了Headless Service只能在集群内部使用。
Headless Service可以有选择器，也可以没有选择器。一般情况下服务都会配置选择器。通过配置选择器，服务会在DNS中为每个Pod配置A记录，通过访问服务名称，DNS会返回后端Pod的地址，这可以用于Pod之间通过域名互相访问。

● ClusterIP：通过集群的内部IP暴露服务。选择该值，服务只能在集群内部访问。
● NodePort：通过在每个节点上创建相同的端口来暴露服务。NodePort服务会路由到ClusterIP服务，这个ClusterIP会自动创建，也可以手工指定。
● LoadBalancer：使用云提供商的负载均衡设备向外暴露服务。

● ExternalName：通过返回CNAME和它的值，可以将服务映射到externalName字段指定的内容。

5．外部IP
可以在服务中设置外部IP，将服务暴露给这个外部IP，从而通过外部IP进入集群。外部IP必须和节点IP在同一个网段。


Service是工作在四层上的服务，在Kubernetes中还有一个工作在七层上的Ingress API对象。它主要将HTTP和HTTPS协议从外部路由到集群内部的Service。

Ingress可以为内部服务提供外网URL，也可以提供流量负载均衡功能、终止SSL/TLS功能以及基于名称的虚拟主机功能。
注意：Ingress本身不会暴露任意端口和协议，只有暴露HTTP和HTTPS协议时才需要Ingress。另外它需要借助Ingress控制器来使用，单独创建一个Ingress资源是无效的。

### 2.6 存储

Kubernetes中的存储是以卷来描述的。卷有静态卷和动态卷两种。云原生基础架构下，大多数场景需要使用动态卷，因此本节只讲述动态卷。

持久卷声明（persistentVolumeClaim）是指在创建Pod时根据storageClass动态申请卷，然后加载到Pod中。这是在云原生架构下强烈建议的方式。下面示例是申请1GB存储类名为nfs-storage的仅供一台机器读写的存储空间，并将其挂载到Pod中的/usr/share/nginx/html路径。

如果不用subPath方式，管理人员需要创建两个持久卷，如果采用subPath方式，则只需要创建一个持久卷，并在其中创建两个子目录html和mysql即可。

### 2.7 RBAC

基于角色的访问控制（Role-based access control, RBAC）是Kubernetes基于某个用户的角色来控制对资源访问的方法。

在RBAC中，角色包含表示一组权限的规则。角色是指在某个命名空间下的权限规则集合，而集群角色则是面向整个集群的权限规则集合。例如cluster-admin角色是整个集群的管理员角色。

角色绑定是将角色中定义的权限授予用户或用户组。

集群角色绑定类似，只是kind字段的值需要变更为ClusterRoleBinding，且roleRef需要引用集群角色。
注意：完善的云原生架构应指定必要的服务账号，并合理分配权限。

### 第3章 敏捷基础架构

敏捷基础架构的目的是通过代码来自动化动态完成服务器部署、更新，以及存储和网络资源的动态分配，这样运维人员可以像开发软件系统一样快速迭代，从而迅速满足各项工作负载的即时需求。

### 3.1 部署本地Repository

因此，在一个完善的DevOps环境下，必须对常用的Yum、Maven、Nuget、NPM和Docker实施本地库镜像。
部署本地Repository的方法有很多种，本书选用了Sonatype Nexus Repository Manager

每一类服务，都可以创建如下三种类型的Repository。
● Proxy，对指定URL的库提供代理服务，并进行缓存。
● Hosted，本地存储包，提供私有包的存储和发布服务。
● Group，组合Proxy和Hosted同种格式在一起，通过同一个URL端点对外提供服务。

使用Repository的常见方法是：先建立一个Hosted类型的库，然后创建一个指向官方Repository的Proxy类型的库，最后创建一个包含刚创建的Hosted类型和Proxy类型的Group类型。一般发布给用户作为读取的库只需要知道Group类型的库就可以了。如果要发布包到本地库中，则需要告知用户Hosted库的访问URL，并做好权限控制。

由于作为本地镜像缓存，需要较大的存储空间，本例中单独挂载/data目录。

6．设置代理服务器
有些网络环境是不能直接访问外网资源的，需要借助代理才可以访问外网。在Docker中设置服务代理是通过修改服务配置来实现的。有三个环境变量可以设置：HTTP_PROXY、HTTPS_PROXY和NO_PROXY。为了防止系统使用代理访问内网，需要设置正确的NO_RPOXY值。

3.1.2 部署nginx和sonatype/nexus3
接下来，将在前端部署一个nginx作为反向代理，对外提供80和443端口服务。这是因为有很多防火墙策略仅允许80和443端口的协议通过。


在虚拟机中运行以下命令来启动Nexus服务栈。
￼

### 3.2 部署Kubernetes

8．系统限制
在Kubernetes中，一个节点上会有很多个容器，需要打开的文件也就比单独一个操作系统时要多，因此需要修改以下系统限制参数。


### 3.7 升级Kubernetes

如果Master节点不止一个，则需要同样操作，但是上面的命令kubeadm upgrade apply需要替换成kubeadm upgrade node。


### 第4章 DevOps实战

DevOps是开发（Development）和运维（Operations）的组合词，它是一种重视软件开发人员和IT运维技术人员之间沟通合作的文化、流程以及平台和工具。通过自动化软件交付和架构变更流程，使得构建、测试、发布软件能够更加快速、频繁和可靠。

### 4.1 DevOps简介

DevOps过程是快速迭代的过程，从计划阶段开始，进入代码编写，然后进行构建，对新构建系统进行测试、发布、部署，再进入运维阶段，经过监控和度量后

开发人员和运维人员的沟通协作。因为开发人员和运维人员都在这个快速迭代过程中，必须能够顺畅地沟通，才能使得DevOps过程完美地运作。这也是DevOps文化概念的由来。

3．构建服务器
构建服务器的功能很简单，主要是构建源代码。常用模式是当版本控制系统中有代码提交时，就触发构建服务器进行源代码构建。常用软件有Jenkins和GitLab Runner。

4．工件库
当构建服务器确认了代码质量并进行构建后，构建结果应当存放在一个工件库中。本书第2章讲述的Sonatype Nexus就是一个工件库管理系统。NPM Repository、Docker Registry、Maven2 Repository都是针对不同开发语言或者对象的工件库。

发布过程亦应自动化进行。例如在GitLab中可以通过GitLab Runner进行发布，也可以借助Jenkins、Ansible、Puppet等工具自动化发布。

云原生架构的DevOps变成了应用开发者和基础架构运维者之间的沟通，他们各自维护自己服务的生命周期，通过专业性提高效率，并且通过统一技术语言（如Kubernetes、容器化、微服务架构等）来进行沟通。

### 4.2 软件部署策略

目前应用部署策略主要有六种：重建部署、滚动部署、蓝绿部署、金丝雀部署、A/B部署以及影子部署。

4.2.2 重建部署（Recreate）
重建部署策略是最简单的一种部署方式。如图4-2所示，它采取的策略是先停止在线业务，然后部署新版本。这也是业界常用的一种策略，企业一般会选择业务空闲时进行升级，并事先告知客户。

4.2.4 蓝绿部署（Blue/Green）
蓝绿部署策略是指在不影响现有业务的情况下新增服务器进行新版本部署，待部署完成测试正常后，通过负载均衡设备将流量切换到新增服务器上，然后删除老版本服务器。这种策略适合业务比较重要且支持一次性切换的场景。

在Kubernetes中，要实现金丝雀部署有很多种方法，简单的方法可以通过Deployment的Replicas来控制，更高级的方法则是通过Istio或者负载均衡设备来实现。

4.2.6 A/B测试
A/B测试部署策略是在金丝雀部署策略的基础上，根据特定条件来分发流量。例如将来自某些IP或者某些用户的流量路由到新版系统上去，因带有一定的测试意味，故而起名A/B测试。这个部署策略在一定范围内测试新版功能的场景下比较适用。企业进行底层支持系统的切换（如单点登录系统、基于微服务的支撑系统等）时可以采取该策略。

4.2.7 影子部署
影子部署策略是指同时部署了新旧两套系统，流量同时分发给新旧两套系统，但是新系统处理后的结果并不会返回给用户。这种方式主要是起到模拟作用，用来测试新版系统的性能、功能是否能够满足现有的流量需求。这种部署设置起来相当复杂，需要特殊要求，尤其是出口流量。

### 4.3 部署GitLab

GitLab系统就是在这种需求场景下诞生的。GitLab也是使用Ruby on Rails语言编写的，具备Wiki和持续集成的功能，便于企业或组织在内部使用。自托管和丰富的功能性使其成为企业内部实施DevOps时的首选工具。

GitLab属于基础设施，如果部署在Kubernetes，则存在技术依赖性。中小规模企业或团队是否应该在Kubernetes中部署生产用GitLab值得商榷。本书建议先在操作系统或Docker中尝试使用GitLab，待时机成熟或GitLab性能需求较高时再迁移至Kubernetes集群。

### 4.4 GitLab集成自动CI/CD

持续集成是指针对每次推送到代码库中的代码，通过一组脚本来自动构建、测试，从而减少向应用程序引入错误的可能性。

持续交付是持续集成的一个步骤。代码推送到代码库后通过一系列构建和测试，然后进行持续部署，如果部署是手动触发的，称为持续交付；如果是自动触发的，则称为持续部署。

### 4.5 容器部署模式

先举一个日志的例子。日志不是业务的核心，只是事后分析问题的依据，所以它不应该和应用系统放在一起。按照传统应用部署的方式，用户在同一台服务器里既部署业务系统，又部署一个日志代理，例如fluentd。如图4-19所示，若采用sidecar模式，则业务系统和日志代理分属不同的容器，两者之间进行了隔离。

（2）保持服务配置更新。如果企业使用Nginx、Apache、Haproxy等服务器软件，配置可能需要更新。借助sidecar设计模式，sidecar容器可以去配置中心下载配置文件，然后向主业务容器发送重新加载配置的信号。

### 第5章 日志记录

日志作为一种特殊的数据，对处理历史数据、诊断问题以及了解系统活动等有着非常重要的作用。数据分析人员、开发人员和运维人员都需要对日志进行分析。

### 5.1 模式

5.1.1 伴生模式
第4章末尾讲述了伴生模式的弊端——伴生容器数量随主容器数量的增加而增加。

5.1.2 DaemonSet模式
由于伴生容器是基于同一个镜像产生的，且功能具有重复性，因此可以考虑在一个node上只部署一个容器，然后通过代理或其他机制把本应伴生容器做的工作中继给这个容器。

### 5.2 日志采集

5.2.2 DaemonSet模式下配置Fluentd
在DaemonSet模式下，每个节点上只有一个Fluentd容器。如图5-4所示，节点上的业务容器都会写日志到节点的/var/log目录下，每个节点上的Fluentd容器加载该目录并读取其中的内容，然后推送到Elasticsearch，Elasticsearch将日志存储在后端。通过该种模式，kubectl logs命令依然可以查看每个Pod的日志。此外，一个节点一个Fluentd容器也提高了资源利用率。所以这是最佳实践的一种方式。

### 5.3 部署Elasticsearch

5.3.1 Elasticsearch简介
Elasticsearch是当前全文搜索引擎的首选。它可以快速地储存、搜索和分析海量数据。维基百科、StackOverflow、GitHub等著名社区都采用它进行检索。Elasticsearch的底层是开源的Lucene，Elasticsearch是对它的封装，并提供了RESTful API。Elasticsearch部署简易，开箱即用。

initContainers中执行了3项必需的操作：
● 修正数据卷权限：根据官方文档，Elasticsearch运行时是以ID为1000的用户身份在运行，所以需要修正elasticsearch数据卷的权限。
● 增加max_map_count。
● 设置ulimit：Elasticsearch需要设置系统的ulimit为65536。

6．Pod中断预算
中断预算（Disruption Budget）主要是限制应用程序的并发中断数量以保证应用的高可用性。

### 第6章 云原生下的监控

日志记录一般使用ELK套件（Elasticsearch+Logstash+Kibana），也可以使用第5章讲述的EFK套件（Elasticsearch+Fluentd+Kibana）。监控则使用GPE套件（Grafana+Prometheus+Exporter）。

### 6.1 Prometheus简介

Prometheus是一个开源的监视和警报工具。2012年，Google前员工受到Borgmon监控系统的启发，在开源社区创建了该项目。2015年，Prometheus正式发布。

Prometheus生态圈中包含了多个组件，其中许多组件是可选的。
● Prometheus server：用于收集和存储时间序列数据。
● 客户端库（Client Library）：为需要监控的服务生成相应度量值并暴露给Prometheus server。当Prometheus server来抓取时，直接返回实时状态的metrics。


● Exporters：用于暴露已有第三方服务的度量给Prometheus。
● 报警管理器（Alert manager）：从Prometheus server端接收到警报后，会去除重复数据、分组，并路由到报警服务，发出报警。常见报警方式有：电子邮件、短信、OpsGenie、钉钉、webhook等。

但是若企业需要100%!准(MISSING)确度，例如按请求计费，Prometheus并不是一个好选择，因为它收集的数据可能不够详细和完整。在这种情况下，企业最好使用其他系统来收集和分析数据以进行计费，并使用Prometheus进行监控。

### 6.2 使用Exporter采集数据

在Prometheus架构中，Prometheus Server并不直接监控目标，它主要负责数据的收集、存储并且对外提供数据查询支持。因此，为了能够监控到目标的某些指标，如主机的CPU使用率，Prometheus需要使用Exporter。可以这样讲，所有向Prometheus提供监控样本数据的程序都可以被称为一个Exporter。Prometheus周期性地从Exporter暴露的HTTP服务地址（通常是/metrics）抓取监控样本数据。

Exporter的一个实例称为目标（Target），Prometheus server周期性地轮询这些Target以获取监控样本数据。图6-2示意了Prometheus server每隔5s轮询3个目标（Target）。

Prometheus社区提供了大量Exporter实现，基本上都是用Go语言编写的。这些Exporter涵盖了基础设施、中间件以及网络等多个目标的监控。

### 6.3 在Kubernetes中部署Prometheus

6.3.1 创建RBAC文件
在Kubernetes集群中为Prometheus创建单独的服务账户，并创建一个集群角色，分配合理权限，然后绑定该用户到该集群角色。prom-rbac.yaml文件内容如下：

添加了kubernetes-apiservers通过API Server抓取Kubernetes集群的整体运行信息；添加了kubernetes-service-endpoints抓取服务端点的信息；

### 第7章 服务网格应用

总结了常见“分布式系统的误区”。主要有以下几个方面：
● 网络是可靠的：任何组件和基础设施都会发生故障，当规模足够大时，这将成为必然。
● 可忽略的延迟、宽带是无限的：当业务高峰时，网络资源不足会导致系统稳定性下降。

Istio、Linkerd作为服务网格技术的代表作，通过sidecar代理拦截了微服务之间的所有网络通信，用统一方式实现服务之间的负载均衡、访问控制、速率限制等功能。应用无须了解底层服务访问细节，sidecar和应用可以独立升级，实现了应用逻辑与服务治理能力的解耦。

### 7.1 Istio架构

在Kubernetes中，代理被注入到Pod中，通过编写iptables规则来捕获流量。注入sidecar代理到Pod中并且修改路由规则后，Istio就能够调解所有流量。这个原则也适用于性能。当Istio应用于部署时，运维人员可以发现，为提供这些功能而增加的资源开销很小。所有组件和API在设计时都必须考虑性能和规模。

Istio中包含四种流量管理配置资源，分别是Virtual Service、Destination Rule、Service Entry以及Gateway。下面会讲解这几个资源的部分重点。
● Virtual Service在Istio服务网格中定义路由规则，控制路由如何路由到服务上。
● Destination Rule是Virtual Service路由生效后，配置应用与请求的策略集。
● Service Entry通常用于在Istio服务网格之外启用服务的请求。
● Gateway为HTTP/TCP流量配置负载均衡器，最常见的是在网格边缘的操作，以启用应用程序的入口流量。

Virtual Service定义了在Istio服务网格中如何控制服务请求的路由规则。例如一个Virtual Service可以把请求路由到不同版本，甚至可以路由到一个完全不同于请求要求的服务上去。路由可以用很多条件进行判断，例如请求的源和目的地、HTTP路径和Header以及各个服务版本的权重等。

Gateway为HTTP/TCP流量配置了一个负载均衡，多数情况下在网格边缘进行操作，用于启用一个服务的入口（ingress）流量。
和Kubernetes Ingress不同，Istio Gateway只配置四层到六层的功能（例如开放端口或TLS配置）。绑定一个Virtual Service到Gateway上，用户就可以使用标准的Istio规则控制进入的HTTP和TCP流量。

### 7.5 总结

服务网格的引入有利于微服务治理，并在不侵入应用程序的情况下实现微服务治理，回避了微服务的多元化问题，提高了维护的简易性。

### 8.1 在Kubernetes中部署Drupal 8站点

Drupal连续多年荣获全球最佳CMS大奖，与WordPress同属于业内排名前二的内容管理系统。著名用户包括联合国、美国商务部、纽约时报、华纳、迪斯尼、联邦快递、索尼、美国众多（包括常青藤联盟在内）大学、Ubuntu等。

### 8.2 云原生架构下的Node.js自动CI/CD方法

K8S_SERVER：Kubernetes API Server，在本例的环境中是k8s.shmtu.edu.cn。
K8S_USER_TOKEN：连接Kubernetes集群的用户令牌。获取用户TOKEN的方式如下：
￼
K8S_CA_PEM：Kubernetes集群的CA证书，获取命令如下：

3．修改.gitlab-ci.yml文件
最后，需要修改.gitlab-ci.yml文件，增加部署（deploy）阶段的方法是：通过kubectl config设置连接Kubernetes的相关信息；替换部署文件中的版本号；应用部署文件。

### 8.3 Apereo CAS自动横向缩放部署策略

为了节约资源，并应对高并发量的情况，可以充分利用Kubernetes自动水平Pod伸缩的特性提高CAS的可用性。通过以下命令可以根据容器的CPU使用率进行伸缩。该命令指明，如果容器CPU使用率超过80%!，(MISSING)则增加Pod，最多增加至10个Pod；如果CPU使用率降低，则自动释放部分Pod，Pod数量不少于2个。
￼

### 8.4 Apache Kafka部署与使用

Kafka也被越来越多的厂商选为消息中间件。
Kafka是基于发布/订阅的消息系统，主要设计目标为：
● 以时间复杂度为O(1)的高效方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间复杂度的访问性能。
● 高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条以上消息的传输。

1．CPU
Kafka对CPU的要求并不高，只有在需要TLS加密的情况下才有额外的CPU消耗，但这可以在Kubernetes中通过服务来接管TLS，从而节约CPU。

Kafka可以持久化消息队列，这样一个Kafka实例重启后可以将消息队列重新加载。若使用emptyDir方式，则Pod重启后消息队列将丢失。所以在生产中需要创建持久卷以保存消息队列。
注意：不建议使用NFS存储，也不建议使用本地存储，应使用共享存储，以便Pod恢复。

与大多数分布式系统一样，Kafka依赖低延迟、高带宽的网络。为了避免Kubernetes节点故障，不能把所有kafka Pod放置在同一个节点上，必须分散在Kubernetes集群中不同的节点上

由于Helm的某些限制，Operator变得非常流行。Operator不仅为Kubernetes打包软件，还可为Kubernetes部署和管理软件。


Operator是一种打包、部署和管理Kubernetes应用程序的方法。Kubernetes应用程序是既部署在Kubernetes上又使用Kubernetes API和kubectl工具进行管理的应用程序。为了能够充分利用Kubernetes，需要扩展一组内聚的API以服务和管理在Kubernetes上运行的应用程序。可以将Operators视为在Kubernetes上管理此类应用程序的运行时。

### 8.5 云原生应用架构在上海海事大学信息化建设中的实践

GitLab支持自动CI/CD，并且支持Kubernetes集群，这为软件系统的部署提供了最大程度的自动化和最小的成本代价。持续交付基本架构如图8-6所示。