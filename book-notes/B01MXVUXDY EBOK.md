## B01MXVUXDY EBOK
> Unknown

### What this book covers

It also covers Kubernetes scaling testing and tooling for stress testing.

### 1. Understanding Kubernetes Architecture

But to understand what Kubernetes is all about, how to use it effectively, and what the best practices are, requires much more.

### Understanding container orchestration

That means making sure that all the containers that execute various workloads are scheduled to run physical or virtual machines.

In addition, Kubernetes must keep an eye on all running containers and replace dead, unresponsive, or otherwise unhealthy containers.

### Physical machines, virtual machines, and containers

It all starts and 
ends with hardware. In order to run your workloads, you need some real hardware provisioned. 

### Containers in the cloud

The great thing about Kubernetes is that it can be deployed on all those clouds. Kubernetes has a cloud provider interface that allows any cloud provider to implement it and integrate Kubernetes seamlessly.

### Cattle versus pets

I remember that, in many of the companies I worked for, we had multi-day discussions to decide on a naming theme for our servers. F

### Cluster

Note that your entire system may consist of multiple clusters. We will discuss this advanced use case of federation in detail later.


### Node

A node
 is a single
 host. It may be a physical or virtual machine. Its job is to run pods. Each Kubernetes node runs several Kubernetes components, such as a kubelet and a kube proxy. 

In the past they were 
called minions. If you read some old documentation or articles, don't get confused. Minions are nodes.


### Pod

Pods provide a great solution for managing groups of closely related containers that depend on each other and need to co-operate on the same host to accomplish their purpose. 

### Label

Labels
 are key-value
 pairs that are used to group together sets of objects, very often pods.

Note that labels are dedicated for identifying objects and not for attaching arbitrary metadata to objects. This is what annotations are for (see the following section).

### Label selector

Label selectors
 are used to select
 objects based on their labels.

Label selectors can have multiple requirements separated by a comma. For example:

### Service

Services are
 used to expose
 some functionality to users or other services. 

### StatefulSet

Pods come and go, and if 
you care about their data then you can use persistent storage. 

•  A stable hostname, available in DNS
•  An ordinal index
•  Stable storage linked to the ordinal and hostname

### Secret

Secrets
 are small 
objects that contain sensitive info such as credentials and tokens. They are stored as plaintext in etcd, accessible by the Kubernetes API server, and can be mounted as files into pods (using dedicated secret volumes that piggyback on regular data volumes) that need access to them. 

Note that secrets in a pod are always stored in memory (tmpfs in the case of mounted secrets) for better security.


### Name

If you delete an object, you can create another object with the same name as the deleted object, but the UIDs must be unique across the lifetime of the cluster. The UIDs are generated by Kubernetes, so you don't have to worry about it.


### Namespace

A namespace
 is a virtual cluster. 

Each virtual cluster is totally isolated from other virtual clusters, and they can only communicate through public interfaces. Note that Node objects and persistent volumes don't live in a namespace. Kubernetes may schedule pods from different namespaces to run on the same node. 

When using namespaces, you have to consider network policies and resource quotas to ensure proper access and distribution of the physical cluster resources.


### Sidecar pattern

 great example is a central logging agent. Your main container can just log to stdout, but the sidecar container will send all logs to a central logging service where they will be aggregated with the logs from the entire system. The benefits of using a sidecar container versus adding central logging to the main application container are enormous.

### Kubernetes components

A Kubernetes cluster 
has several master components used to control the cluster, as well as node components that run on each cluster node. Let's get to know all these components and how they work together.

### API server

The kube API 
server exposes the Kubernetes REST API. It can
 easily scale horizontally as it is stateless and stores all the data in the etcd cluster. The API server is the embodiment of the Kubernetes control plane.

### What is a CI/CD pipeline?

A CI/CD pipeline
 is a set of steps that a set of changes by developers or operators that modify the code, data or configuration of a system, test them and deploys them to production. Some pipelines are fully automated and some are semi-automated with human checks.

In large organizations, there may be test and staging environments where changes are deployed to automatically, but release to production requires manual intervention. 

### 2. Creating Kubernetes Clusters

•  Creating a single-node cluster with Minikube


### Getting ready

Type k version to verify kubectl is correctly installed and functioning:



### Creating the cluster

Let's review what
 Minikube did by following the output. You'll need to do a lot of it when creating a cluster from scratch:

### Troubleshooting

If something goes wrong 
during the process, try to follow the error messages. I ran into several issues 

### Checking out the cluster

First, let's ssh into the VM:

Next, let's check out the nodes in the cluster using get nodes:

### Doing work

To expose our pod as a service, type the following:



To access the service, we need the cluster IP and exposed port:


### Installing the required software

 like Ansible a lot for 
configuration management. 

### The vars.yml file

The vars.yml file just keeps a list 
of the packages I want to install on each node.

The others are required by Kubernetes:

### The playbook.yml file

If you run into connection failures, try again. The Kubernetes APT repo is sometimes slow to respond. You need to do this just once (per node).


### Setting up the pod network

To verify:


### Adding the worker nodes

You should see the following:



### Creating clusters in the cloud (GCP, AWS, Azure)

But, in the end, Kubernetes is designed for cloud-native applications (applications that run in the cloud). Kubernetes doesn't want to be aware of individual cloud environments because that doesn't scale.

### GCP

The fact that Kubernetes is a built-in part of the GCP means it will always be well integrated and well tested, and you don't have to worry about changes in the underlying platform breaking the cloud-provider interface.

### The process

•  Cluster services: DNS, logging, monitoring, and GUI


### 3. Monitoring, Logging, and Troubleshooting

Once the cluster is up and running, you need to make it sure it is operational, all the necessary components are in place and properly configured, and that enough resources are deployed to satisfy the requirements. 

Responding to failures, debugging, and troubleshooting is a major part of managing any complicated system, and Kubernetes is no exception.

### Troubleshooting scenarios

There are so many things that can
 go wrong in a large Kubernetes cluster – and they will. This is expected.

### Designing robust systems

When you want to design 
a robust system you first need to understand the possible failure modes, the risk/probability of each failure, and the impact/cost of each failure. 

### Hardware failure

When the node is not responsive it can
 be difficult sometimes to determine if it's a networking issue, a configuration issue or actual hardware failure. 

### Quotas, shares, and limits

•  Insufficient resources: If a pod requires a certain amount of CPU or memory and there is no node with available capacity then the pod can't be scheduled.

### Bad configuration

In some data-driven systems, configuration is stored in various data stores. Configuration issues are very common because usually, there aren't any established good practices to test them. 

•  Incorrect labeling of nodes, pods, or containers
•  Scheduling pods without a replication controller
•  Incorrect specification of ports for services
•  Incorrect ConfigMap

Most of these problems can be addressed by having a proper automated deployment process. But you must have a deep understanding of your cluster architecture and how Kubernetes resources fit together.

Configuration problems typically occur after you change something. It is critical, after each deployment or manual change to the cluster, to verify its state.


 suggest starting from the services and verifying that they are available, responsive, and functional. Then, you can dive deeper and verify that the system also operates within the expected performance parameters.

### Managing cost on the cloud

The root cause can be external (a botnet attack), misconfiguration, an internal test gone awry, or a bug in the code that detects or allocate resources.


### Summary

In this chapter, we looked at monitoring, logging, and troubleshooting. This is a crucial aspect of operating any system,

### 4. High Availability and Reliability

In the previous chapter, we looked at monitoring your Kubernetes cluster, detecting problems at the node level, identifying and rectifying performance problems, and general troubleshooting.


 this chapter, we will dive into the topic of highly available clusters. 

This is a complicated topic.

There are many aspects to highly available Kubernetes clusters, such as ensuring that the control plane can keep functioning in the face of failures, protecting the cluster state in etcd, protecting the system's data, and recovering capacity and/or performance quickly. 

How to design and implement a highly available Kubernetes cluster will depend on those requirements.

### High-availability concepts

Networks will fail; configuration will be wrong; software will have bugs; people will make mistakes. Accepting that, we need to design a system that can be reliable and highly available even when components fail.

detect component failure, and replace bad components quickly.


### Hot swapping

 Clients can retry failed requests and the hot-swapped component will service them.

### Smart load balancing

Load balancing is about distributing
 the workload across
 multiple components that service incoming requests. 

### Idempotency

The end result is that the same work may be performed twice. It is very difficult to avoid this situation.

### Self-healing

Self-healing starts with automated detection of problems followed by automated resolution. Quotas and limits help create checks and balances to ensure an automated self-healing doesn't run amok due to unpredictable circumstances such as DDOS attacks.


we will apply them and demonstrate best practices for systems deployed on Kubernetes clusters.


### Creating highly available clusters

That means etcd must be deployed as a cluster (typically across three or five nodes) and the Kubernetes API server must be redundant.

You may prefer, for example, to deploy a standalone etcd cluster to optimize the machines to their workload or if you require more redundancy for your etcd cluster than the rest of the master nodes.


### Making your nodes reliable

Nodes will fail, or
 some components will fail, but many failures are transient. The basic guarantee is to make sure that the Docker Daemon and kubelet restart automatically in case of a failure.


If you run CoreOS, a modern Debian-based OS (including Ubuntu >= 16.04), or any other OS that uses systemd as its init mechanism, then it's easy to deploy Docker and the kubelet as self-starting Daemons:


### Protecting your cluster state

The Kubernetes cluster state
 is stored in etcd. The etcd cluster was designed to be super reliable and distributed across multiple nodes. 

### Clustering etcd

There are several methods to accomplish that.

### Etcd discovery

Here is the command:


When working with a discovery service, you need to pass the token as a command-line argument:

You can also pass it as an environment variable:

### DNS discovery

The details are outside the scope of this book.

### The etcd.yaml file

Let's look at the different parts of the etcd.yaml manifest file:

The initial section contains the name of the pod, specifies that it uses the host network, and defines a container called etcd-container. 

### Verifying the etcd cluster

Verifying the etcd cluster

Once the etcd cluster is up and 
running you can access the etcdctl tool to check on the cluster status and health. Kubernetes lets you execute commands directly inside pods or container via the exec command (similar to docker exec).


Recommended commands are as follows:

### etcd 2 versus etcd 3

Hopefully, by the time you read this v3 will be the official version. If not, it is possible to migrate etcd v2 to etcd v3.

### Protecting your data

Protecting the cluster state and
 configuration is great, but even more important is protecting your own data.

But if your own data is corrupted or lost, you're in deep trouble. The same rules apply; redundancy is king. B

### Running redundant API servers

Once you have multiple API servers running you can put a load balancer in front of them to make it transparent to clients.


### Running leader election with Kubernetes

The correct way to have a highly scalable Kubernetes cluster is to have these components run in leader election mode. This mean that multiple instances are running, but only one is active at a time and if it fails, another one is elected as leader and takes its place.


There is a documented procedure for supporting leader election for your application via the leader-elector container from Google. The basic concept is to use the Kubernetes endpoints combined with ResourceVersion and Annotations. When you couple this container as a sidecar in your application pod, you get leader-election capabilities in a very streamlined fashion.


It is very useful to parse the output of the Kubernetes API or kubectl:

### Making your staging environment highly available

unless you're Netflix, where you test in production).

The essential point is that, just like anything else, if you don't test it, assume it doesn't work.


We've established that you need to test reliability and high availability. The best way to do it is to create a staging environment that replicates your production environment as closely as possible. This can get expensive. There are several ways to manage the cost:

•  Ad hoc HA staging environment: Create a large HA cluster only for the duration of HA testing

At the end of your performance and stress tests, overload the system and see how the reliability and high availability configuration handles the load


### Testing high-availability

if the tests pass, you'll be confident that the system behaves as expected.

There may be many other things you want to test, such as whether the problem was logged, whether relevant alerts went out to the right people, and whether various stats and reports were updated.

### Live cluster upgrades

One of the most complicated 
and risky tasks involved in running a Kubernetes cluster is a live upgrade. 

Large clusters with many users can't afford to be offline for maintenance. 

Microservice architecture helps a lot here. You never upgrade your entire system. You just constantly upgrade several sets of related microservices, and if APIs have changed then you upgrade their clients, too. 

In this section, we will discuss how to go about upgrading your cluster using various strategies, such as rolling upgrades and blue-green upgrades. 

Then we will get into the critical topic of schema and data migration.

### Rolling upgrades

Rolling upgrades are
 upgrades where you gradually upgrade components from the
 current version to the next. This means that your cluster will run current and new components at the same time. There are two cases to consider here:


But, the deployment resource introduced in Kubernetes 1.2 makes it much easier and supports replica sets. It has the following capabilities built-in:

Here is a sample manifest for a deployment that deploys three nginx pods:

To start the rolling update, you create the deployment resource:



You can view the status of the deployment later:

$ kubectl rollout status deployment/nginx-deployment

### Blue-green upgrades

With a blue-green release, you prepare a full copy of your production environment with the new version. Now you have two copies, old (blue) and new (green). It doesn't matter which one is blue and which one is green.

You can run all your tests on green. Once you're happy, you flip the switch and green becomes active. If something goes wrong, rolling back is just as easy; just switch back from green to blue. 

### Migrating data

Data migration is a big deal. 

The essential point is that if you have a lot of data and you need to migrate it, it can take a while. In a previous company, I oversaw a project to migrate close to 100 terabytes of data from one Cassandra cluster of a legacy system to another Cassandra cluster.


The legacy system was still in place side-by-side with the next-gen system long after the original estimate.


### Deprecating APIs

If you have a lot of users (or a few very important users) using your API, you should consider deprecation very carefully. Deprecating an API means you force your users to change their application to work with you or stay locked to an earlier version.

There are a few ways you can mitigate the pain:
•  Don't deprecate. Extend the existing API or keep the previous API active. It is sometimes pretty simple, although it adds testing burden.


•  If you have to deprecate, explain why, allow ample time for users to upgrade, and provide as much support as possible (for example, an upgrade guide with examples). Your users will appreciate it.

### Large-cluster performance, cost, and design trade-offs

We also 
discussed difficult problems such as breaking changes, data contract changes, data migration, and API deprecation. 

### Best effort

Best effort is great for 
developers. Developers can move fast and break things.

### Maintenance windows

In a system with 
maintenance windows, special times are dedicated for performing various maintenance activities, such as applying security patches, upgrading software, pruning log files, and database cleanups. 

The benefit of maintenance windows is that you don't have to worry how your maintenance actions are going to interact with live requests coming into the system. It can drastically simplify operations. System administrators love maintenance windows just as much as developers love best effort systems.


The downside, of course, is that the system is down during maintenance. It may only be acceptable for systems where user activity is limited to certain times (US office hours or week days only).

With Kubernetes, you can do maintenance windows by redirecting all incoming requests via the load balancer to a web page (or JSON response) that notifies users about the maintenance window.

### Quick recovery

How quickly can you get back to normal?


Sometimes it's not up to you. For example, if your cloud provider has an outage (and you didn't implement a federated cluster, as we will discuss later) then you just have to sit and wait until they sort it out.

There are, of course, time-related issues, and even calendar-related issues. Do you remember the leap-year bug that took down Microsoft Azure on February 29, 2012?


On the other hand, rolling updates mean that if the problem is discovered early then most of your pods still run the previous version.


Data-related problems can take a long time to reverse, even if your backups are up to date and your restore procedure actually works (definitely test this regularly).


### Zero-downtime

Finally, we arrive at the 
zero-downtime system. There is no such thing as a zero-downtime system. All systems fail and all software systems definitely fail.

Think about zero-downtime as a best effort distributed system design. You design for zero-downtime in the sense that you provide a lot of redundancy and mechanisms to address expected failures without bringing the system down.

The plan for zero-downtime
 is as follows:
•  Redundancy at every level: This is a required condition. You can't have a single point of failure in your design because when it fails, your system is down.

but for some reason, it doesn't work. If you set an alert for when disk space is over 95%!f(MISSING)ull, then you'll catch it and be able to prevent the system failure.


That's right. For zero-downtime, you need to test both the application and the infrastructure together. Your 100%!p(MISSING)assing unit tests are a good start, but they don't provide much confidence that when you deploy your application on your production Kubernetes cluster it will still run as expected. 

  Performance tests
◦  Stress tests
◦  Rollback tests
◦  Data restore tests

### Performance and data consistency

When you develop or operate distributed systems, the CAP theorem should always be in the back of your mind.

### Summary

we will address the important topic of security in Kubernetes. We will also discuss the challenges of securing Kubernetes and the risks involved. 

### Node challenges

If an attacker gets access 
to a node, this is a serious threat. It can control at least the host itself and all the workloads running on it. But it gets worse. The node has a kubelet running that talks to the API server.

The danger here is that Kubernetes and your infrastructure may scale automatically and allocate more resources.


Another problem is installation of debugging and troubleshooting tools or modifying configuration outside of automated deployment. Those are typically untested and, if left behind and active, can lead to at least degraded performance, but can also cause more sinister problems.


### Network challenges

Making sure two containers can find each other and exchange information is not trivial.

### Configuration and deployment challenges

•  Remote or out-of-office employees risk extended exposure, allowing an attacker to gain access to their laptops or phones with administrative access


### Pod and container challenges

In Kubernetes, pods are the unit of work and contain one or more containers.

The containers all share the same localhost network and often share mounted volumes from the host. 

•  Bad containers might easily poison other containers in the pod
•  Bad containers have an easier time attacking the node

### Organisational, cultural, and process challenges

Traditionally, when developer and operations were separate, this conflict was managed at an organizational level. Developers pushed for more productivity and treated security requirements as the cost of doing business.

The DevOps movement brought down the wall between developers and operations. Now, speed of development often takes a front seat. 

Kubernetes was designed for this new world of DevOps and clouds. But, it was developed based on Google's experience. Google had a lot of time and skilled experts to develop the proper processes and tooling to balance rapid deployments with security. 

•  Continuous deployment might make it difficult to detect certain security problems before they reach production

In this section, we 
reviewed the many challenges you face when 
you try to build a secure Kubernetes cluster. 

In the next section, we will look at the facilities Kubernetes provides to address some of those challenges. 

### Understanding service accounts in Kubernetes

Kubernetes has regular 
users managed outside the cluster for humans connecting to the cluster (for example, via the kubectl command), and it has service accounts.

Regular users are global and can access multiple namespaces in the cluster. Service accounts are constrained to one namespace. This is important. It ensures namespace isolation, because whenever the API server receives a request from a pod, its credentials will apply only to its own namespace.


Kubernetes manages service accounts on behalf of the pods. Whenever Kubernetes instantiates a pod it assigns the pod a service account. The service account identifies all the pod processes when they interact with the API server.

Each service account has a set of credentials mounted in a secret volume. Each namespace has a default service account called default. When you create a pod, it is automatically assigned the default service account unless you specify a different service account.

That will result in the following output:


Note that a secret was created automatically for your new service account.
To get more detail, type the following:

You can see the secret 
itself, which includes a ca.crt file and a token, by typing the following:

The API token is created and added to the secret by another component called the Token Controller 
 whenever a service account is created. The Token Controller also monitors secrets and adds or removes tokens wherever secrets are added or removed to/from a service account.


The service account controller ensures the default service account exists for every namespace.

### Accessing the API server

Accessing the API 
requires a chain of steps that include authentication, authorization, and admission control. 

### Authenticating users

When you first create the cluster, a 
client certificate and key are created for you. Kubectl uses them to authenticate itself to the API server and vice versa over TLS on port 443 (an encrypted HTTPS connection). You can find your client key and certificate by checking your .kube/config file:


### Using admission control plugins

Unlike the authenticators and the authorizers, if a single admission controller rejects a request, it is denied.

•  AlwaysAdmit: Passthrough (I'm not sure why it's needed)
•  AlwaysDeny: Reject 
everything (useful for testing)


•  ResourceQuota: Reject 
requests that violate the namespace's resource quota

### Securing pods

Pod security is a
 major concern, since Kubernetes schedules the pods and lets them run.

### Using a private image repository

This approach gives you a lot of
 confidence that your cluster will
 only pull images that you have previously vetted, and you can manage upgrades better. You can configure your $HOME/.dockercfg or $HOME/.docker/config.json on each node. But, on many cloud providers, you can't do it because nodes are provisioned automatically for you.

### ImagePullSecrets

This approach is recommended for 
clusters on cloud providers. The
 idea is that the credentials for the registry will be provided by the pod, so it doesn't matter what node it is scheduled to run on. 

You can create secrets for multiple registries (or multiple users for the same registry) if needed. The kubelet will combine all the ImagePullSecrets.


But, since pods can access secrets only in their own namespace, you must create a secret on each namespace where you want the pod to run.

The pod will use the credentials from the secret to pull images from the target image registry:

### Specifying a security context

A security context
 is a set of operating-system-level security settings such as UID, gid, capabilities, and SELinux role. These settings are applied at the container level as a container security content

The container security context is applied to each container and it overrides the pod security context. 

### Protecting your cluster with AppArmor

You configure AppArmor though profiles.


### Requirements

If the result is Y then it's enabled.
The profile must be loaded into the kernel. Check the following file:

### Securing a pod with AppArmor

To verify the profile was 
attached correctly, check the attributes of process 1:

### Pod security policies

Pod security policy (PSP)
 is available as Beta in Kubernetes 1.4. It must be enabled, and you must also enable the PSP admission control to use them. A PSP is defined at the cluster-level and defines the security context for pods. 

•  Apply the same policy to multiple pods or containers

•  Let the administrator control pod creation so users don't create pods with inappropriate security contexts

This means that many pod templates and containers will have the same security policy. Without PSP, you have to manage it individually for each pod manifest.


### Managing network policies

 At the core, a network policy is a set of firewall rules applied to a set of namespaces and pods selected by labels. This a very flexible because labels can define virtual network segments and be managed as a Kubernetes resource.

### Choosing a supported networking solution


Choosing a supported networking solution
Some networking backends 
don't support network policies. For example, the popular Flannel can't be used to apply policies.
Here is a list of supported network backends:
•  Calico

### Defining a network policy

You define 
a network policy using a standard YAML manifest.
Here is a sample policy:


Note that the network policy is cluster-wide, so pods from multiple namespaces in the cluster can access the target namespace. Th

It's important to realize that the network policy operates in whitelist fashion. By default, all access is forbidden, and the network policy can open certain protocols and ports to certain pods that match the labels. This means that, if your networking solution doesn't support network policies, all access will be denied.

### Using secrets

They can be credentials such as username and password, access tokens, API keys, or crypto keys. Secrets are typically small. If you have large amounts of data you want to protect, you should encrypt it and keep the encryption/decryption key as secrets.


### Storing secrets in Kubernetes

Kubernetes stores
 secrets in etcd as plaintext. This means that direct
 access to etcd should be limited and carefully guarded. Secrets are managed at the namespace level. Pods can mount secrets either as files via secret volumes or as environment variables.

When a secret is mounted to a pod it is never written to disk. It is stored in tmpfs. When the kubelet communicates with the API server it is uses TLS normally, so the secret is protected in transit.

### Creating secrets

Secrets must be 
created before you try to create a pod that requires them. The secret must exist, otherwise the pod creation will fail.
You can create secrets with the following command:

You can create secrets 
from files using --from-file instead of --from-literal, and you can also create secrets manually if you encode the secret value as base64.


### Decoding secrets

To get the content 
of a secret you can use kubectl get secret:


The values are base64-encoded. You need to decode them yourself:


### Using secrets in a container

Using secrets in a container
Containers can access 
secrets as files by mounting volumes from the pod. Another
 approach is to access the secrets as environment variables. Finally, a container can access the Kubernetes API directly or use kubectl get secret.


To use a secret mounted as a volume, the pod manifest should declare the volume and it should be mounted in the container's spec:


### The case for a multi-user cluster

•  Managing separate testing, staging, and production environments

### Using namespaces for safe multi-tenancy

Kubernetes namespaces
 are the perfect answer to safe multi-tenant clusters. This is not a surprise as this was one of the design goals of namespaces.


The status field
 can be active or terminating. When you delete a namespace, it will get into the terminating state

To work with a namespace, you add the --namespace argument to kubectl commands:

Listing pods without the namespace returns the pods in the default namespace:



### Avoiding namespace pitfalls

The best way to avoid this situation is to hermetically seal the namespace and require different users and credentials for each namespace.


Otherwise, every time they forget to specify a namespace, they'll operate quietly on the default namespace.


### Summary

In this chapter, we covered the many security challenges facing developers and administrators building systems and deploying applications on Kubernetes clusters

In Chapter 6, Using Critical Kubernetes Resources, we will look in detail into many Kubernetes resources and concepts, and how to use them and combine them effectively.

### 6. Using Critical Kubernetes Resources

It will obviously need to store a lot information, integrate with many external services, respond to notifications and events, and be smart about interacting with you.

We will take the opportunity in this chapter to get to know kubectl and related tools a little better 

### Defining the scope of Hue

Here are a few ideas in the right direction.
•  Strong identity via a dedicated device with multi-factor authorization, including multiple biometric reasons:


◦  Frequently rotating credentials


  The Hue backend will interact with all external services via short-lived 

### External service

Hue is an aggregator of 
external services. It is not designed to replace your bank, your 
health provider, or your social network. It will keep a lot of metadata about your activities, but the content will remain with your external services. 

### Data stores

•  Relational database
•  Graph database
•  Time-series database
•  In-memory caching

### Stateless microservices

The state will be managed by the stores and accessed by the microservices with short-lived access tokens.

### Queue-based interactions

All these microservices need to talk to 
each other. Users will ask Hue to perform tasks on their 
behalf. External services will notify Hue of various events. Queues coupled with stateless microservices provide the perfect solution. Multiple instances of each microservice will listen to various queues and respond when relevant events or requests are popped from the queue. 

### Using Kubectl effectively

kubectl connects to your cluster via the API. It reads your .kube/config file, which contains information necessary to connect to your cluster or clusters. The commands are divided into multiple categories:


•  Generic commands – Deal 
with resources in a generic way: create, get, delete, run, apply, patch, replace, and so on


•  Cluster management commands – Deal 
with nodes and the cluster at large: cluster-info, certificate, drain, and so on
•  Troubleshooting commands – Describe, 
logs, attach, exec, and so on


•  Deployment commands – Deal with 
deployment and scaling: rollout, scale, auto-scale, and so on

•  Misc commands – Help, config, and 
version


### Understanding Kubectl resource configuration files

Kubectl uses YAML or JSON configuration files. Here is a JSON configuration file for creating a pod:


### Kind

Kind tells
 Kubernetes what type of resource it is dealing with. In this case, Pod. This is always required.

### Metadata

Metadata
A lot of information that describes the
 pod and where it operates:
•  Name – Identifies the pod uniquely within its namespace
•  Labels – Multiple labels can be applied
•  Namespace – The namespace the pod belongs to
•  Annotations – A list of annotations available for query


### Creating pods

The environment section allows the cluster administrator to provide environment variables that will be available to the container.

In a testing environment, it 
may use a different discovery method:

### Decorating pods with labels

The "tier" label can be used to query all pods that belong to a particular tier. These are just an example; your imagination is the limit here.

### Deploying long- running processes with deployments

Let's create the deployment and check its status:

### Updating a deployment

Let's update the deployment to upgrade to version 4.0. Modify the version in the deployment.yaml file. Don't modify labels; it will cause an error. Typically, you modify the image and some related metadata in annotations. Then we can use the apply command to upgrade the version:



### Deploying an internal service

Services solve this issue by providing a single access point to all the pods. The service is:


The service has a 
selector that selects all the pods that have labels that match it. It also exposes a port, which other services will use to access it (it doesn't have to be the same port as the container's port).

### Creating the Hue-reminders service

The service is up-and-running. Other pods can find it through environment variables or DNS. 

The environment variables for all services are set at pod creation time. That means that, if a pod is already running when you create your service, you'll have to kill it and let Kubernetes recreate it with the environment variables (you always have a replication controller or replica set, right?):


But using DNS is much
 simpler. Your service DNS name is <service name>.<namespace>.svc.cluster.local:


### Exposing a service externally

If you want to expose it to the world, Kubernetes provides two ways to do it:
•  Configure nodePort for direct access
•  Configure a Cloud load balancer if you run it in a Cloud environment

Ingress
 is a Kubernetes configuration object that lets you expose a service to the outside world and take care of a lot of details. It can do the following:
•  Provide an externally visible URL to your service
•  Load-balance traffic
•  Terminate SSL
•  Provide name-based virtual hosting

To use Ingress, you 
must have an Ingress controller running in your cluster. Note that Ingress is still in Beta and has many limitations. If you're running your cluster on GKE, you're probably OK. Otherwise, proceed with caution. 

The nginx Ingress controller will interpret this Ingress request and create a corresponding configuration file for the nginx web server:


### Launching jobs

Note that the restartPolicy must be either Never or OnFailure The default Always value is invalid because a job shouldn't restart after a successful completion.


Let's start the job and check its status:

The pods of completed
 tasks are not displayed by default. You must use the --show-all option:

### Cleaning up completed jobs

It's your responsibility to clean up completed jobs and their pods. The easiest way is to simply delete the job object, which will delete all the pods too:

### Scheduling cron jobs

Kubernetes cron 
jobs are jobs that run for a specified time, once or repeatedly. They
 behave as regular Unix cron jobs specified in the /etc/crontab file.


### Kubectl get pods

As usual, you can check the output of the pod of a completed cron job using the logs command:


You must also clean up all the individual jobs, otherwise they will stick around forever. Just deleting the cron job is not enough; it will just stop scheduling more jobs.

In summary, the cleanup of a cron job involves the following:
•  Deleting the cron job
•  Deleting all job objects that match the label


You can also suspend a cron job so it doesn't create more jobs.

### Managing the Hue platform with Kubernetes

This is tricky because if a service or pod is not ready yet but is already receiving requests, then you need to manage it somehow: fail (puts responsibility on the caller), retry (how many, how long, how often?), and queue for later (who will manage this queue?).


Using liveness probes to ensure your containers are alive


With a liveness probe, you get to decide when a container is considered alive. 

Make sure your restart policy is not Never, because that will make the probe useless.

### Using readiness probes to manage dependencies

Readiness probes
 are used for different purpose. Your 
container may be up-and-running, but it may depend on other services that are unavailable at the moment.

When a readiness probe fails for a container, the container's pod will be removed from any service endpoint it is registered with. 

Here is a sample
 readiness probe. I use the exec
 probe here to execute a custom command. If the command exits a non-zero exit code, the container will be torn down:

### Sharing with DaemonSet pods

They are typically used for keeping an eye on nodes and ensuring they are operational. 

The DaemonSet pod will collect all the data from the 50 pods and, once a second, will report it in aggregate to the remote service. 

The interesting part about this configuration file is that the hostNetwork, hostPID, and hostIPC options are set to true. This enables the pods to communicate efficiently with the proxy, utilizing
 the fact they are running on the same physical host:


### Utilizing Hue in the enterprise

Either way, Hue for enterprise must support on-premise clusters and/or bare-metal clusters.

While Kubernetes is most often deployed on the Cloud, and even has a special Cloud-provider interface, it doesn't depend on the Cloud and can be deployed anywhere.

CoreOS provides a lot of material regarding deploying Kubernetes clusters on bare-metal lusters.

### Summary

In this chapter, we designed and planned the development, deployment, and management of the Hue platform 

In particular, we focused on deploying pods for long-running services as opposed to jobs for launching short-term or cron jobs, explored internal services versus external services, and also used namespaces to segment a Kubernetes cluster. 

Data is king, but often the least flexible element of the system. Kubernetes provides a storage model, and many options for storing and accessing data.

### Persistent volumes walkthrough

Let's start by understanding the problem of storage. 

It's pretty clear that for a large-scale system, you need persistent storage accessible from any node to reliably manage the data.


### Volumes

 This is nothing new, and it is great because, as a developer who writesapplications that need access to data, you don't have to worry about where and how thedata is stored.

### Using emptyDir for intra-pod communication

The most basic volume is the emptyDir. An emptyDirvolume is an empty directory on the host. Note that it is not persistent because when thepod is removed from the node, the contents are erased. 

To use the shared memory option, we just need to add medium: Memory to the emptyDirsection:

### Using HostPath for intra-node communication

Since the pod doesn't have a privileged security context, it will not be able to write to thehost directory. Let's change the container spec to enable it by adding a security context:

### Provisioning persistent volumes

Remember that persistent volumes are resources that the Kubernetes cluster is using similarto nodes. As such they are not managed by the Kubernetes API server.You can provision resources statically or dynamically.

### Provisioning persistent volumes dynamically

We will see examples later when we discuss persistent volumeclaims and storage classes.

### Access modes

There are three access modes:ReadOnlyMany: Can be mounted read-only by many nodesReadWriteOnce: Can be mounted as read-write by a single nodeReadWriteMany: Can be mounted as read-write by many nodes

### Reclaim policy

The reclaim policy determines what happens when a persistent volume claim is deleted.There are three different policies:Retain – the volume will need to be reclaimed manuallyDelete – the associated storage asset such as AWS EBS, GCE PD, Azure disk, or OpenStackCinder volume is deletedRecycle – delete content only (rm -rf /volume/*)

The Retain and Delete policies mean the persistent volume is not available anymore forfuture claims. The recycle policy allows the volume to be claimed again.

### Making persistent volume claims

It's important to realize that claims don't mention volumes by name. The matching is doneby Kubernetes based on storage class, capacity, and labels.

Finally, persistent volume claims belong to a namespace. Binding a persistent volume to aclaim is exclusive. That means that a persistent volume will be bound to a namespace. Evenif the access mode is ReadOnlyMany or ReadWriteMany, all the pods that mount thepersistent volume claim must be from that claim's namespace.

### Demonstrating persistent volume storage end to end

To illustrate all the concepts, let's do a mini demonstration where we create a HostPathvolume, claim it, mount it, and have containers write to it.

Let's start by creating a hostPath volume. Save the following in persistent-volume.yaml:

To check out the available volumes, you can use the resource type persistentvolumes orpv for short:

We have a persistent volume. Let's create a claim

As you can see, the claim and the volume are bound to each other. The final step is tocreate a pod and assign the claim as a volume.

Let's create the pod and verify that both containers are running:

### Public storage volume types - GCE, AWS, and Azure

 If you choose to run your Kubernetes cluster on apublic cloud platform, you can let your cloud provider deal with all these challenges andfocus on your system. 

### GCE persistent disk

You can use a GCE persistentdisk to share data as read-only between multiple pods in the same zone.

The pod that's using a persistent disk in ReadWriteOnce mode must be controlled by areplication controller, a replica set, or a deployment with a replica count of 0 or 1. Trying toscale beyond 1 will fail for obvious reasons:

### Adding a GlusterFS Kubernetes service

To make the endpoints persistent, you use a Kubernetes service with no selector to indicatethe endpoints are managed manually:

### Summary

In this chapter, we took a deep look into storage in Kubernetes. 

### Stateful versus stateless applications in Kubernetes

A stateless Kubernetes application is an application that doesn't manage its state in theKubernetes cluster. All of the state is stored outside the cluster and the cluster containersaccess it in some manner. 

### Understanding the nature of distributed data-intensive apps

 All massive data processing must be done close to the data itself becausetransferring data is prohibitively slow and expensive. Instead, the bulk of processing codemust run in the same data center and network environment of the data.

### Why manage states in Kubernetes?

The main reason to manage states in Kubernetes itself as opposed to a separate cluster isthat a lot of the infrastructure needed to monitor, scale, allocate, secure and operate astorage cluster is already provided by Kubernetes. Running a parallel storage cluster willlead to a lot of duplicated effort.

### Why manage states outside of Kubernetes?

You may want to approach stateful apps in Kubernetes incrementally, starting with aseparate storage cluster and integrating more tightly with Kubernetes later.

### Shared environment variables versus DNS records for discovery

There are two main methods:DNSEnvironment variablesIn some cases, you may want to use both where environment variables can override DNS.

### Accessing external data stores via environment variables

Kubernetes offers the ConfigMap resource as a way to keepconfiguration separate from the container image. The configuration is a set of key-valuepairs. The configuration information can be exposed as an environment variable inside thecontainer as well as volumes. You may prefer to use secrets for sensitive connectioninformation.

### Using DaemonSet for redundant persistent storage

 We can label a set of nodes and make sure that the stateful pods are scheduled on aone-by-one basis to the selected group of nodes.

### Utilizing StatefulSet

Unlike regular pods, the pods of a stateful set are associated withpersistent storage.

### When to use StatefulSet

Stable, unique network identifiersStable, persistent storageOrdered, graceful deployment, and scalingOrdered, graceful deletion, and termination

### The components of StatefulSet

There are several pieces that need to be configured correctly in order to have a workingStatefulSet:A headless service responsible for managing the network identity of the StatefulSet podsThe StatefulSet itself with a number of replicasPersistent storage provision dynamically or by an administrator

Now, the StatefulSet configuration file will reference the service:

The next part is the pod template that includes a mounted volume named www:

Last but not least, the volumeClaimTemplates use a claim named www matching themounted volume. The claim requests 1Gib of storage with ReadWriteOnce access:

### The Cassandra Docker image

Deploying Cassandra on Kubernetes as opposed to a standalone Cassandra clusterdeployment requires a special Docker image. This is an important step because it meanswe can use Kubernetes to keep track of our Cassandra pods. 

Here are the essential parts of the Docker file.

Add and copy the necessary files (Cassandra.jar, various configuration files, run script, and read-probe script), create a data directory for Cassandra to store its SSTables, and mount it:

### Creating a Cassandra headless service

The clusterIP is None, which means the service is headless and Kubernetes will not do any load balancing or proxying. This is important because Cassandra nodes do their own communication directly.


The 9042 port is used by Cassandra to serve CQL requests. Those can be queries, inserts/updates (it's always an upsert with Cassandra), or deletes.


### Using a replication controller to distribute Cassandra

•  Replication controller instead of a stateful set
•  Storage on the node the pod is scheduled to


### Summary

You are armed with multiple methods for various use cases and maybe you've even learned a little bit about Cassandra.


### 9. Rolling Updates, Scalability, and Quotas

In this chapter, we will explore the automated pod scalability that Kubernetes provides, how it affects rolling updates, and how it interacts with quotas. 

### Horizontal pod autoscaling

Kubernetes
 can watch over your pods and scale them when the CPU utilization or some other metric crosses a threshold. 

The autoscaler automatically does what we had to do ourselves before.

 The autoscaler will do it for us.


### Declaring horizontal pod autoscaler

To declare a horizontal 
pod autoscaler, we need a replication controller, or a deployment, and an autoscaling resource. Here is a simple replication controller configured to maintain 3 nginx pods:


The minReplicas and maxReplicas specify the range of scaling. This is needed to avoid runaway situations that could occur because of some problem. 

exhausted

 Kubernetes will ensure that there are always between 2 to 4 Nginx instances running.


Kubernetes has a tolerance, which is currently (Kubernetes 1.5) hardcoded to 0.1. That means that, if TCUP is 90%!,(MISSING) then scaling up will occur only when average CPU utilization goes above 99%!((MISSING)90 + 0.1 * 90) and scaling down will occur only if average CPU utilization goes below 81%!
(MISSING)

### Performing rolling updates with autoscaling

Since rolling updates are such an important capability, I recommend that you always bind horizontal pod autoscalers to a deployment object instead of a replication controller or a replica set. 

### Handling scarce resources with limits and quotas

Scheduling can easily get out of control, and inefficient use of resources is a real concern. 

First, let's 
understand the core issue.

Even if you provision new nodes, the DaemonSet will immediately commandeer half of the memory.


Most of these problems can be mitigated by judiciously using namespace resource quotas and careful management of the cluster capacity across multiple resource types such as CPU, memory, and storage.


### Enabling resource quotas

Note that there may be at most one ResourceQuota object per namespace to prevent potential conflicts. This is enforced by Kubernetes.


### Resource quota types

There are different
 types of quota we can manage and control. The categories are compute, storage, and objects.

### Compute resource quota

•  limits.cpu: Across all pods in a non-terminal state, the sum of CPU limits cannot exceed this value

### Object count quota

Too many API objects means a lot of work for Kubernetes.

Here are all the supported objects:
•  ConfigMaps: The total number of config maps that can exist in the namespace

### Quota scopes

•  BestEffort: Match pods that have best effort quality of service


### Creating quotas

3.  We can observe all the quotas:


kubectl get quota

Since there is a compute quota in the namespace, every container must specify its CPU, memory requests, and limit. The quota controller must account for every container compute resources usage to ensure the total namespace quota is respected.


### Using limit ranges for default compute quotas

Using limit ranges for default compute quotas
1.  A 
better way is to specify default compute limits. Enter limit ranges. Here is a configuration file that sets some defaults for containers:


### Choosing and managing the cluster capacity

If all your nodes are running at 100%!c(MISSING)apacity, you need to add more nodes to your cluster. There is no way around it. Kubernetes will just fail to scale. 

### Trading off cost and response time

If money is not an issue you can just over-provision your cluster. Every node will have the best hardware configuration available, you'll have way more nodes than are needed to process your workloads, and you'll have copious amounts of available storage. Guess what? Money is always an issue!


### Autoscaling instances

Autoscaling instances
All the big cloud providers have instance
 autoscaling in place. 

There are some differences, but scaling up and down based on CPU utilization is always available, and sometimes custom metrics are available too. 

### Mind your cloud quotas

When working with cloud providers, some of the 
most annoying things are quotas. I've worked with four different cloud providers (AWS, GCP, Azure, and Alibaba cloud) and I was always bitten by quotas at some point.

However, a human must approve quota requests, and that can take a day or two. In the meantime, your system is unable to handle the load.


### Manage regions carefully

When planning your cluster, you should consider carefully your geo-distribution strategy. If you need to run your cluster across multiple regions, you may have some tough decisions to make regarding redundancy, availability, performance, and cost.


### Caching reads in the API server

Kubernetes keeps the
 state of the system in etcd, which is very reliable, though not superfast.

The various Kubernetes components operate on snapshots of that state and don't rely on real-time updates. That fact allows the trading of some latency for throughput. All the snapshots used to be updated by etcd watches.

The in-memory read cache is updated by etcd watches. These schemes significantly reduces the load on etcd and increase the overall throughput of the API server.


### The pod lifecycle event generator

In Kubernetes 1.1, the official (tested and advertised) number was 30 pods per node. I actually ran 40 pods per node on Kubernetes 1.1, but I paid for it in excessive Kubelet overhead that stole CPU from the worker pods. In Kubernetes 1.2, the number jumped to 100 pods per node.


The way the PLEG works
 is that it lists the state of all the pods and containers and compares it to the previous state. This is done once for all the pods and containers. Then, by comparing the state to the previous state, the PLEG knows which pods need to sync again and invokes only those pods.

### Serializing API objects with protocol buffers

In a large-scale Kubernetes cluster, a lot of components need to query or update the API server frequently. The cost of all that JSON parsing and composition adds up quickly. 

The JSON format is still there, but all internal communication between Kubernetes components uses the protocol buffers serialization forma

such as using gRPC instead of HTTP+JSON for etcd API). This change may provide an additional 30%!i(MISSING)mprovement.

### Measuring API responsiveness

The response time is understandably greater than the single API calls, but it is still way below the SLO of one second (1,000 milliseconds):

### Measuring end to end pod startup time

You could say that the primary function of Kubernetes is to schedule pods.


### Summary

In this chapter, we've covered many topics relating to scaling Kubernetes clusters

how to perform rolling updates correctly and safely in the context of auto-scaling, and how to handle scarce resources via resource quotas.

### Understanding the Kubernetes networking model

In addition, containers in the same pod share their pod's IP address and can communicate with each other through localhost. 

### Pod to service communication

In a Kubernetes cluster, pods can be destroyed and created constantly. The service provides a layer of indirection that is very useful because the service is stable even if the set of actual pods that respond to requests is ever-changing. 

### External access

The following diagram shows how all that the external load balancer on the right side does is send traffic to all nodes that reach the proxy, which takes care of further routing if needed:

### Lookup and discovery

In order for pods and containers to 
communicate with each other, they need to find each other. There are several ways for containers to locate other containers or announce themselves. 

### Services and endpoints

Kubernetes services can be
 considered as a registration service. Pods that belong to a service are registered automatically based on their labels. Other pods can look up the endpoints to find all the service pods or take advantage of the service itself and directly send a message to the service that will get routed to one of the backend pods.

### Kubernetes ingress

Kubernetes offers an 
ingress resource and controller that is designed to expose Kubernetes services to the outside world. 

The ingress object is often used for smart load balancing and TLS termination. Instead of configuring and deploying your own Nginx server, you can benefit from the built-in ingress. 

### Kubernetes network plugins

Kubernetes has a
 network plugin system since networking is so diverse and different people would like to implement it in different ways. 

### Virtual Ethernet devices

Virtual Ethernet (veth) devices
 represent physical network devices. 

### Routing

Routing connects separate networks
, typically based on routing tables that instruct network devices how to forward packets to their destination. Routing is done through various network devices, such as routers, bridges, gateways, switches, and firewalls, including regular Linux boxes.

### Setting the MTU

Another example is IPSEC, that requires lowering the MTU due to the extra overhead from IPSEC encapsulation overhead, but the Kubenet network plugin doesn't take it into consideration.

The solution is to avoid relying on the automatic calculation of the MTU and just tell the Kubelet what MTU should be used for network plugins via the --network-plugin-mtu command-line switch that is provided to all network plugins. 

### Configuring network policies

Network policies are
 configured via the NetworkPolicy resource. Here is a sample network policy:

### Configuring an external load balancer

We use a service type of LoadBalancer instead of using a service type of ClusterIP, which directly exposes a Kubernetes node as a load balancer. This depends on an external load balancer provider properly installed and configured in the cluster. Google's GKE is the most well-tested provider, but other cloud platforms provide their integrated solution on top of their cloud load balancer.

### Ingress

Ingress 
in Kubernetes is at its core a set of rules that allow inbound connections to reach cluster services. In addition, some ingress controllers support the following:
•  Connection algorithms
•  Request limits
•  URL rewrites and redirects
•  TCP/UDP load balancing
•  Access control and authorization

### HAProxy

 we want a custom external load balancer we can create a custom external load balancer provider and use LoadBalancer or use the third service type, NodePort. High-Availability (HA) Proxy
 is a mature and battle-tested load balancing solution. It is considered the best choice for implementing external load balancing with on-premises clusters. 

### Summary

Kubernetes is a very flexible platform, designed for extension. 

### 11. Running Kubernetes on Multiple Clouds and Cluster Federation

•  A deep dive into what cluster federation is all about
•  How to prepare, configure, and manage a cluster federation

•  How to run a federated workload across multiple clusters

### The motivation for Helm

It's simple to share charts that can be versioned and hosted on public or private servers. When you need to rollback recent upgrades, Helm provides a single command to rollback a cohesive set of changes to your infrastructure.


### The Tiller server

•  Listening for incoming requests from the Helm client
•  Combining a chart and configuration to build a release

### Using Helm

Helm is a 
rich package management system that lets you perform all the necessary steps to manage the applications installed on your cluster.

### Installing Helm

Helm is implemented in Go, and the same binary executable can serve as either client or server.


### Installing the Helm client

You must have Kubectl configured 
properly to talk to your Kubernetes cluster because the Helm client uses the Kubectl configuration to talk to the Helm server (Tiller)

For Mac OSX and Linux, you can install the client from a script:

### Installing Tiller locally

You need to tell the Helm client to connect to the local Tiller server. You can do it by setting an environment variable:

### Finding charts

Helm, by default, searches the official Kubernetes chart repository, which is called stable:


### Installing packages

2.  Connect using the mysql cli, then provide your password:



### Checking installation status

Checking installation status
Helm doesn't wait for the 
installation to complete because it may take a while.

Let's check it out now:

### Customizing a chart

You can specify multiple values using comma-separated lists: --set a=1,b=2.

### Additional installation options

The helm install command 
can install from several sources:
•  A chart repository (as we've seen)
•  A local chart archive (helm install foo-0.1.1.tgz)
•  An unpacked chart directory (helm install path/to/foo)
•  A full URL (helm install https://example.com/charts/foo-1.2.3.tgz)

### Upgrading and rolling back a release

Upgrading and rolling back a release

Note that we've lost our root password. All the existing values are replaced when you upgrade. 

OK, let's roll back. The helm history command shows us all the available revisions we can roll back to:


Let's verify our
 changes 
were rolled back:


### Working with repositories

Note that Helm doesn't provide tools for uploading charts to remote repositories because that would require the remote server to understand Helm, to know where to put the chart, and how to update the index.yaml file.

### Managing charts with Helm

You can also 
use helm to help you find issues with your chart's 
formatting or information:

### Creating your own charts

A chart 
is a collection of files that describe a related set of Kubernetes resources.

### The appVersion field

The appVersion field is not related to the version field. It is not used by Helm and serves as metadata or documentation for users that want to understand what they are deploying.

### Managing chart dependencies

These dependencies are expressed explicitly by copying the dependency charts into the charts/ sub-directory during installation.


### Managing dependencies with requirements.yaml

Instead of manually 
placing charts in the charts/ sub-directory, it is better to declare dependencies using a requirements.yaml file inside of your chart.


Once you have a dependencies file, you can run the Helm dependency update and it will use your dependency file to download all of the specified charts into the charts sub-directory for you:

### Writing template files

Template files are just text files
 that follow the Go template language rules. They can generate Kubernetes configuration files. Here is the service template file from the Gitlab CE chart:

### Feeding values from a file

You can provide your own YAML values files to override the defaults during the install command:


### Summary

In this chapter, we took a look at Helm, the Kubernetes package manager. Helm gives Kubernetes the ability to manage complicated software composed of many Kubernetes resources with inter-dependencies. 

It organizes packages and lets you search charts, install and upgrade charts, and share charts with collaborators. You can develop your charts and store them in repositories.

The Future of Kubernetes, we will look ahead to the future of Kubernetes and examine its roadmap and a few personal items from my wish list.

### 14. The Future of Kubernetes

We'll start with the roadmap and forth coming product features, including diving into the design process of Kubernetes. 

### Kubernetes releases and milestones

•  Mark Kubelet's master-service-namespace flag as deprecated
•  Remove the deprecated --babysit-daemons kubelet flag
•  Clean up the pre-ControllerRef compatibility logic


You can check the changelog here:

Here are some of the 
changes in the 1.7 alpha release:
•  Juju: Enable GPU mode if 
GPU hardware detected
•  Check the error before parsing the apiversion


•  Use http2 in kubeapi-load-balancer to fix kubectl exec uses
•  Support status.hostIP in downward API


### The value of bundling

But some developers or organizations worry about vendor lock-in, or need to run on multiple Cloud platforms or a hybrid public/private. Kubernetes has a strong advantage here.

### Docker Swarm

Docker wants to get a piece of the orchestration cake and released the Docker Swarm product. The main benefit of 
Docker Swarm
 is that it comes as part of the Docker installation and uses standard Docker APIs. 

### AWS

•  Automate the provisioning of Kubernetes clusters in AWS
•  Deploy highly available Kubernetes masters
•  The ability to generate Terraform configurations

### Azure

Azure provides the Azure
 container service, and they don't pick favorites. You can choose if you 
want to use Kubernetes, Docker Swarm, or DC/OS. This is interesting because, initially, Azure was based on Mesosphere DC/OS and they added Kubernetes and Docker Swarm as orchestration options later.

### Alibaba Cloud

I'm not going to make any predictions on whether or not they'll adopt Kubernetes officially.


### GitHub

Kubernetes is developed on 
GitHub and is one of the top projects on GitHub. It is in the top 0.01 percent in stars and number one in terms of activity.


### Mindshare

Kubernetes is getting a lot of 
attention and deployments. Large and small companies that get into the containers/DevOps/microservices arena adopt Kubernetes and the trend is clear. 

### Other players

There are a number of other companies that use Kubernetes as a foundation, such as Rancher and Apprenda. A large number of startups develop add-ons and services that run inside the Kubernetes cluster. The future is bright.

### Education and training

The official Kubernetes documentation is getting better and better, but there is still a long way to go. The 
online tutorials are great for getting started.


As the popularity of Kubernetes grows even further, more and more options will be available.
