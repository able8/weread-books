## Kubernetes生产化实践之路
> 孟凡杰等

### 内容简介

本书从设计层面剖析了Kubernetes 的设计原理，并阐述了其设计背后的生产系统问题。Kubernetes 作为开放式平台，具有对不同类型的应用（有状态应用或无状态应用，在线服务或离线任务）进行统一管控的能力。

分享了如何构建高可用的多租户集群，如何确保集群的稳定性和高性能。

### 推荐序

eBay 基础架构工程部研发总监　 许健

### 前言

云计算三要素——计算、网络和存储，均以插件形式与Kubernetes 集成。这样做的好处是，使用者可以选择自己的插件实现来落地。因此，当计算技术、网络技术或存储技术更新换代时，Kubernetes 能够很容易地集成。

而是尝试去介绍这些精巧设计背后的细节，比如它的设计考量、设计选择，以及选择这样的设计所付出的代价。

经历了五年的摸爬滚打，Kubernetes 在eBay 历史上成为了唯一一个统管大数据、搜索后台和云业务的支撑平台。截至目前，Kubernetes 已经管理了上百个集群和数万台物理机，其最终目标是管理所有共计十多万台计算节点，

本章介绍了镜像仓库的实现，以及基于容器镜像扫描和准入的安全保证方案。

第12 章：容器技术和Kubernetes 及相应的工具链打通了DevOps 的所有环节。

### 第1章 架构基础

再到2008 出现的Linux Container（LXC），容器技术经历了几代技术革新，其核心目标从未改变：如何将应用进程限制在独立的运行环境中，以满足封装和隔离的需求。

Docker 引入了容器镜像的概念，将应用、应用的全部依赖、甚至操作系统打包存储和分发，彻底解决了软件交付的方式，简化了应用部署的复杂性。

谷歌自Kubernetes 诞生之日起即开始容器云的标准化工作，其目标不仅仅是维护一个开源项目，而是联合众多厂家，形成强大的联盟，定制云计算标准以求行业统一。巧妙的业务抽象和强大的扩展性，使其在短短数年时间，已然成为云计算行业的事实标准。

Kubernetes 是基础平台，运行在其之上的应用需要做适度的改造和适配，才能充分利用其提供的故障转移、负载均衡、自动扩缩容等功能，云原生（Cloud-Native）就是运行在Kubernetes 平台之上的应用需要追求的目标。云原生是将应用程序构建为微服务，并将其运行在完全动态地利用云计算模型优势的容器编排平台上的方法。

设计云原生应用程序可以从以下几个方面考虑：
● 松耦合的微服务：将单个应用程序开发为一组微服务，每个微服务都在各自的进程中运行，并使用轻量级协议（例如HTTP）进行通信。这些服务是围绕业务功能构建的，但又可以全自动地独立部署某个微服务。

● 故障的容忍性和弹性：应用程序跨多个物理数据中心部署，各个服务可以在各个云供应商之间自由启动、关闭和迁移。当硬件发生故障或者部分网络中断，以至单个服务的部分实例不能继续提供服务时，也不会影响整体服务的质量。

对企业而言，应用程序迁移成云原生架构，是需要经过长期过渡和增量改进的。长远来看，这是降低和规避运维风险的标准策略。

### 1.1 云计算的变革

应用程序的部署经历了三个时代：物理机时代、虚拟化时代和容器化时代，相应的应用部署如图1-1 所示。
￼
图1-1　应用部署的时代变迁

有了虚拟化技术，一台物理节点可以切分成多个粒度更小的逻辑节点。随之而来的是管理成本的提升，由于架构的变更，管理员需要管理的应用实例是以前的数十倍甚至上百倍，由于虚拟化使得需要管理的主机数量是原来的数倍，管理复杂度的提升显而易见。

云计算就是为解决海量应用在大规模服务器集群中的管理而产生的技术，可以说云计算是虚拟化的伴生技术。
所谓云计算，是将成千上万的计算节点组成一个集群，统一管控，对计算、存储和网络等计算机系统资源进行抽象，使得云用户无须关心基础架构，只需定义自己需要的计算资源，云平台可以自动选择最合适的计算资源，并分配给用户以满足业务需求。

1.基础架构即服务（Infrastructure as a Service，即IaaS）
顾名思义，基础架构即服务只负责到基础架构层面，它是对计算、网络和存储资源的抽象，并提供这些基础资源的访问和监控服务。IaaS 的用户在对云平台发出请求后，云平台只负责为用户提供基础资源，例如一个虚拟机，如何使用该虚拟机依然由用户自己负责。

（1）容器的运行基于进程而非虚拟机，无须模拟操作系统。其特点是启动速度快、占用资源少，这有利于计算资源全部向应用倾斜，降低硬件成本。
（2）容器基于Linux Namespace 技术隔离进程，Namespace 技术可以使用户进程拥有独立的网络配置、文件系统、用户空间、进程空间，等等。虽然容器只是一个应用进程，但因为较好的隔离性，其行为甚至可以模拟虚拟机。

（3）容器基于Linux Control Group 技术，对用户进程进行资源限定，可以为每一个容器实例分配CPU、Memory、磁盘I/O 等资源上限，能够隔离同一主机上多个用户进程彼此之间的干扰。

（4）与虚拟机类似，容器也有容器镜像，而容器镜像的打包相比虚拟机而言要优雅很多。比如Docker 支持Dockerfile，允许用户像源代码一样管理容器镜像源文件，包括指定基础操作系统镜像、安装中间件、拷贝应用代码、启动应用等，容器镜像是面向应用而不是操作系统的。

（6）每个文件层都有基于其内容计算出来的Digest，在文件分发时，如果某个文件层未发生变化，则无须重新拉取，由此解决增量文件部署的问题。无论基础镜像有多大，只要基础镜像不更新，则更新镜像版本时，都只拉取变更的部分，不会过多消耗带宽。
（7）容器镜像可上传至镜像仓库，在任何其他计算节点上，都可以从镜像仓库拉取和运行镜像，镜像可以通过不同的Tag 进行版本管理。

Kubernetes 的核心竞争力究竟在哪里？
1.声明式系统
声明式系统（Declaritive System）与命令式系统（Imperative System）相对应。

声明式系统追求的是最终一致性，由系统保证一直尝试，并使实际状态一致，因此整个系统都基于异步调用。Kubernetes 的最核心优势是，将其解决问题领域中的所有对象做了非常好的抽象，比如将计算节点抽象成Node，将运行应用的实体抽象成Pod，将可供访问的应用服务抽象成Service 等。


2.控制器模式
控制器模式是Kubernetes 系统运作的关键，Kubernetes 中每种抽象出来的对象，都有其对应的控制器组件。每个控制器监听其所关注的对象的变更，然后按照对象中最新的期望状态进行系统配置，配置完成后，更新该对象的实际状态。这些控制器通力合作，负责让整个集群及集群上运行的应用与用户的期望一致。

因此Kubernetes 提供了容器运行接口、容器存储接口、容器网络接口，使得不同企业可按需定制方案。比如企业有统一的认证系统，Kubernetes 则避免重复造轮子，它提供了多种认证接口，比如可以通过Webhook 方便地与企业认证平台进行整合。Kubernetes 不是一个孤岛，从诞生开始起，其定位就是与企业现有的平台进行整合，构建生态系统。

比如谷歌主导的Istio 的服务网格（ServiceMesh）尝试解决的问题是流量管理标准化，比如谷歌主导的Knative 尝试解决的问题是无服务器架构。而这些方案大多数是在解决通用的问题，都在朝着向社区开放、构建业界标准的方向努力。这些项目与Kubernetes 核心项目一起形成一个完整的生态，来解决容器云平台的全部命题。大厂背书和活跃的社区推动，使得这些技术方案在未来极可能成为整个业界的标准。可以说掌握了这些技术，就掌握了云计算的未来。

### 1.2 Kubernetes 模型设计

1）Kubernetes 将业务模型化，这些对象的操作都以API 的形式发布出来，因此其所有API 设计都是声明式的。


（3）所有对象应该是互补和可组合的，而不是简单的封装。通过组合关系构建的系统，通常能保持很好的高内聚、松耦合特性。

1.2.2　模型设计
1.2.2.1　TypeMeta
TypeMeta 是Kubernetes 对象的最基本定义，它通过引入GKV（Group，Kind，Version）模型定义了一个对象的类型。下面分别介绍一下Group、Kind、Version。
（1）Group

1.2.2.2　Metadata
TypeMeta 定义了 “我是什么”，Metadata 定义了 “我是谁”。为方便管理，Kubernetes将不同用户或不同业务的对象用不同的Namespace 进行隔离。Metadata 中有两个最重要的属性——Namespace 和Name，分别定义了对象的Namespace 归属及名字，这两个属性唯一定义了某个对象实例。

Kubernetes 采用了更巧妙的方式管理对象和对象的松耦合关系，其依赖的就是Label和Selector。Label，顾名思义就是给对象打标签，一个对象可以有任意对儿标签，其存在形式是键值对儿。不像名字和UID，标签不需要独一无二，多个对象可以有同一个标签，每个对象可以有多组标签。

其他对象只需要定义Label Selector 就可以按条件查询出其需要关联的对象。Label 的查询可以基于等式，如app=web 或app!=db，或基于集合，如app in (web, db)或app notin (web, db)，可以只查询Label 键，如app。Label 对多个条件查询只支持 “与” 操作，如app=web, tier=front。


事实上Annotation 是对象的属性扩展。社区在开发新功能（需要对象发生变更）之前，往往会先把需要变更的属性放在Annotation 中，当功能经历完实验阶段再将其移至正式属性中。
Annotation 作为属性扩展，更多是面向系统管理员和开发人员的，因此Annotation 需要像其他属性一样做合理归类。

比如一个最常用的场景，为Pod 标记Annotation 以告知Prometheus 为其抓取系统指标，具体代码如下：
￼

Finalizer 本质上是一个资源锁，Kubernetes 在接收某对象的删除请求时，会检查Finalizer是否为空，如果为空则只对其做逻辑删除，即只会更新对象中的metadata.deletionTimestamp字段。具有Finalizer 的对象，不会立刻删除，需等到Finalizer 列表中所有字段被删除后，也就是只有该对象相关的所有外部资源已被删除，这个对象才会被最终删除。

4.ResourceVersion
通常在多线程操作相同资源时，为保证实物的一致性，需要在对象进行访问时加锁，以确保在一个线程访问该对象时，其他线程无法修改该对象。

如果第二个线程对该对象在1 的版本基础上做了更改，回写API Server 时，所带的新的版本信息也为2，那么API Server校验会发现第二个线程新写入的对象ResourceVersion 与服务器端的ResourceVersion 相冲突，即写入失败，需要第二个线程读取最新版本，以便重新更新。
此机制确保了分布式系统中的任意多线程能够无锁并发访问对象，极大地提升了系统的整体效率。

1.2.2.3　Spec 和Status
Spec 和Status 才是对象的核心，Spec 是用户的期望状态，由创建对象的用户端来定义。Status 是对象的实际状态，由对应的控制器收集实际状态并更新。与 TypeMeta 和Metadata 等通用属性不同，Spec 和Status 是每个对象独有的

内部隐藏的机制也非常不利于系统维护的设计方式。如图 1-4 所示，StatefulSet、ReplicaSet 和DaemonSet，是三种Pod 的集合，Kubernetes 用不同的API 对象来定义它们，而不是将它们封装在同一个资源对象中，内部再通过特殊的隐藏算法来区分这个资源对象是有状态的、无状态的，还是节点服务。

Secret、ConfigMap和PVC 是不同的资源对象定义，都可以作为存储卷在Pod 中使用。而在Pod 中使用时，只需要指定该对象的名称即可，无须将其具体信息在Pod 资源对象中进行扩展。

容器云平台需要解决的最核心的问题是应用运行，Kubernetes 将容器化应用运行的实体抽象为Pod，Pod 类似豆荚，它是一个或者多个容器镜像的组合。

3.ServiceAccount
Pod 中运行的进程有时需要与Kubernetes API 通信，在启用了安全配置的集群后，Pod一定要以某种身份与 Kubernetes 通信， 这个身份就是系统账户（ServiceAccount）。Kubernetes 会默认为每个 Namespace 创建一个 default ServiceAccount， 并且为每个ServiceAccount 生成一个JWT Token，这个Token 保存在Secret 中。用户可以在其Pod 定义中指定ServiceAccount（默认为default），其对应的Token 会被挂载在Pod 中，Pod 中的进程可以通过该Token 与Kubernetes 进行通信。

Deployment 就是一个用来描述发布过程的对象，其实现机制是，当某个应用有新版本发布时，Deployment 会同时操作两个版本的ReplicaSet。其内置多种滚动升级策略，会按照既定策略降低老版本的Pod 数量，同时创建新版本的Pod，并且总是保证正在运行的Pod总数与用户期望的副本数一致，并依次将该Deployment 中的所有副本都更新至新版本。图1-5 展示了Deployment 的滚动升级策略。


PersistentVolume（PV）是集群中的一块存储卷，可以由管理员手动设置，或当用户创建PersistentVolumeClaim（PVC）时根据StorageClass 动态设置。PV 和PVC 与Pod 生命周期无关。也就是说，当Pod 中的容器重新启动、Pod 重新调度或者删除时，PV 和PVC不会受到影响，Pod 存储于PV 里的数据得以保留。

社区鼓励基于CRD 的业务抽象，众多主流的扩展应用都是基于CRD 构建的，比如Istio、Knative。甚至基于CRD推出了Operator Mode 和Operator SDK，可以以极低的开发成本定义新对象，并构建新对象的控制器。

1.2.4　控制器模式
声明式系统的工作原理是什么？当用户定义对象的期望状态时，Kubernetes 通过何种机制确保实际状态与期望状态最终保持一致？在定义了如此多的对象后，这些对象又是如何联动起来，完成一个个业务流的呢？秘密就是控制器模式，Kubernetes 定义了一系列的控制器，事实上几乎所有的Kubernetes 对象都被一个或数个控制器所监听，当对象发生变化时，控制器会捕获对象变化并完成配置操作。

Informer()用于接收资源对象的变化的Event，针对Add、Update 和Delete 的事件，可以注册相应的EventHandler。在EventHandler 内，根据传入的object 调用controller.KeyFunc计算出字符串key，并把它加入控制器的队列中。

Kubernetes 运行一组控制器，以使资源的当前状态与所需状态保持匹配。

除API Server 和etcd 外，所有Kubernetes 组件，不论其名称是Scheduler、Controller Manager、或是kubelet，其本质都是一致的，都可以被称为控制器，因为这些组件中都有一个控制循环。它们监听API Server 中的对象变更，在自己关注的对象发生变更后完成既定的控制逻辑，再将控制逻辑执行完成后的结果更新回API Server，并持久化到etcd 中。

API Server 作为集群的API 网关，接收所有来自用户的请求。用户创建Deployment之后，该请求被发送至API Server，经过认证、鉴权和准入三个环节，该Deployment 对象被保存至etcd。

Deployment Controller 会读取 Deployment 中定义的 podTemplate， 将其做哈希计算， 得到值为[pod-template-hash]，并依照如下约定创建新的ReplicaSet：
● 新的ReplicaSet 的命名格式为[deployment-name]-[pod-template-hash]。
● 为ReplicaSet 添加label，此Label 为pod-template-hash: [pod-template-hash]。
● 将Deployment 的值赋给ReplicaSet 的OwnerReference。
Deployment Controller 将新的ReplicaSet 创建请求发送至API Server，API Server 经过认证授权和准入步骤，将该对象保存至etcd。

ReplicaSet Controller 监听API Server 中所有ReplicaSet 对象的变更，新对象的创建令其唤醒并开始执行控制逻辑。ReplicaSet Controller 读取ReplicaSet 对象的Selector 定义，并通过该属性过滤当前 Namespace 中的所有的 Pod 对象，并判断是否有 Pod 对象的OwnerReference 为该ReplicaSet。由于此ReplicaSet 刚刚创建，所以没有满足此查询条件的Pod，于是ReplicaSet 会按照如下约定创建Pod：

此架构模式的优势是每个组件各司其职，巧妙而灵活，代码易维护，缺点是运维复杂度相对较高，在整个业务流中有任何组件出现故障都会使Kubernetes 不可用。

### 1.3 Kubernetes 核心架构

Kubernetes 也遵循主从结构。通常将固定规模的计算节点组成一个集群，在集群中挑选数台计算节点作为管理节点（Master），其余的计算节点作为工作节点（Minion）。

图1-10　etcd 工作原理
对于etcd，这里有几个概念：领导者、选举和任期。任何etcd 集群成员都可以处理读请求，不需要共识。但只有领导者才能处理写请求，包括更改、新增、删除等。如图1-10所示，当来自客户端API Server 的写请求被提交到etcd 成员处时，如果它不是领导者，那么它会将此请求转移给领导者。领导者会将请求复制到集群的其他成员中进行仲裁，当超过半数成员同意更改时，领导者才将更改后的新值提交到日志wal 中，并通知集群成员进行相应的写操作，将日志中的新值写入磁盘中。

根据Raft 的工作机制，每个写请求需要集群中的每个成员做仲裁，因此我们建议etcd 集群成员数量不要超过7 个，推荐是5 个，个数越多仲裁时间会越多，写的吞吐量会越低。如果集群中的某个成员处理请求特别慢，就会让整个etcd 集群不稳定，且性能受到限制。因此，我们要实时监测每个etcd 成员的性能，及时修复或者移除性能差的成员。

集群内部的Pod 就可以通过服务名访问API Server。Pod 到API Server 的流量只会在集群内部转发，而不会被转发到外部的负载均衡器上。

控制器是Kubernetes 集群的自动化管理控制中心，里面包含30 多个控制器，有Pod管理的（Replication 控制器、Deployment 控制器等）、有网络管理的（Endpoints 控制器、Service 控制器等）、有存储相关的（Attachdetach 控制器等）

大多数控制器的工作模式雷同，都是通过API Server 监听其相应的资源对象，根据对象的状态来决定接下来的动作，使其达到预期的状态。

很多场景都需要多个控制器协同工作，比如某个节点宕机，kubelet 将会停止汇报状态到Node 对象。NodeLifecycle 控制器会发现节点状态没有按时更新，超过一段时间（可通过参数--pod-eviction-timeout 来指定）后，它将驱逐节点上的Pod。

控制器采用主备模式和Leader Election 机制来实现故障转移（Fail Over），如图1-12所示，也就是说允许多个副本处于运行状态，但是只有一个副本作为领导者在工作，其他副本作为竞争者则不断尝试获取锁，试图通过竞争成为领导者。一旦领导者无法继续工作，其他竞争者就能立刻竞争上岗，而无须等待较长的创建时间。在Kubernetes 中，锁就是一个资源对象，目前支持的资源是 Endpoint 和 Configmap。控制器的锁在 kube-system Namespace 下名为kube-controller-manager 的Endpoint 对象中。

与控制器类似，调度器也是采用 Leader Election 的主备模式，通过 kube-system Namespace 下名为kube-scheduler 的Endpoint 对象进行领导者仲裁。调度器监听 API Server 处Pod 的变化，当新的Pod 被创建后，如果其Pod 的spec.nodeName 为空，就会根据这个Pod 的Resouces、Affinity 和Anti-Affinity 等约束条件和Node 的实时状态等为该Pod 选择最优节点，然后更新节点名字到Pod 的spec.NodeName 字段。接下来后续的工作就由节点上的kubelet 接管了。

由于调度器在整个系统中承担着 “承上启下” 的重要功能，所以调度器的性能也就容易成为系统的瓶颈。在Kubernetes 1.12 之前，调度器做Predicate 时都是检查所有的节点的。1.12 版本添加了一个新的特性，允许调度器在发现一定数量的候选节点后，暂时停止寻找更多的候选节点， 这会提高调度器在大集群中的性能。

一个新的Node 加入集群是非常容易的，在节点上安装kubelet、kube-proxy、容器运行时和网络插件服务，然后将kubelet 和kube-proxy 的启动参数中的API Server URL 指向目标集群的API Server 即可。API Server 在接受kubelet 的注册后，会自动将此节点纳入当前集群的调度范围，这样Pod 就能调度该节点了。

如果kubelet停止汇报这些信息，那么NodeLifecycle 控制器将认为kubelet 已经不能正常工作，会将Node 状态设置为Unknown，并在一段时间后开始驱逐其上的Pod 对象。


容器沙箱是 “pause” 容器的抽象概念，有时也称为infra 容器，与用户容器 “捆绑” 运行在同一个 Pod 中，共享 CGroup、Namespace 等资源，与其他 Pod 资源隔离。在PodSandbox 中运行一个非常简单的pause 进程，它不执行任何功能，一启动就永远把自己阻塞住了（pause 系统调用）。容器沙箱最大的作用是维护 Pod 网络协议栈。

在创建容器之前，kubelet 首先会调用容器运行时为该Pod 创建容器沙箱，容器运行时为容器沙箱设置网络环境。当容器沙箱成功启动后，kubelet 才会调用容器运行时在该容器沙箱的网络命名空间（Net Namespace）中创建和启动容器。用户的容器可能因为各种原因退出，但是因为有容器沙箱存在，容器的网络命名空间不会被摧毁，当重新创建用户容器时，无须再为它设置网络了。

kubelet 并不是直接进行容器操作的，如图1-14 所示。它都是通过容器运行时的接口（Container Runtime Interface，CRI）调用容器运行时对容器和镜像进行操作的，例如创建、启动、停止和删除容器，下载镜像等。容器运行时的选用，这里有多条路可选：使用内置的dockershim 和远端的容器运行时等。目前默认情况下，kubelet 是通过内置的dockershim 调用 Docker 来完成容器操作的。我们也可以指定 remote 模式（通过参数--container-runtime 来指定），使用外部的遵循CRI 的容器运行时。虽然kubelet 不直接参与容器的创建与运行，但是它是管理和监控该节点上Pod 及Pod 中容器 “生老病死” 的核心。

kubelet 还会启动一个PLEG（Pod Lifecycle Event Generator）线程，每秒钟重新查询一次容器运行时容器的状态，更新Pod 的缓存，并根据容器的状态产生同步的事件。

除管理Pod 外，kubelet 还有很多其他功能：
● 对容器的Liveness 和Readiness 进行检测

Readiness 用来探测容器中的用户进程是否处于 “可服务状态”，如果kubelet 检测容器当前处于 “不可服务状态”，则kubelet 不会重启容器，但会把Pod 中的容器状态更新为ContainersReady=false。这对Service 的高可用而言非常重要。如果Pod 的容器处于 “不可服务状态”，Endpoint 控制器就会将该Pod 的IP 地址从Endpoint 中移除，该Pod 将不能再接收任何用户请求。

● 保护节点不被容器抢占所有资源。如果镜像占用磁盘空间的比例超过高水位（默认值为90%!，(MISSING)可以通过参数ImageGCHighThresholdPercent 进行配置），kubelet 就会清理不用的镜像。当节点CPU、Memory 或磁盘少于某特定值或者比例（由参数EvictionHard 配置）时，kubelet 就会驱逐低优先级的Pod（例如BestEffort 的Pod）。通过这些操作，保障节点上已有的Pod 能够在保证的QoS（Quality of Service）下继续正常运行。


处理Master 节点下发到本节点的任务，比如exec、logs 和attach 等请求。API Server是无法完成这些工作的，此时API Server 需要向kubelet 发起请求，让kubelet 完成此类请求处理。

kube-proxy 也是一个“控制器”。它也从API Server 监听Service 和Endpoint对象的变化，并根据Endpoint 对象的信息设置Service 到后端Pod 的路由，维护网络规则，执行TCP、UDP 和SCTP 流转发。

kube-proxy 有两种模式都可以实现流量转发，分别是iptables 模式和IPVS（IP Virtual Server）模式（可以通过参数--proxy-mode 来指定）。默认是iptables 模式，该模式是通过每个节点上的iptables 规则来实现的。我们可以通过iptables 命令查看相关的iptables rules：

当我们查询到规则③时，它将有33.33%!的(MISSING)概率命中，并跳转到KUBE-SEP-RXBFMC7CATPNMAHP。如果规则③未命中，则接下来我们考虑规则④，它将有50%!的(MISSING)概率进入Chain KUBE-SEP-CCTNN4A277RJLBDD。如果此条仍没有命中，就会进入Chain KUBE-SEP-HJGBTFNTDVVP5Q3I。因此分别进入这三个Chain 的概率是一样的，kube-proxy 也是利用iptables 的这一特性实现流量的负载均衡。

随着Service 数量的增大，iptables 模式由于线性查找匹配、全量更新等特点，其性能会显著下降。从Kubernetes 的1.8 版本开始，kube-proxy 引入了IPVS 模式，IPVS 与iptables同样基于netfilter，但是采用的是哈希表而且运行在内核态，当Service 数量达到一定规模时，哈希表的查询速度优势就会显现出来，从而提高Service 的服务性能。

可以通过ipvsadm 命令查看IPVS 模式下的转发规则：

容器运行时是真正删除和管理容器的组件。容器运行时可以分为高层运行时和底层运行时。高层运行时主要包括Docker、Containerd 和Cri-o，底层运行时包含运行时runc、kata 及gVisor。

Docker 是Kubernetes 支持的第一个容器运行时，kubelet 通过内嵌的DockerShim 操作Docker API 来操作容器，进而达到一个面向终态的效果。在这之后，又出现了一种新的容器运行时——rkt，它也想要成为 Kubernetes 支持的一个容器运行时，当时它也合到了Kubelet 的代码之中。这两个容器运行时的加入使得Kubernetes 的代码越来越复杂、难以维护。

1.5 版本后，Kubernetes 推出了CRI（Container Runtime Interface）接口，把容器运行时的操作抽象出一组接口，如图1-17 所示。Kubelet 能够通过CRI 接口对容器、沙盒及容器镜像进行操作。

Docker内部关于容器运行时功能的核心组件是Containerd，后来Containerd 也可以直接与kubelet通过CRI 对接，独立在Kubernetes 中使用。相对Docker 而言，Containerd 减少了Docker所需的处理模块Dockerd 和Docker-shim，并且对Docker 支持的存储驱动进行了优化，因此在容器的创建、启动、停止、删除，以及对镜像的拉取上，都具有性能上的优势。架构的简化同时也带来了维护的便利。

在Kubernetes 中，提供了一个轻量的通用容器网络接口CNI（Container Network Interface），专门用于设置和删除容器的网络连通性。容器运行时通过CNI 调用网络插件来完成容器的网络设置。

Flannel 是由CoreOS 开发的项目，是CNI 插件早期的入门产品，简单易用。Flannel使用Kubernetes 集群现有的etcd 集群来存储其状态信息，从而不必提供专用的数据存储，只需要在每个节点上运行flanneld 来守护进程。

Calico 以其性能、灵活性和网络策略而闻名。不仅涉及在主机和Pod 之间提供网络连接，而且还涉及网络安全性和策略管理。如图1-20 所示，对于同网段通信，基于第3 层，Calico 使用BGP 路由协议在主机之间路由数据包，使用BGP 路由协议也意味着数据包在主机之间移动时不需要包装在额外的封装层中。

对于跨网段通信，基于IPinIP 使用虚拟网卡设备tunl0，用一个IP 数据包封装另一个IP 数据包，外层IP 数据包头的源地址为隧道入口设备的IP 地址，目标地址为隧道出口设备的IP 地址。


网络策略是Calico 最受欢迎的功能之一，它通过ACLs 协议和kube-proxy 来创建iptables 过滤规则，从而实现隔离容器网络的目的。此外，Calico 还可以与服务网格Istio集成，在服务网格层和网络基础结构层上解释和实施集群中工作负载的策略。这意味着您可以配置功能强大的规则，以描述Pod 应该如何发送和接收流量、提高安全性，以及加强对网络环境的控制。Calico 属于完全分布式的横向扩展结构，允许开发人员、管理员快速和平稳地扩展部署规模。对于性能和功能（如网络策略）要求高的环境，Calico 是一个不错的选择。

我们已经在1.2.5 节总结了从Deployment 对象到Pod 运行这个过程中，各个控制器是如何协同工作的。接下来，我们再总结一下从Pod 创建到容器运行起来的整个周期中，各个模块做了哪些工作，Pod 的创建流程如图1-21 所示。

图1-21　Pod 的创建流程

（1）用户或者控制器通过kubectl、Rest API 或其他客户端向API Server 提交Pod 创建请求。

（2）API Server 将Pod 对象写入etcd 中进行永久性存储。如果写入成功，那么API Server会收到来自etcd 的确认信息并将创建结果返回给客户端。
（3）API Server 处就能反映出Pod 资源发生的变化，一个新的Pod 被创建出来。


（4）Scheduler 监听到API Server 处新的Pod 被创建。它首先会查看该Pod 是否已经被调度（spec.nodeName 是否为空）。如果该Pod 并没有被调度到任何节点，那么Scheduler会给它计算分配一个最优节点，并把它更新到spec.nodeName 中，从而完成Pod 的节点绑定。

6）kubelet 也会一直监听API Server 处Pod 资源的变化。当其发现Pod 被分配到自己所在节点上（自身节点名称和Pod 的spec.nodeName 相等）时，kubelet 将会调用CRI gRPC向容器运行时申请启动容器。

（7）kubelet 首先会调用CRI 的RunPodSandbox 接口。Containerd 要确保PodSandbox（即Infra 容器）的镜像是否存在。因为所有PodSandbox 都使用同一个pause 镜像，如果节点上已经有运行的 Pod，那么这个 pause 镜像就已经存在。接着它会创建一个新的Network Namespace，调用CNI 接口为Network Namespace 设置容器网络，Containerd 会使用这个Network Namespace 启动PodSandbox。

kubelet 才会在PodSandbox 下请求创建容器。这里kubelet 会先检查容器镜像是否存在，如果容器镜像不存在，则调用CRI 的PullImage 接口并通过Containerd 将容器镜像下载下来。
（9）当容器镜像下载完成后，kubelet 调用CRI 的CreateContainer 接口向容器运行时请求创建容器。

11）无论容器是否创建和启动成功，kubelet 都会将最新的容器状态更新到Pod 对象的Status 中，让其他控制器也能够监听到Pod 对象的变化，从而采取相应的措施。


（2）Pod 对象不会立刻被API Server 删除。API Server 会在Pod 中添加deletionTimestamp和deletionGracePeriodSeconds（默认值是30s）字段，并将Pod 的Spec 更改写回etcd 中。

（4）当kubelet 监听到API Server 处的Pod 对象的deletionTimestamp 被设置时，就会准备删除这个Pod（killPod）。

5）kubelet 首先会停止Pod 内的所有容器，调用CRI 的StopContainer 接口向容器运行时发起停止容器的请求。这里我们同样以Containerd 为例，Containerd 会先调用runC 向容器发送SIGTERM 信号，容器停止或者deletionGracePeriodSeconds 超时，再发送SIGKILL信号去杀死所有容器进程，完成容器的停止过程。

9）当Pod 内所有容器被停止后，kubelet 可以通过StopPodSandbox 停止PodSandbox。Containerd 首先调用CNI 将容器的网络删除，然后停止PodSandbox。PodSandbox 停止后，kubelet 会进行一些清理工作，例如清除Pod 的CGroup 等。
（10）如果Pod 上有finalizer，即使Pod 的容器和PodSandbox 被全部停止，这个Pod也不能消失，需要等到其他控制器完成相关的清理工作，并将Pod 上的finalizer 删掉。

Pod 的QoS 级别有三种：
（1）Guarantee：Pod 的每个容器申请的CPU 使用量与Memory 的limits 值和requests值相等。如果容器只指定了Memory 或者CPU 的limits，没有指定requests，那么Kubernetes会自动给它填写一个与limits 值相等的requests。Pod 的QoS 是Guarantee 的。Guarantee是Kubernetes 的最高优先级。Kubelet 不会主动杀死Pod，除非它们所用的资源超过了Pod的limits。

2）Burstable：Pod 内至少有一个容器指定了Memory 或CPU 的requests 或limits，Pod 不满足Guarantee 的条件，requests 的值和limits 的值不相等。requests 和limits 的配置规则如下：

● 如果容器只指定了CPU 的requests，没有指定limits，那么当节点压力不大时，该容器可以使用超过requests 值的节点上的剩余可用的CPU。


● 如果容器只指定了Memory 的requests，没有指定limits，那么当容器的内存使用超过requests 时，容器可以使用节点上剩余的内存，但是当节点内存不足以满足其他容器时，该容器可能会被 “杀” 掉。
（3）BestEffort：Pod 的每个容器都没有Memory 和CPU 的requests 或limits 的值。这个级别的Pod 的优先级是最低的。当系统有了CPU 和Memory 的压力时，Pod 会率先被“杀” 掉。

节点亲和性的使用方式有两种：nodeSelector 和nodeAffinity。nodeSelector 是一个最简单的方法，采用label selector 选择节点。如果Pod 指定了nodeSelector，那么它将会被调度到标签满足dedicated=demo 且run=nginx 的节点上。

对于taints 的effect，除了NoSchedule，还可以指定PreferNoSchedule 和NoExecute。PreferNoSchedule 是NoSchedule 的软限制，它告诉调度器，尽量不要将没有此Tolerations的Pod 调度到节点上。如果我们将NoExecute 的taints 添加到节点上，并且已经运行在此节点上的Pod 没有相应的Tolerations，那么Pod 将会立刻被驱逐。


● 对DaemonSet 来说，它需要在各种 “花式taints” 的节点上运行，这里有种 “霸道” 的Tolerations 方式：当key 为空时，operator 为Exists，这意味着，只要所有的key存在就可以容忍任何taints。

内置的taints 如下：
● node.kubernetes.io/not-ready：节点状态是NotReady。

● node.kubernetes.io/unreachable：NodeController 不清楚节点状态。
● node.kubernetes.io/out-of-disk：节点的磁盘空间不足。
● node.kubernetes.io/memory-pressure：节点有内存压力。
● node.kubernetes.io/disk-pressure：节点磁盘有压力。
● node.kubernetes.io/network-unavailable：节点的网络不可用。
● node.kubernetes.io/unschedulable：节点不可调度。
● node.cloudprovider.kubernetes.io/uninitialized：当kubelet 用external 的provider 时，taints 会被添加到节点上。在cloud-controller-manager 初始化该节点后，即可移除taints。

### 第2章 计算节点管理

容器在运行时需要复用主机内核，因此主机内核的选择决定了所有运行在其之上的应用可用的内核版本。

### 2.3 容器核心技术

事实上，容器所依托的技术并非新技术，而是早已存在多年的两种相对较成熟的技术：Namespace 和CGroups。科技界的技术创新经常出现这种情形：几种成熟技术的组合往往会成就一个巨大创新。容器技术就是通过组合Namespace 和CGroups 这两个耳熟能详的技术，对云计算进行根本性的变革。

Linux 内核从2.4.19 版本开始引入Namespace 技术，译为 “命名空间”，它提供了一种内核级别的系统资源的隔离方式。系统可以为进程分配不同的Namespace，并保证不同的Namespace 资源独立分配、进程彼此隔离，即不同的Namespace 下的进程互不干扰。

简言之，cpu.shares 主要用于表示当系统CPU 繁忙时，给该CGroup 分配的CPU 时间份额。

OCI 标准的设计要考虑以下几方面：
● 操作标准化：容器的标准化操作包括使用标准流程创建、启动和停止容器，使用标准文件系统工具复制和创建容器快照，使用标准化网络工具进行下载和上传。

为了兼容OCI 标准，Docker 也做了架构调整。将容器运行时相关的核心代码从Docker引擎剥离出来，形成Containerd，并将它贡献给CNCF。由Containerd 向Docker 提供管理容器的API，而Docker 引擎则专门负责上层的封装编排，提供对Images、Volumes、Network及Builds 操作的API

图2-5　Docker 框架

另外，kubelet 与Docker 及Rocket 紧密耦合，它们的接口变化会影响Kubernetes 的稳定性。于是从1.5 版本开始，Kubernetes 推出了CRI（Container Runtime Interface）接口，有了CRI 接口无须修改kubelet 源代码就可以支持更多的容器运行时，并逐步将内置的Docker 和rtk 从Kubernetes 源代码中移除

同时将CNI 的实现迁到CRI Runtime 内，如图2-6 所示。也就是说，外部的容器运行时除了实现CRI 接口真正负责管理镜像和容器的生命周期，还需要实现CNI，负责容器配置网络。

CRI 其实就是一组gRPC 接口，包括两类：RuntimeService 和ImageService，如图2-7所示，RuntimeService 包括一组对容器沙箱和容器查询进行操作和管理的接口，一组与容器交互的接口，以及运行时版本和状态查询的接口

对文件的添加、修改、删除和读写都只发生在容器的可写层，因此不同容器对文件的修改都相互独立、互不影响。

1.AUFS
AUFS 是一种联合文件系统（Union Filesystem），是文件级的存储驱动，支持将节点上的多个目录挂载到同一挂载点。这里的目录等同于AUFS 中的Branch 或者Docker 中的层的概念，详情如图2-13 所示。
￼

### 2.4 节点资源管理

Kubernetes 计算节点资源管理方案已渐趋成熟：具体体现在状态汇报、资源预留、防止节点资源耗尽的防御机制驱逐及容器和系统资源的配置。

Unknown 表示节点控制器在最近40s 内没有收到节点的消息。调度器在调度Pod 时会过滤掉所有Ready 状态为非True 的节点。

以下三个参数可以控制kubelet 更新节点状态频率：
● NodeStatusUpdateFrequency。
● NodeStatusReportFrequency。
● NodeLeaseDurationSeconds。

容量资源（Capacity）是指 kubelet 获取的计算节点当前的资源信息。CPU 是从/proc/cpuinfo 文件中获取的节点CPU 核数；memory 是从/proc/memoryinfo 中获取的节点内存大小；ephemeral-storage 是指节点根分区的大小

依据Pod Spec 中对资源请求定义的不同，Pod 可划分为不同的QoS 等级：Guaranteed、Burstable 和BestEffort。kubelet 对不同QoS 等级的Pod 会做不同的处理。

Kubernetes 调度Pod 时，会判断当前节点正在运行的Pod 的CPU Request 的总和，再加上当前调度Pod 的CPU request，计算其是否超过节点的CPU 的可分配资源。如果超出，则该节点应被过滤掉。换言之，调度器会判断当前节点的剩余CPU 资源是否满足Pod 的CPU Request。


kubectl describe node 命令可显示当前节点的CPU 和内存使用率。命令返回结果展示了每个Pod 的资源请求、申请资源占整个节点的比例，以及节点的总资源等信息，如下所示：

● 如果Pod 定义中的nodeName 直接指定了目标节点，那么nodeName 被创建后，会直接被节点上的kubelet 监听到，无须通过kube-scheudler 进行调度。kubelet 在启动该Pod 的容器之前，会启动准入机制计算当前节点空闲的CPU 资源是否能够满足Pod 需求，如果不满足则停止启动，并将Pod 标记为OutOfCPU 状态。

● 如果选择Containerd 作为运行时，那么日志的管理是通过kubelet 定期（默认为10s）执行du 命令，来检查容器日志的数量和文件的大小的。每个容器日志的大小和可以保留的文件个数， 可以通过 kubelet 的配置参数 container-log-max-size 和container-log-max-files 进行调整。

Kubernetes 允许用户通过hostPath 卷将计算节点的文件目录挂载给容器，但目录挂载完成后，系统无法限制用户往该卷写入的数据的大小。这就导致删除容器时，如果不主动进行数据清理，数据就会遗留在节点上，占用磁盘空间。

基于文件系统的Quota 特性监控emptyDir 卷的使用情况，相比du 更精准。du 无法监测文件打开后尚未关闭就被删除的情况，但是Quota 能够跟踪到已被删除而文件描述符尚未关闭的文件。

容器会共享节点的根分区和运行时分区。如果容器进程在可写层或emptyDir 卷进行大量读写操作，就会导致磁盘I/O 过高，从而影响其他容器进程甚至系统进程。

针对磁盘的资源管理，磁盘空间使用率和磁盘I/O 是非常必要且有用的节点监控指标。在节点根分区和运行时分区磁盘使用率到达一定的门限，或者一段时间内I/O 使用率一直比较高的情况下，可以先触发告警，再进行针对性的排查，寻找问题出现的原因，并采取相关措施进行后续防范。

kubelet 会把相关的限制信息通过运行时传递给CNI 网络插件，再由CNI 网络插件通过Linux Traffic Control 限制带宽。同一计算节点的所有容器和主机共享网络带宽、相互竞争。如果网络负载较高的多个容器被部署到同一节点，则会导致系统的CPU 负载较高、网络延时增大、抖动增加等问题。

当系统进程数不足时，应用程序尝试创建子进程，此时可能会出现设备空间不足的错误信息，一般我们会将其归咎于磁盘空间不足。然而，其主要原因是，在内核实现过程中如果无法创建新进程，那么某些代码分支会返回NOSPACEERR，从而导致产生这样的错误。

### 2.5 存储方案

Kubernetes 支持多种存储类型，可按照数据持久化方式分为临时存储（如emptyDir）、半持久化存储（如hostPath）和持久化存储（包含网络存储和本地存储）。

目前Kubernetes 提供了FlexVolume 和容器存储接口（Container Storage Interface，即CSI）两种out-of-tree 存储插件。在1.13 版本之后的版本，CSI 进入GA(General Available)阶段，现已成为默认的推荐方式。FlexVolume 还会继续维护，但新的特性只会加在CSI上。

FlexVolume 是指Kubernetes 通过调用计算节点的本地可执行文件与存储插件进行交互。不同的存储驱动对应不同的可执行文件，该可执行文件可以实现FlexVolume 存储插件需要的attach/detach、mount/umount 等操作。部署与Kubernetes 核心组件分离的可执行文件，使得存储驱动有独立的升级和部署周期，解决了in-tree 存储插件的强耦合问题。

● FlexVolume 插件需要宿主机用root 权限来安装插件驱动。
● FlexVolume 存储驱动需要宿主机安装attach、mount 等工具，也需要具有root 访问权限。

Kubernetes 社区引入了一个in-tree 的CSI 存储插件，用于用户和外挂的CSI 存储驱动的交互。相对于FlexVolume 基于可执行文件的方式，CSI 通过RPC 的方式与存储驱动进行交互。在设计CSI 的时候，Kubernetes 对CSI 存储驱动的打包和部署要求很少

常见的临时存储主要有emptyDir 卷。
emptyDir 是一种经常被用户使用的卷类型，顾名思义，“卷” 最初是空的。当Pod 从节点上删除时，emptyDir 卷中的数据也会被永久删除。但当Pod 的容器因为某些原因退出再重启时，emptyDir 卷内的数据并不会丢失。

emptyDir 也可以通过将 emptyDir.medium 字段设置为 “Memory” 来通知Kubernetes 为容器安装tmpfs，此时数据被存储在内存中，速度相对于本地存储和网络存储快很多。但是在节点重启的时候，内存数据会被清除；而如果存在磁盘上，则重启后数据依然存在。另外，使用tmpfs 的内存也会计入容器的使用内存总量中，受系统的CGroup限制。

因为emptyDir 的空间位于系统根盘，被所有容器共享，所以在磁盘的使用率较高时会触发Pod 的eviction 操作，从而影响业务的稳定。

使用hostPath 卷需要注意如下几点：
● 使用同一个目录的Pod 可能会由于调度到不同的节点，导致目录中的内容有所不同。
● Kubernetes 在调度时无法顾及由 hostPath 使用的资源。
● Pod 被删除后，如果没有特别处理，那么hostPath 上写的数据会遗留到节点上，占用磁盘空间。

PVC 由用户创建，代表用户对存储需求的声明，主要包含需要的存储大小、存储卷的访问模式、stroageclass 等类型，其中存储卷的访问模式必须与存储的类型一致，包含的三种类型如表2-17 所示。

PV 由集群管理员提前创建，或者根据PVC 的申请需求动态地创建，它代表系统后端的真实的存储空间，可以称之为卷空间。

用户使用Cinder 存储的工作流程
下面对图2-24 中的10 个部分分别进行介绍：
（1）创建PVC：用户创建了一个使用Cinder 存储的PVC。
（2）创建卷：当PV Controller 监听到该PVC 的创建时，调用Cinder 的存储插件，从Cinder 存储后端申请卷。
（3）创建PV：pv controller 创建PV，PV 中包含如下主要字段：
● spec.accessMode: 卷的访问模式。
● spec.capactiy.storage: 卷的大小。
● spec.cinder.volumeID : Cinder 卷的ID

（4）绑定PVC/PV：在PV 成功创建后，PV Controller 会将PVC 和PV 进行绑定，PVC和PV 的状态都设置为Bound。
（5）创建Pod：用户创建Pod，并在其中申明使用这个PVC。
（6）调度Pod：kube-scheduler 将Pod 调度到某个节点。
（7）attach 卷：Attach-Detach Controller 检测到Pod 已经被调度到某个节点，遂将该Pod 使用的卷attach 到对应的节点。
（8）更新节点卷的attach 信息：Attach-Detach Controller 将卷的相关信息添加到节点对象的node.status.volumesAttached 中。
（9）更新节点卷的InUse 信息：当kubelet 监听到有Pod 已经调度到本机后，在汇报节点状态的时候，将Pod 使用的卷信息添加到节点对象的node.status.volumesInUse 中。

（10）挂载卷，启动容器：kubelet 从节点对象上获知该卷已经执行了attach，在节点上找到该卷的设备信息，开始对盘做mount 操作。


Pod 删除流程为上面流程的逆过程，具体如下：
（1）Pod 接收delete 请求后，kubelet 会中止容器，对磁盘进行umount 操作，并对Pod执行删除（将grace-period 设置为0）操作，这样Pod 就从API Server 中被删除了。
（2）kubelet 在汇报节点状态的时候，将该卷的相关信息从节点对象的 node.status.volumesInUse 中删除。

在Kubernetes 支持的存储类型中，本地存储得到支持的时间比较晚。与网络存储相比，本地存储有一个很大的不同：网络存储可以attach 到不同的节点上，但是本地存储与节点是一一绑定的，即如果某个本地存储的PVC 和PV 绑定，那么Pod 就必须调度到该PV 所在的节点。

使用本地存储的工作流程
下面对图2-27 中的9 个部分分别进行介绍。
（1）创建PV：通过Local-volume-provisioner daemonset 创建本地存储的PV。
（2）创建PVC：用户创建PVC，由于它处于pending 状态，所以kube-controller-manager并不会对该PVC 做任何操作。
（3）创建Pod：用户创建Pod。
（4）Pod 挑选节点：kube-scheduler 开始调度Pod，通过PVC 的resources.request.storage和volumeMode 选择满足条件的PV，并且为Pod 选择一个合适的节点。

9）mount 卷并启动容器：kubelet 监听到有Pod 已经调度到节点上，对本地存储进行mount 操作，并启动容器。
Pod 被创建后，用户可能会因为升级等原因将Pod 删除再创建，而此时由于PVC 已经是Bound 状态，所以重新创建的Pod 只能被调度到本地存储PV 所指定的节点上。这样可能会引发如下问题：

1）如果PV 所指向的节点出现问题，处在NotReady 状态，导致Pod 无法调度该节点，那么Pod 就会无节点可调度，处于pending 状态

PVC 和PV 的绑定需要kube-scheduler 的参与。Pod 如果直接指定了nodeName，就不会被kube-scheduler 调度，PVC 也不会和PV 绑定，因此Pod 的容器就不会被kubelet运行起来。

### 2.6 节点调优

2.6.2.2　CPU 绑定
在多核处理器中，每个核都有专属的缓存，而数据一次只能保留在一个CPU 的缓存中，这样可以有效地保证不同核数据的一致性。当数据被导入一个CPU 的缓存时，其他CPU 中该数据的缓存都会失效。如果进程/线程在多个核间频繁切换，会导致缓存经常失效，进而对性能产生影响。

2.6.2.3　中断处理
在Linux 系统中，硬中断和软中断都可以打断当前程序的执行，如果中断数量比较多，系统的性能会大幅下降。因此，可以通过减少中断的处理时间或者合并中断、减少中断数量的方式来提高性能。

在Linux 系统中以页为单位来分配和管理内存，每个页的大小默认是4KB。为了提高对物理内存的访问速度，也可以将页分为2MB 或者1GB 的大页（Huge Page）来进行管理。

2.6.5　网络性能
2.6.5.1　数据包处理流程
理解Linux 操作系统处理数据包的机制是网络优化的第一步。Linux 操作系统分为内核态和用户态：内核态网络协议栈处理数据，用户态应用消费数据。再加上网卡驱动的介入，Linux 系统处理数据包的大致流程如图2-31 所示。

下面详细描述一下Linux 系统处理数据包的流程。
（1）网卡接收数据包后，首先需要做数据包校验，比如判断该数据包的目标地址是否与网卡地址匹配，数据包是否完整等。

（2）数据包校验完成后，通过直接内存访问（DMA）和网卡驱动将数据直接写入系统内存。DMA 内存地址由网卡驱动初始化时的分配，DMA 的内存写入由网卡独立完成，无须CPU 介入。

3）网卡发起硬中断，通知CPU 有数据被接收。

4）系统查询中断表，调用中断处理函数。Linux 中断处理函数分为两个部分，上半段（Top Half）和下半段（Bottom Half）。中断处理函数执行时，CPU 无法响应其他中断，如果中断处理时间较长，那么在此期间其他中断请求无法被响应。因此上半段时间应该尽快结束，以便释放CPU 处理更多中断事件。

（5）网卡驱动禁用网卡中断，以避免网卡反复发起中断，浪费CPU 时间。
（6）网卡驱动发起软中断，至此，硬中断处理函数结束，CPU 可以重新响应硬中断。

11）调用协议栈相应的函数，将数据包交给协议栈处理，协议栈处理数据时，只需修改skb_buff 中的数据包头。
（12）待内存中的所有数据包处理完成，或者执行poll 的配额完成后，启用网卡的硬中断。

通过mpstat 命令能查看系统用于处理irq 的CPU 开销，如果大部分CPU 用来响应中断，只剩非常少的空闲CPU，例如5%!，(MISSING)那么会有非常大的概率出现因不能及时处理而导致的丢包现象。

2.6.5.2　网卡offload
为提高对网络数据的处理，网卡上集成了很多硬件功能来处理特定的网络数据包。将原来需要消耗软件操作的步骤分配给网卡执行，从而减少CPU 的处理时间，增加网络吞吐率，该行为称为offload。网卡offload 有以下三个常见的功能：

2.6.5.3　网卡多队列RSS
Receive Side Scaling（RSS）是指网卡接收数据包后，利用多CPU 处理数据包的技术将不同的数据包发送至不同的接收队列。网卡根据数据包头将它们归并为不同的数据流，同一数据流的所有数据包会被分发至特定的接收队列，不同接收队列的数据包被不同CPU处理。在数据包较多时，多CPU 同时处理数据包的机制保证了数据传输的性能不会因为单个CPU 过忙而显著降低。

当客户端和服务器端应用进行网络通信时，应用程序只负责组装请求包或者响应包并发送，应用层的数据包的大小不受限制，它也无须关心数据如何传输，传输控制是交由下层协议栈处理的。然而下层协议栈传输数据时，是不可能将应用层数据包直接传输的。这是因为网络传输的带宽限制和网络的不可靠性，若将大数据包作为一个整体传输，则出错重试的开销过大，因此将大数据包拆分成小的碎片，分批传输是明智之举。这样即使某个碎片传输失败，也只需重新传输该碎片即可。

任何基于网络传输的数据包有最大传输单元的限制（Maximum Transmission Unit，即MTU），MTU 是指在网络层传输的最大数据报单元，MTU 的大小通常由链路层设备决定。比如，最常见的以太网设备帧的大小是1518 字节，去掉链路层包头，IP 层最多只能使用1500 字节，这就是MTU 的默认限制。

数据传输的两个重要目标是可靠和高效，为实现这两个目标，TCP 引入了数据包序号、应答（Acknowledge，即ACK)机制和窗口机制。
TCP 会将缓冲区中的待发送数据包按顺序编号，并按序号发送数据，然后暂停传输，等待接收方确认。

为控制传输速率，操作系统网络协议栈在数据发送方维护发送缓冲区，在接收方维护接收缓冲区。TCP 引入滑动窗口（Sliding Window）机制实现数据传输的流量控制。滑动窗口的大小在通信双方建立连接时协商确定，并且在通信过程中不断更新，故取名为滑动窗口。它本质上是描述接收方数据缓冲区大小的数据，发送方根据接收方窗口的大小计算能够同时发送的数据包数量。

当发送方发送数据时，除了考虑接收窗口的大小，还需要考虑链路拥塞情况，拥塞控制（Congestion Control）就是为了解决此问题而引入TCP 的。

### 第3章 构建高可用集群

为了实现高可用（High Availability，即HA）的目标，对基础云设施来说，在设计和搭建集群时应该考虑两个方面：在某个故障发生后，服务是否依然可用；灾难性故障造成服务不可用时，能否通过故障转移或者数据修复手段恢复服务。

（2）软件故障（Software Failure）。
软件会越来越复杂和庞大。软件的故障率与人息息相关。在软件项目构建与发展的过程中，人为错误难以避免，下文只列出最关键的几点：
● 开发团队技能水平：经验丰富的工程师往往能在早期发现和规避问题，减少不必要的后期变更。
● 代码复杂度：简单的代码有助于提升代码质量。
● 软件开发和运营变更流程控制：设计和代码审查、系统变更控制等。

### 3.1 高可用的常用手段

因此，在架构设计之初，就应充分考虑服务的高可用性，充分利用常规的高可用手段来保障后续迭代过程的平稳。

下面介绍一些提高系统可用性的常规方法。
1.服务冗余
每个服务运行多个实例，牺牲更多资源换取更高的可用性。

3.服务拆分
如图3-3 所示，将一个大的系统拆分成多个独立的小模块，各个模块之间相互调用，是减少故障影响范围的主要手段。当一个模块出了问题时，只会影响系统的局部服务。模块之间的调用尽量异步化，调用的响应时间越长，存在的超时风险就越大；逻辑越复杂、执行的步骤越多，存在的失败风险就越大。可以在业务允许的情况下，将复杂的业务进行拆分以降低复杂度。读写分离是拆分的一种方式。写请求依赖主数据设备，读数据依赖备数据设备。当出现故障时，可以只开发读服务，写服务暂时关闭，从而减少了故障的影响面。但需要关注数据的一致性问题。

存储高可用方案的本质是将数据复制到多个存储设备中，通过数据冗余的方式来实现高可用。但是，无论是正常情况下的传输延时，还是异常情况下的传输中断，都会导致系统的数据在某个时间点出现不一致。数据的不一致又会导致业务出现问题。分布式领域中有一个著名的CAP（Consistency、Availability、Partition Tolerance，一致性、可用性、分区容错性）定理，从理论上论证了存储高可用的复杂度，也就是说，存储高可用不可能同时满足 “一致性、可用性、分区容错性”。

限流是服务器端出于自我保护的目的，限定自己所接收的并发请求上限的手段。超出流量控制上限的请求将被直接拒绝或者随机选择拒绝，以防止服务过载。

熔断是客户端在发出请求后，由于无法在固定期限收到预期目标，从而采取服务降级的手段。服务降级的常用手段包括返回错误提示、排队、关闭核心业务调用等，是客户端在上游服务出现故障时，避免将局部故障扩大到全局的自保手段。

6.负载均衡
负载均衡已经是高可用架构中必不可少的手段之一，可以通过按权重负载均衡、按地域就近访问等手段提升系统的整体销量，避免因为过载而导致整个系统全地域失效。

7.变更流程管理
变更是影响可用性的最大因素，因此完备的流程管理（包括流程标准化和工具集的支持）直接影响变更的风险等级。对已发生的变更来说，为降低变更潜在的影响，灰度发布是对可用性的最后保障。

8.服务监控
完善的监控系统对整个系统的可靠性和稳定性是非常重要的，可靠性和稳定性是高可用的一个前提。服务的监控更多是对风险的预判，在出现不可用之前就能发现问题，

一套完善的服务监控体系需要涵盖服务运行的各个层次：基础设施监控（例如网络、交换机、路由器等底层设备的丢包错包情况）、系统层监控（例如物理机、虚拟机的CPU、内存利用情况）和应用层监控（例如请求数量和延时、服务性能和错误率等）等。

### 3.3 控制平面的高可用保证

控制平面所在的节点，应确保在不同机架上，以防止因为某些机架的交换机或电源出问题，造成所有的控制面节点都无法工作。保证控制平面的每个组件有足够的CPU、内存和磁盘资源，过于严苛的资源限制会导致系统效率低下，降低集群可用性。

应尽可能地将控制平面和数据平面解耦，确保控制平面组件出现故障时，将业务影响降到最低。

不同于传统的以表格形式存储数据的数据库，etcd 为每个记录创建一个数据页面，在更新一个记录时不会妨碍其他记录的读取和更新。如图3-5 所示，etcd 中有3 个数据，即3 个键值对，其键分别是 “/foo”“/bar/this”“/bar/that”，对应的值分别是数组[“i”,“am”,” array”]、整数42 和字符串 “i am a string”。客户端可利用restful API（通过数据的地址链接）对这些数据进行同步更新，而不会相互影响。这使etcd 非常容易应对高并发写请求的应用场景。

所有Master 节点上的etcd 实例组成etcd 集群，但API Server 仅与此节点本地的etcd 成员通信。

外部etcd 集群的高可用拓扑
这里API Server 有两种etcd 地址的配置方式：第一种是配置etcd 集群的负载均衡器的VIP 地址，具体连接到哪个etcd 实例由负载均衡策略决定；第二种是配置多个etcd 实例的地址，用逗号隔开，API Server 会按顺序依次尝试连接。

该拓扑将控制平面和etcd 成员解耦。丢失一个Master 节点对etcd 成员的影响较小，不会像堆叠式拓扑那样对集群冗余产生太大影响。但是，此拓扑所需的主机数量是堆叠式拓扑的两倍。具有此拓扑的集群至少需要三个主机用于控制平面节点，三个主机用于etcd集群。

至少需要3 个etcd 实例呢？原因有两个。
第一，如果 etcd 集群仅有一个成员，那么一旦这个成员出现故障，会导致整个Kubernetes 集群不可用。第二，基于raft 协议的etcd 的Leader 选举和数据写入都需要半数以上的成员投票通过确认，因此，集群最好由奇数个成员组成，以确保集群内部一定能够产生多数投票通过的场景。这也就是为什么etcd 集群至少需要3 个以上的成员。建议etcd集群成员数量不要超过7 个，推荐是3 个或5 个。个数越多，投票所需时间就越多，写的吞吐量会越低。虽然提高了读的性能和可用性，但是极大损伤了写的性能。

如若另一个成员再发生故障，集群将会变成只读的。只读的集群表象是平台变为静止。但因为集群中所有节点的kubelet 都无法不汇报状态，所以Controller Manager 中的Pod 驱逐控制器会将所有节点上的Pod 放入驱逐队列，一旦etcd 恢复，会导致大量Pod 被同时驱逐，从而造成服务器过载、服务不可用等事故。

当发现etcd 集群性能低下时，应如何调优etcd 性能呢？影响etcd 的性能（尤其是提交延迟）的因素主要有两个：网络延迟和磁盘I/O 延迟，即etcd成员之间的网络往返时间（Round Trip Time，RTT）和将数据提交到永久存储所需的时间。

2.减少磁盘I/O 延迟
对于磁盘延迟，典型的旋转磁盘写延迟约为10ms。对于SSD（Solid State Drives，固态硬盘），延迟通常低于1ms。HDD（Hard Disk Drive，硬盘驱动器）或网盘在大量数据读写操作的情况下延时会不稳定。因此，强烈建议使用SSD。同时，为了降低其他应用程序的I/O 操作对etcd 的干扰，建议将etcd 的数据存放在单独的磁盘内。

如果不可避免地，etcd 和其他业务共享存储磁盘，那么就需要通过ionice 命令对etcd服务设置更高的磁盘I/O 优先级，尽可能避免其他进程的影响，代码如下：

声明式系统是一把双刃剑。一方面，Kubernetes 基于此机制构建出故障转移、版本发布、扩容缩容等强大的功能。另一方面，数据的破坏或丢失，会导致控制器发起Pod 删除或重建等破坏性的行为。举个例子，etcd 中的Pod 数据不小心被损坏了，kubelet 将会把正在运行的Pod 清除，以确保与当前状态及数据库中的用户期望一致。作为Kubernetes 的“首脑”，确保写入效率和防止数据丢失是规划etcd 存储的主要目标。

在etcd 的默认工作目录下会生成两个子目录：wal 和snap。wal 用于存放预写式日志，其最大的作用是记录整个数据变化的全部历程。所有数据的修改在提交前都要先写入wal中。snap 用于存放快照数据。

官方推荐etcd 集群的备份方式是定期创建快照。与etcd 内部定期创建快照的目的不同，该备份方式依赖外部程序定期创建快照，并将快照上传到网络存储设备以实现 etcd数据的冗余备份。上传到网络设备的数据都应进行加密。

对于集群内部的客户端，应优先访问API Server 的ClusterIP，利用kube-proxy 建立的转发规则将流量送达API Server 处。这样，当外部的负载均衡器出现问题时，不会影响集群内部客户端访问API Server，也不会对集群内的客户端产生巨大影响。

Kubernetes 定义了两种类型的Admission Webhook，即Mutating Admission Webhook和Validating Admission Webhook。在API 请求进行身份验证和授权后，Mutating Admission Webhook 首先被调用，能够修改发送到API Server 的对象，填充自定义的默认值。在所有对象修改完成之后，API Server 对传入对象进行验证，然后调用 Validating Admission Webhook。Validating Admission Webhook 可以根据自定义策略允许或拒绝请求。

Mutating Admission Webhook 可以修改其允许的对象；Validating Admission Webhook允许拒绝请求。这些都是特殊的控制器。如果没有Admission Webhook 对API Server 进行扩展，就需要将这些代码编译到API Server 中，并且只能在API Server 启动时启用。Admission Webhook 的精髓在于减少了代码耦合，可以在运行时动态配置API Server 的扩展Admission 服务，用于接收准入请求并对其进行处理，让API Server 支持你所期望的所有功能。而且Admission 服务可以单独部署在集群内或集群外，可以单独对API Server 和Admission 服务进行部署和升级，增强了集群管理的灵活性。

3.设置合适的缓存大小
API Server 与etcd 之间基于gPRC 协议进行通信，gPRC 协议保证了两者在大规模集群中的数据能够高速传输。
gPRC 协议是基于HTTP2 协议的。HTTP2 通过stream 支持了连接的多路复用。一条TCP 连接可以包含多个stream，多个stream 发送的数据互不影响。在API Server 和etcd之间，相同分组的资源对象的请求共享同一个TCP 连接，组内不同资源对象的请求由不同的stream 进行传输。多路复用会引入资源竞争，流量控制可以保证stream 之间不会严重影响彼此，但是也限制了能支持的并发请求数量。

API Server 提供了集群对象的缓存机制，当客户端发起查询请求时，API Server 默认会将其缓存直接返回给客户端。缓存区大小可以通过参数“--watch-cache-sizes”进行设置。针对访问请求比较多的对象，适当设置缓存的大小，能够降低etcd 的访问频率，节省网络调用，减少etcd 集群的读写压力，从而提高对象访问的性能。

3.3.3　控制器高可用保证
Kubernetes 提供了Leader 选举机制，用以确保多个控制器的实例同时运行，并且只有Leader 实例提供真正的服务。其他实例处于准备就绪状态，如果Leader 出现故障，则取代Leader 以保证Pod 能被及时调度。此机制以占用更多资源为代价，提升了Kubernetes控制器的可用性。

Leader 选举的核心是利用Configmap、Endpoints 或Lease 对象实现分布式资源锁。当多个实例同时启动后，在运行任何业务逻辑之前，都会尝试读取该资源锁。

资源锁可保存在Configmap、Endpoints 和Lease 三种对象中。推荐使用Lease，因为Lease 对象本身就是用来协调租约对象的，其Spec 定义与Leader 选举机制需要操控的属性是一致的。使用Configmap 和Endpoints 对象更多是为了向后兼容，伴随着一定的负面影响。

3.3.4.1　数据加密传输
对于数据传输，Kubernetes 及其支持组件都应使用基于SSL/TLS 的HTTPS 协议来确保传输的安全性，特别是客户与API Server、API Server 与etcd、API Server 与kubelet、etcd 成员之间这种跨节点的数据通信。

服务端将自己的证书下发给客户端，让客户端验证自己的身份。客户端收到服务端传来的证书后，验证证书是否过期、服务端证书的CA 机构是否可靠、返回的公钥是否能正确解开返回证书中的数字签名、服务端证书上的域名是否和服务端的实际域名相匹配等。验证通过后，客户端取出证书中的服务端公钥，否则，中止通信。

如何判定服务端证书的CA 机构是否可靠呢？这要看客户端是否安装了此CA 机构的根证书。根证书是CA 认证中心给自己颁发的证书，是信任链的起始点。安装根证书意味着对这个CA 认证中心的信任。因此，通常我们把根证书简称为CA。

当kubelet 主动连接API Server 时，API Server 作为服务端，应使用服务端证书AS，kubelet 作为客户端，应使用客户端证书AC；当API Server 主动连接kubelet 时，kubelet作为服务端，应使用服务端证书KS，API Servr 作为客户端，应使用客户端证书KC。同样，为了验证双方证书的有效性，在所有kubelet 和API Server 实例处都有一份Kubernetes CA，也就是Kubernetes 的根证书

任何事物都是有利有弊的，引入SSL/TLS 机制固然能够保证安全，但是从性能上引入了更长的延时。性能影响集中在每一个连接的开始握手阶段。SSL/TLS 的握手需要三次往返，比正常TCP 握手多加了两个往返。对于网络延时比较高的环境，应适当进行调优，例如调整拥塞窗口、尽量使用长连接保持每个连接不断开等。Kubernetes 的各个组件之间的通信都是使用长连接的方式。

### 3.4 面向应用的高可用特性

3.Cluster 层面的高可用保证
在Cluster 级别，可以通过在多个集群上部署应用程序来实现。利用Kubernetes 的Federation 机制，建立集群联邦，通过集群联邦部署跨多云的服务。

### 4.1 镜像仓库综述

镜像仓库根据文件层的校验码来管理每个块文件。当多个镜像基于同一个基础镜像构建时，这些镜像拥有相同的基础块文件，这些镜像在镜像仓库中共享这部分块文件。也因此，在删除镜像时，不能直接删除镜像引用的所有镜像块文件，而是由专门的垃圾回收器来清理没有被引用的块文件，如图4-3 所示。

### 4.3 镜像仓库缓存

为了让缓存服务提供更稳定的性能，缓存服务实例之间可以通过共享块文件的方式将块文件分享到每个缓存实例中，如图4-14 所示。缓存服务实例之间通过P2P 的方式共享块文件，这种方式能使每个新缓存的块文件分享到所有的缓存实例，从而提供稳定的缓存服务性能。

### 4.4 镜像安全

修复方式可以通过在项目目录中添加.dockerignore 条目来跳过.git 目录；也可以通过调整COPY 命令行，精确地指定要添加的文件清单；也可以通过使用多阶段构建的方式，在第一阶段编译出应用，在后面的阶段将编译结果添加到输出阶段的镜像中，修改为多阶段构建的Dockerfile 如下：

3.Pod 删除
Pod 的删除是优雅删除（Graceful Deletion），即在调用删除的命令时，实质上是为Pod添加DeletionTimestamp，后续在多个控制器、代理对Pod 发起一系列的更新动作后才会删除。例如：不同的控制器会通过添加、删除Finalizer 来控制一些资源的释放。

### 5.1 租户

Kubernetes 提供两个层级的授权管理：
1.集群级
在此层级授权的对象，可以操作非命名空间的资源与所有命名空间的资源。例如：Node、PodSecurityPolicy 等资源为非命名空间的资源，只能通过集群级的权限定义进行授权。

Namespaces are a way to divide cluster resources between multiple users (via resource quota).

It is not necessary to use multiple Namespaces just to separate slightly different resources, such as different versions of the same software: use labels to distinguish resources within the same Namespace.

### 5.2 认证

认证是访问所有服务的第一步，用于验证当前的访问者是否为合法用户，包含用户的身份校验与有效校验。

### 5.4 隔离

的网络请求。但在某些场景下，比如多租户的集群中，需要禁止不同租户的Pod彼此访问。对于不同安全性要求的应用，也需要在网络层面进行隔离。Kubernetes 引入了网络策略（NetworkPolicy）对象定义隔离需求，基于此对象，用户可以设定谁能访问哪些Pod。网络策略核心包含如下四个定义：
1.podSelector
podSelector 定义了一组键值对儿，选取标签与之相匹配的Pod。下面的示例代码中选择了标签包含 “role=db” 的所有Pod，如果podSelector 为空，则选择当前Namespace 的所有Pod。

### 6.1 数据中心基础架构

传统业务基本是单体应用，流量以 “南北流量”（纵向）为主，三层架构契合此类流量的特征。随着业务复杂性的提高，单体应用的复杂性也随之升级，系统配置也变得愈加复杂。业界没有停止寻找解决方案的脚步，从早期的模块化编程到SOA，到微服务架构，再到当下的服务网格。一个个巨大的系统被拆解成成百上千的子系统，这些子系统又形成了一个基于网络通信的彼此合作的生态系统。流量从南北向变成了东西向，数据中心网络架构演进为基于路由交换技术的叶脊网络。

### 6.3 Linux 网络基础

我们先看一下Linux 是如何处理数据包的。当网卡收到一个与其MAC 地址相同的Ethernet Frame 时，会直接通过DMA 把数据包放入RAM 预先分配的内存块中，紧接着对CPU 发送中断请求。CPU 在接收中断请求以后，按照网卡接收队列中的地址描述符读取数据，并把数据交给上层协议栈进行处理。
不同的数据队列对应不同的CPU Core，相应的CPU Core 会直接访问数据所在的内存块，对数据进行校验。

Netfiter 是一个基于用户自定义的Hook 实现多种网络操作的Linux 内核框架。Netfilter支持多种网络操作，比如包过滤、网络地址转换、端口转换等，以此实现包转发或禁止包转发至敏感网络。

Netfilter 提供了5 个Hook 点，系统内核协议栈在处理数据包时，每达到一个Hook 点，都会调用内核模块中定义的处理函数。调用哪个处理函数取决于数据包的转发方向，进站流量和出站流量触发的Hook 点是不一样的。

（3）NF_IP_FORWARD：接收的数据包经过路由判断后，如果目标地址在其他机器上，则将触发此回调函数。

6.3.3　ipset
IP 集合是Linux 内核的一个框架，它允许管理员通过ipset 命令配置一系列IP 的集合。同时根据类型的不同，IP 集合可以存储IP 地址、网段、TCP/UDP 端口号、MAC 地址、interface名的不同组合。
ipset 的引入使iptables 规则得到简化。把要处理的IP 地址及端口放进一个集合后，对这个集合设置一条iptables 规则即可取代原来成百上千条规则。使用ipset 来配置规则有如下好处：

● 动态更新ipset 地址即可达到更新iptables 规则的目的，而无需更新iptables 表本身，大大提升了规则更新效率。

● iptables 在进行规则匹配时，从规则列表中从头到尾逐条进行匹配。ipset 将计算复杂度从O(n)变成O(1)。

### 6.4 负载均衡

什么是负载均衡？顾名思义，就是将网络请求分发至多个后端服务器的过程。负载均衡的目标是，资源使用最大化，吞吐量最大化，响应时间最小化，避免单个后端服务器过载。相比单计算节点，基于负载均衡的多组件能够提高软件的可靠性和可用性。负载均衡通常涉及一系列专用的软件或硬件，比如多层交换机、DNS 服务器等。


由于网络的不确定性，不同子服务之间的访问控制和流量转发变得愈发复杂，理解网络传输的原理和方法也变得尤其重要。可以说在分布式系统中，网络是核心，理解了网络就理解了微服务。

浏览器作为客户端，首先会尝试与example.com 的服务器进行TCP 握手，也就是向1.2.3.4 发送sync 包，收集IP 地址作为该数据包的源IP 地址，源端口一般采用随机端口，目标地址是1.2.3.4，目标端口是80。

此时浏览器对example.com 发起一个GET 请求，并进行应用层面的数据包组装。网站的访问请求是一个标准的HTTP request，它由包头和包体组成。此时包体为空，包头如下面代码所示，GET 表示HTTP 请求的方法，“/”表示访问路径，HTTP/1.1 表示协议和版本。

应用组装好数据包以后，尝试发起HTTP 请求，将此请求包发给服务器。此时操作系统会介入，在HTTP 包外封装TCP 包头，包头里面包括源端口，一般是随机端口和目标端口，针对HTTP 请求默认端口是80。在TCP 包外面再封装IP 包头，包头主要包括源IP地址和目标IP 地址，在IP 包的外面会再增加ethernet 包头。作为一个完整的ethernet 数据包转发出去，数据包的组装细节如图6-9 所示，包头分别对应我们常说的IP 头、TCP头、HTTP 头。


微服务架构下，系统往往是异构的，微服务之间的协议也不尽相同，随着应用层协议越来越强大，从HTTP1 到HTTP2，到GRPC，数据传输效率获得了极大提升，这同时给负载均衡技术带来了额外的挑战。

因此，针对HTTP2、GRPC 这类应用，必定需要支持HTTP2、GRPC 协议的负载均衡，而这类协议是应用层协议，必然需要应用软件来承担这一职责。
基于应用层的负载均衡器，支持一个应用层进程处理所有进出的数据包，并按照应用层逻辑进行转发。应用层进程意味着灵活性非常强，我们可以基于这类应用层软件实现几乎一切流量控制方面的需求，如熔断、限流、错误注入、日志、跟踪等。

6.4.3.2　进程内负载均衡
进程内负载均衡方案将负载均衡功能以内嵌的类库的形式集成到服务消费方进程中，该方案也被称为客户端负载方案。服务注册中心（Service Registry）配合支持服务自注册和自发现：服务提供方启动新实例时，首先将服务实例地址注册到服务注册中心。当服务消费方要访问某个服务时，客户端通过内置的负载均衡组件向服务注册中心查询目标服务地址，并从注册中心返回的地址列表中，以既定负载均衡策略选择一个目标服务地址，并向目标服务地址发起请求。

进程内负载均衡是一种分布式负载均衡模式，负载均衡和服务发现能力被分散到每一个服务消费者的进程内部，同时服务消费方和服务提供方之间采用直接调用方式，没有额外网络跳转造成的延时，性能比较好。另外，该模式避免了在集中式负载均衡模式下，流量集中导致的负载均衡器网络带宽瓶颈和单点故障问题。

该方案的缺点是，负载均衡组件以类库的形式内嵌在应用中，若企业内有基于不同编程语言的异构系统，则需要为每种语言和平台开发不同的负载均衡软件。当客户端跟随服务调用方发布到生产环境中时，如果要对客户库进行升级，则服务调用方必须修改代码并重新发布。

3.最小连接（LeastConnection 或LeastRequest）
负载均衡器永远选择并发连接最少的后端服务器。

4.一致性哈希（ConsistentHash）
根据数据包的某些特征，通常针对N 元组信息（比如源IP 地址、目标IP 地址、源端口、目标端口、协议等）进行哈希计算，不同的哈希值对应不同的后端服务实例。该策略的优势是，同一个连接的数据包N 元组的哈希值不变，它们总会被转发至相同的后端。因此，该策略可以应用在需要会话保持的场景。

5.权重（Weighted）
在某些场景下，我们希望对流量进行微调，需要在一个负载均衡器有多个后端服务器时，按照固定比例调整流量转发比例。该策略通常应用在多版本发布时，可以通过权重的负载均衡进行流量微调，实现多版本的灰度测试。

区别是，Pod 的liveness probe 是由kubelet 触发的，而负载均衡器的健康检查是从负载均衡器发起的，其测试结果不仅体现了后端服务实例的健康状况，也体现了整个网络链路的连通性。

### 6.5 Kubernetes 中的服务发布

Kubernetes 提供了基于Service 和Ingress 的负载均衡技术，使得Kubernetes 用户可以通过创建相应的Spec 来完成服务发布。

6.5.1　创建服务
服务定义有两个重要属性：Service Selector 和Ports。为降低Kubernetes 对象之间的耦合度，Kubernetes 允许将Pod 对象通过标签（Label）进行标记，并通过Service Selector定义基于Pod 标签的过滤规则，以便选择服务的上游应用实例

Service 的selector 不为空时，Kubernetes Endpoint Controller 会侦听服务创建事件，创建与Service 同名的Endpoint 对象。selector 能够选取的所有PodIP 都会被配置到addresses属性中。

如果服务没有定义selector 属性，则该服务会被Endpoint Controller 忽略。此时用户有机会手工创建不被Kubernetes 管控的Endpoint 对象，并维护Endpoint 中的地址。通过此方法可以将不在Kubernetes 中管理的应用实例发布成Kubernetes Service，比如运行在集群外部虚拟机中的Web 服务。

Service 有一个属性publishNotReadyAddresses，在该属性被设置为true 以后，只要Pod不是处于running 状态，该Pod 对应的IP 地址在Endpoint 中就会被标记为就绪，也就是无论Pod 的状态是否为ready，都会参与流量转发。

### 6.6 DNS

CoreDNS 包含一个内存态DNS，以及与其他controller 类似的控制器。如图6-18 所示，CoreDNS 的实现原理是，控制器监听Service 和Endpoint 的变化并配置DNS，客户端Pod在进行域名解析时，从 CoreDNS 中查询服务对应的地址记录。CoreDNS 为不同类型的Service 创建的DNS 记录也不同。

2.Headless Service
顾名思义，无头，是用户在Spec 显式指定ClusterIP 为None 的Service，对于这类Service，API Server 不会为其分配ClusterIP。CoreDNS 为此类Service 创建多条A 记录，并且目标为每个就绪的PodIP。
另外， 每个 Pod 会拥有一个 FQDN 格式为$podname.$svcname.$namespace.svc.$clusterdomain 的A 记录指向PodIP。

CoreDNS 完成域名配置后，要查询其中配置好的域名信息，则需要完成调用方的DNS配置。Kubernetes Pod 有一个与DNS 策略相关的属性DNSPolicy，默认值是ClusterFirst，在此模式下，Pod 启动后的/etc/resolv.conf 会被改写，所有的地址解析优先发送至CoreDNS。改写后的resolv.conf 的内容如下：

### 第7章 API 网关和服务网格

Service 支持TCP 和UDP 协议。Service 支持常见的TCP 或HTTP1.X 应用毫无问题。但是对一些支持连接复用的应用协议，比如HTTP2 或GRPC，就显得力不从心了。这类应用协议的特性是，当上游服务器接收并响应一个客户端请求时，连接不会立即断开，客户端和服务器端之间会保持一个长连接，只要是同一个客户端发送来的请求，都会被转发至同一个上游服务器中。Service 在TCP 协议层对应用层协议无法感知，如果用标准的TCP Service 来发布GRPC 协议应用，就会导致同一个客户端的所有请求都转向同一个上游服务器，从而失去负载均衡的调整能力。

Service 的四层负载均衡由网络协议栈处理，不具备轻量级剔除上游服务器的功能，只能配合其他健康检查组件，如果发现某个上游实例不能正常工作，则需删除相应的转发规则，这类操作开销较大。

### 7.1 API 网关

API网关本质上是一组专门用于网络请求转发的应用服务实例，作为集群的入站流量入口，API 网关将各系统对外暴露的服务聚合在一起，所有要调用这些服务的系统都需要通过API 网关进行访问，基于这种方式，网关可以对API 进行统一管控，例如认证、鉴权、流量控制、协议转换、监控等。

API 网关提供的TLS Termination 功能正是为这种场景而生的。应用开发者在部署应用时只需开放HTTP 端口。该服务注册到HTTPS 网关上时，对外提供的就是HTTPS 服务。当用户通过API 网关访问该服务时，HTTPS 请求首先被API网关处理，加密请求被解密，再由网关发起新的HTTP 连接到上游服务器。通过此种配置，业务开发人员只需关注业务逻辑，证书由网关及负责的组织统一管理。

限流规则可以使微服务设置自己能接收的最大请求数，超出的请求会被直接返回，保护自己不被过量请求压垮。熔断规则使得微服务可以设置自己访问上游服务的最大并发数，防止把上游压垮，缓存机制可以进一步减少对上游请求的压力。

4.可观测性API 网关作为流量入口，可以提供统一的日志、Metrics 和Tracing 解决方案。

业界基于Kubernetes 的成熟控制平面组件方案包括轻量级的API 网关方案Kubernetes Ingress、VMWare Contour 及Istio。这些组件可以自动化生成反向代理软件的配置文件，完成数据转发。反向代理软件包括Nginx、HAProxy、Envoy 等，它们用于转发网络请求，因此被称为转发平面（Forwarding Plane）组件或者数据平面组件。

### 7.2 服务网格

看出任何服务到服务之间的调用都要从调用方通过负载均衡器到API 网关再到目标服务。此种模式存在如下缺点：
● 额外的网络跳转，通常应用的部署模式是，一个业务流的所有相关的微服务会部署在同一集群中，这样微服务之间的调用在同一集群或数据中心内部的网络中就完成了，业务会有更低的网络延迟。然而API 网关的存在，使得所有的微服务调用从集群内部先转发到负载均衡器，再到API 网关，最后转发给目标服务。这些额外的跳转增加了网络延迟和超时出错的概率。

● 错误域的控制。API 网关是集中部署的，是整个集群的通用服务。如果API 网关出现故障，则所有微服务都不可用，这几乎是致命的。因此，在生产系统实践过程中，我们需要考虑对错误域的控制。有时需要根据不同的业务域切分API 网关，以减小影响。

那么如何优化部署以减少对API 网关的依赖呢？答案是，使用服务网格。服务网格是一个可配置的、低延迟的网络架构，与API 网关的功能有很多共通性，两者都需要处理服务发现、请求路由、认证、鉴权、限流、监控，等等。两者也有区别。服务网格的目标是管理微服务之间的通信，而API 网关的目标是管理外部客户到服务之间的通信。

而服务网格的目的就是将这类南北流量转向东西（横向）。为减少网络延迟及对集中式负载均衡器的依赖，我们需要做的就是将集中式负载均衡分散开来，变成分布式负载均衡，这其实就是服务网格的本质。我们可以把kube-proxy 理解成为一种四层的服务网格实现，它实现了分布式负载均衡的功能。

一个更完整的服务网格架构如图7-5 所示，可见所谓服务网格，就是把API 网关的功能下发，作为Sidecar，存在于用户进程旁边。Sidecar 与API 的功能一致，但分散在集群中的所有计算节点，这样服务到服务之间的东西流量不再需要经过API 网关，只需将请求转交给Sidecar，由Sidecar 做请求路由和负载均衡，然后直接发起向上游服务器的连接。从数据传输层面看，集中式负载均衡功能被分散到了所有Sidecar，而每个Sidecar 承担的职责与API 网关一致。通过这样的方式，服务网格将东西流量和南北流量统一管理起来。

### 7.3 深入了解Envoy

Envoy 公司倡导在微服务架构下，应用的业务逻辑和网络传输应解耦，网络对应用应透明，网络控制逻辑应尽可能由网络传输组件即Envoy 处理。应用甚至无需关心超时、重试等逻辑，这些控制逻辑均由Envoy 完成。重要的网络参数可在应用运行时、不影响业务的前提下灵活变更，将网络和应用逻辑解耦，使得微服务之间出现网络调用问题时，排查代价显著降低。

事实上，业界已经存在的成熟微服务架构（如Spring Cloud），在数据转发层面的设计已经有些服务网格的影子。比如服务提供方可基于Eureka 注册服务实例，服务消费方依赖Ribbon 做客户端负载均衡。当消费方发起网络调用时，其通过本地的负载均衡组件查找服务提供方的健康实例，并直接发起网络调用。

Envoy 支持gRPC 的负载均衡，由于gRPC 是应用协议，所以传统的网络层负载均衡无法满足真正的平衡负载的目的。Envoy 作为应用层软件，可以实现真正的gRPC 负载。

其与传统反向代理软件需要读取配置文件并重启进程、更新配置不同，Envoy 允许配置管理服务器，只要管理服务器中的配置发生变更，这些变更都会被Envoy的xDS 发现并自动加载。这种低开销的动态配置加载方式，使得Envoy 配置无需保存在本地，其本身变成了无状态应用，且动态更新成本非常小，完美地契合了云原生的应用场景。

当Envoy运行在API 网关模式下时，用户应用与网关通常不在同一台主机上，健康检查测试的是整个网络链路的连通性。被动健康检查是对主动健康检查的补充，当某个上游实例持续返回错误时，会被Envoy 自动从转发列表中剔除。

8.可观察性强
可观察性是衡量一个代理软件的重要指标，当数据平面出现故障时，需要迅速定位并解决问题。Envoy 支持指标上报、日志、链路追踪等功能，可以适应不同场景的问题定位分析及监控。

基于服务发现机制，Envoy 进程无需重启即可加载新的配置，这满足了云原生平台中一切都是快速变化的假设。在Envoy 出现之前，任何数据平面组件都是以读取静态配置文件的形式完成配置加载的。当配置文件发生改变时，数据平面组件需要重启以完成配置变更。

Envoy 另辟蹊径，开创了动态加载配置的先河，允许在配置文件中指定发现服务器（xDS Server）。Envoy 会向该配置地址发起请求并获取配置清单，无需进程重启即可完成配置加载

● Secret Discovery Service (SDS)：Secret 发现。用于动态加载证书，对应Kubernetes的Secret，这些Secret 通常用来保存网关证书。


### 7.4 Ingress

Kubernetes 提供了Ingress 用于定义高级路由的功能，借助Ingress Spec，可以定义一个https 网关。事实上，Kubernetes Ingress 只定义了一套模型规范，不同代理软件都开发了自己的Ingress Controller，Ingress Controller 的主要功能就是将Ingress Spec 转换成代理软件的配置文件，并自动加载。Ingress 配置文件示例代码如下：

● 定义 host，也就是访问该应用的域名，将该域名配置进反向代理软件中的hostFilter，如果请求Header 的域名跟该host 不匹配，则请求会被拒绝。

Ingress Controller 控制器和Nginx 组件，Ingress Controller 负责读取Ingress、Service 和Endpoint 配置，解析和生成Nginx 配置文件，并调用nginx reload 命令重启Nginx并加载新配置。

● 缺少证书自动化管理。Ingress 要求用户从第三方获取证书，并手工将证书放入secret 中，然后由ingress controller 上传并安装。虽然业界有ingress 与letsencrypt的整合方案，但这意味着需要维护更多组件，

比如，Ingress 目前支持Context Path 的转发规则，但不支持HTTP 头的匹配，不支持rewrite 规则，不支持多协议和多端口，不支持限流等。

### 7.6 Istio

从管理范畴来看，Istio 管理API 网关，它不仅管理入站流量，还管理基于服务网格的微服务之间的流量，以及出站流量。从功能层面来看，Istio 管理数据转发，它包含一套完备的日志收集、指标上报、策略管理功能，能够通过统一的方式管理、监控和连接微服务应用。

Istio 提供的功能特性包括：
● HTTP、gRPC、WebSocket 和TCP 流量的自动负载均衡。
● 通过丰富的路由规则、重试、故障转移和故障注入，对流量行为进行细粒度控制。
● 可插入的策略层和配置API，支持访问控制、速率限制和配额。
● 对出入集群入口和出口的所有流量自动度量指标、日志记录和跟踪。
● 通过强大的基于身份的验证和授权，在集群中实现安全的服务间通信。

Envoy 作为数据平面组件，以两种形式运行，当它以Ingress Gateway Pod 的形式运行时，实现的是API 网关的功能，主要实现南北流量的转发和管理功能；当它以Sidecar 的形式运行在用户Pod 中时，实现的是东西向流量的转发和管理功能。

Istio 架构看似是完美的分布式架构，但在实际运营中，过于分散的部署会导致控制平面脆弱、可用性差，且定位问题困难。自1.5 版本开始，Istio 引入了Istiod，其主旨是将核心模块集中部署，以简化运维，提高控制平面的性能和稳定性。

● 流量管理
Istiod 进行流量管理时消费两类对象，一类是配置，即networking.istio.io group 的所有CRD，对应Envoy 中的Listener 和Route；另一类是状态，具体是指Kubernetes 的Service和Endpoint 信息，对应Envoy 中的Cluster 和Hosts。Istiod 将这两类对象合并成Envoy 的配置信息，并通过XDS API 推送给Envoy。

由于Istiod 是向Envoy 推送配置信息的唯一组件，所以Envoy 支持的所有功能（包括认证、访问控制、全局限流）都需要Istiod 参与，最终生成的配置文件也需要通过Istiod来推送。

● 遥测收集
Istio 强大的跟踪、监控和日志记录可以使我们深入了解服务网格部署。通过Istio 的监控功能，可以真正了解服务性能如何影响上游和下游的功能，而其自定义仪表板可以提供对所有服务性能的可视性，使我们了解该性能如何影响其他进程。

Istio 提供了一个针对部署和服务的清晰视图，能够让用户清晰地感知一个服务对上下游的影响。通过自定义的监控看板，可以清楚地了解每一个服务的性能，以及性能对业务流的影响。
所有这些功能可以更有效地设置、监控和实施服务上的SLO，快速有效地检测和修复问题。

但是我们还需要协议转换、认证授权框架、灵活的负载均衡策略等，kube-proxy 这个基于传输层的组件都是无法完成的。因此，我们还需要基于应用层的分布式负载均衡，这就是Sidecar 提供的基本功能。所谓Sidecar，顾名思义，就是在应用进程旁边运行一个专门用来做流量管理的Envoy 进程，其生命周期依赖于用户主进程

从字面意思理解，就是当Namespace 打了istio-injection: enabled 标签后，任何Pod 创建都会激活该Webhook，而该Webhook 会调用istio-system 下的 istio-sidecar-injector 服务，该服务对应的Pod 就是上面展示的sidecar-injector Pod，它会读取对用户输入的Pod 信息做变形处理，插入Sidecar。

查看注入后的结果，可以看到用户Pod 被注入了两个容器：
（1）注入了init-container istio-init，运行命令如下：
￼
（2）注入了sidecar container istio-proxy，其镜像就是Envoy。

init container 是一种特殊容器，主要负责用户在主进程启动之前的初始化工作，它会在主容器启动之前先启动，从init container 的命令行看，它其实改写了iptables 规则，然后退出

（3）ISTIO_INBOUND 只有一条规则，如果请求访问的是容器服务暴露的9080 端口，则转交给ISTIO_IN_REDIRECT 处理。
（4）ISTIO_IN_REDIRECT 会将所有TCP 协议的数据转发至15006 端口，这个端口正是Envoy Sidecar 监听的端口，至此，流量被Envoy 劫持完成。

5）Sidecar Envoy 启动后，其管理服务器被配置为Istiod 服务，Istiod 读取Pod 信息，并生成Envoy 的配置，通过XDS API 推送给Sidecar。Istio 使用istio-proxy 用户身份运行Sidecar，其UID 为1337，即Envoy 所处的用户空间。
（6）当请求被Envoy 劫持以后，会按照Envoy 的配置决定转发目标，Envoy 最终决定将请求转发给本机的9080 端口。

所以入站和出站的流量都被劫持到Envoy，服务到服务的通信被转换为Sidecar Envoy 和Sidecar Envoy 之间的通信。客户端的Envoy 用于实现路由判决、负载均衡等功能，服务器端的Envoy 用于配合客户端完成协议升级、日志收集、指标上报，以及将入站流量转至应用端口等。

Istio 在安装完后，将一组Ingress Gateway Pods 作为集群的默认API 网关，当集群用户将服务发布至API 网关时，只需要三个对象。

首先是Gateway 对象。Gateway 定义了将微服务发布到哪个API 网关，通过何种协议、什么端口发布出去，如果是TLS 协议，则证书地址在哪等信息。

通过VirtualService 对象配置转发规则，转发规则中可以配置端口匹配规则、路径匹配规则、请求包头匹配规则等。

至此，用户即可将自己的微服务注册至API 网关。可见Istio 对流量的管理模型做了更进一步的抽象，它将API 网关、转发规则和目标规则等分离出来，各司其职，更好地遵循了Kubernetes 设计的对象功能简单且互补的原则。这样做的优势是，对于每一个对象，Istio 都可以通过丰富其定义来完成复杂的流量管理功能。

Gateway、VirtualService 和DestinationRule 是Envoy 的高级封装。Istio 模型的一个主要目标是将Envoy 的配置功能抽象出来，封装成对用户更友好、更容易管理的表现形式。Istio 模型是对Envoy 配置信息的抽象，本节不会把所有功能都列一遍，只分析一些流量管理常用的功能。

VirtualService 对象提供了基本的 L7 转发规则，基于请求路径（URI）和请求包头（header）的转发规则配置的示例代码如下：

为实现流量灰度，首先需要定义DestinationRule，DestinationRule 中的hosts 与服务名一致。然后将不同版本的服务切分成不同子集subset，不同的subset 可以通过Pod 的label 来区分。下面的示例代码展示了DestinationRule 是如何基于不同Pod 标签定义多个subsets 的，subset v1 会选择所有（1000 个）v1 版本Pod，subset v2 会选择新的v2 版本Pod。


Istio 作为平台解决方案，提供了统一的认证授权机制，使得应用无须重复造轮子。平台级的统一认证授权使得不同微服务之间的权限统一管控变得可能。

7.6.5.1　认证
1.应用实例间的认证
我们知道，Pod 在启动时，用户可以指定ServiceAccount 作为其身份，默认Pod 的SA是default 状态。Istiod 监听集群所有ServiceAccount，并为ServiceAccount 生成CA、server key 和Certs，以及Client Key 和Certs。


认证的主要目的是验证身份，常见的TLS 就是一种验证方式，通常服务提供方用TLS来证明自己是可信的。服务提供方也可以对客户端做证书验证，证明客户端是可信的，这就是我们常说的双向TLS（Mutual TLS）。

PeerAuthentication 对象创建后，相同Namespace中所有Pod 的Sidecar 中的Envoy 监听器都会加载server key 和certs，并将使用协议升级为HTTPS 协议。

用户还需要创建DestinationRule，以保证从网关到上游服务器发起的连接也是TLS 的，并开启客户端TLS 认证。开启相应的的设置以后，istio-system namespace 下面的任何Pod，在与上游服务器通信时都会尝试使用TLS 协议，并向服务器端认证自己的身份，代

2.终端用户认证
Json Web Token（JWT）是一种在网络应用中传递用户签名信息的互联网标准，通常用于用户认证场景。JWT 由认证中心签发，认证中心通常会维护一个Json Web Key Set（JWKS）文件，该文件中包含用于签发JWT 的私钥及认证JWT 的公钥。


1.Logs
Envoy 的访问日志记录了所有请求的来源、目标、返回值、处理时间、传输时间等，这对定位问题非常重要。访问日志的输出配置在Envoy 的配置文件中，日志首先被输出到Docker 标准输出中，由Docker 的Log Driver 收集（或者直接保存至文件系统）。然后，由统一的日志收集组件（比如filebeat）将日志统一收集到日志管理平台（比如常用的ELK套件）。

2.Metrics
默认配置下，Envoy 会将请求的关键指标（请求数量、处理时长、请求包大小等）信息保存至Promethus。Istio 对不同的协议默认收集不同的指标，如需收集自定义指标，则需开发WASM（Web Assembly）插件。

Envoy会对所有经过的请求打Tracing 锚点，如果希望能看到完整的调用链，那么应用在进行服务调用时要将下列header 传递给下一个应用：
x-request-id
x-b3-traceid

随着Sidecar 的引入，服务和服务之间的通信无须经过集中式负载均衡，而是从客户端本地的Sidecar 做服务寻址和路由，直接到服务器端的Sidecar 减少了一次网络跳转。

Istio 的优势很明显，它能够将南北流量和东西流量作为一个统一命题管理起来，基于同一套技术栈，将微服务架构从API 网关演进到服务网格。Istio 已成为社区最活跃的开源服务网格项目，它具有如下诸多特性。
● 可移植性强，不仅支持Kubernetes，也支持虚拟平台Openstack 及Consul。
● 跨语种的服务网格平台，统一Java、Scala、Node.Js 等诸多语言。


网格和API 网关的用户体验，降低运营成本。
● 功能丰富，可以选择性地安装和维护需要的组件。

● 背后有强大的社区支持，谷歌将Istio 作为下一代微服务治理平台，IBM、微软、华为、阿里等云计算巨头都积极参与Istio 项目的推进和生产化。

● 存量业务的迁移。很多企业已经开发了基于SpringCloud 等开源框架的微服务系统，此系统已经支持了诸多熔断限流、API 网关等功能，与Istio 提供的功能重复。是否要将这些存量业务迁移到Istio，以及如何迁移都是巨大的挑战。

### 第8章 集群联邦

为提高系统效率，Kubernetes 的API Server 作为API 网关，会对该集群的所有对象做缓存。集群越大，缓存需要的内存空间就越大。其他Kubernetes 控制器也需要对侦听的对象构建客户端缓存，这些都需要占用系统内存。这些内存需求都要求对系统的规模有所限制。

多集群管理成为云平台生产化过程中需要解决的共有问题。为此，Kubernetes 引入集群联邦（Federation）管理多集群。它在Kubernetes 集群基础之上提供了一份集中控制平面，该控制平面与普通集群类似，也有etcd、API Server 等。

之所以称之为集群联邦，是因为上层控制平面负责统一请求处理，多集群协调，每个成员集群自治，这正如联邦制国家的运作机制。

例如，联邦可以保证一个应用的Deployment 被部署到多个集群中，同时能够满足全局的调度策略。


2.跨集群服务发现
联邦汇总各个集群的服务和Ingress，并暴露到全局DNS 服务中。例如，每个集群都有一个负载均衡类型的服务，联邦汇总每个集群中的负载均衡器的虚拟IP 地址，并暴露到同一个DNS 上，通过全局的DNS 访问入口，访问所有集群中的具体应用实例。

5.降低延迟
在多个地区部署集群，通过地域感知的DNS 策略将用户请求转发到距离用户最近的实例以减少访问延迟。

### 8.1 集群联邦概览

V2 版本中，集群联邦不再使用原生的Kubernetes 对象，而是使用联邦专属的一组联邦资源，以及一整套联邦层面的控制对象来完成联邦层面的控制逻辑。集群联邦V2 提供了统一的工具集（Kubefedctl），允许用户对单个对象动态地创建联邦对象。

8.1.2　集群注册中心
集群注册中心（ClusterRegistry）提供了所有联邦下辖的集群清单，以及每个集群的认证信息、状态信息等。集群联邦本身不提供算力，它只承担多集群的协调工作，所有被管理的集群都应注册到集群联邦中。

Placement 用来配置联邦对象的目标集群，其值可以是具体的集群名单，也可以是clusterSeletor 选择对应标签（label）的集群。当两者同时存在时，明确定义的集群名单具有较高优先级。联邦根据优先级来定义要同步对象的目标集群，如果提供了集群名单（哪怕是一个空List），则无论clusterSelector 提供什么内容，都会被忽略。

同步控制器根据 Placement 来确定 Template 中定义的对象要同步到哪些集群。当Placement 发生改变时，同步控制器会解析Placement 并将Template 中的对象同步到新的集群中，或从已经部署的集群中删除。

Overrides 用于针对每个集群进行本地化定制。在联邦资源中，Template 部分定义了要部署到每个集群中的Kubernetes 对象，Placement 定义了要在哪些集群中部署这些对象。在同步控制器同步该对象时，以Template 作为模板在目标集群创建Kubernetes 对象。而在实际部署应用时，通常会通过调整不同集群中的配置模板，部署符合特定集群需求的应用，以更好地发挥网络、计算资源、存储等的优势。为了满足不同集群的个性化定制需求，可以通过添加Overrides 覆盖不同目标集群的配置模板。

联邦支持两种Kubernetes 资源的全局DNS 服务：Service 和Ingress。在Kubernetes 集群中，可以通过nodeport 或LoadBalancer 暴露服务。集群联邦通过DNSEndpoint 管理最终要暴露的DNS，并通过DNS 同步控制器将所有的DNS 注册到外部DNS 服务以提供访问。全局DNS 服务涉及的集群、集群联邦及其关联关系如图8-7 所示。

下面通过一个ServiceDNSRecord 的示例来描述全局DNS 服务是如何将具体Kubernetes 集群中的服务暴露到外部DNS 服务中的。

### 8.2 集群联邦对象抽象

集群联邦管理多个Kubernetes 集群，将每个集群抽象为KubeFedCluster。集群联邦不直接管理具体应用的单个应用实例（Pod），也不对单个集群的Deployment 进行管理，而是在更高的抽象层对多个集群的资源统一进行管理。为管理这些更高抽象层的应用，联邦抽象出联邦资源（如FederatedDeployment、FederatedService 等）。

部署在各个集群中的服务要被访问，需要有访问入口，在集群控制层面有Service，在联邦控制层面，为了能够在全局提供统一的访问入口及服务发现，集群联邦抽象出DNSEndpoint，通过Service 控制器和Ingress 控制器将每个集群的服务收集到联邦层面，并通过外部DNS 服务暴露出来。每个集群都是动态的，同样每个集群都会出现故障，为了能够在某些集群出现故障时依然保障应用在全局层面的稳定性和可靠性， 联邦需要在多个集群之间进行调度。 为此， 联邦抽象出ReplicasSchedulingPreference。


### 8.3 联邦应用

在集群联邦部署应用时，则需要从全局角度考虑应用需要多少实例、跨多少个Kubernetes 集群、每个集群分配多少实例、自动分配策略、如何暴露服务、访问入口等。

我们可以通过编写联邦对象来部署应用，也可以先编写 Kubernetes 对象，再使用kubefedctl 命令行工具将编写的Kubernetes 对象转为联邦对象，然后对转换后的对象进行微调

在Kubernetes 集群中部署的应用，因为不用再考虑底层的硬件、网络拓扑、虚拟机（或物理机）等细节，所以在运维时关注的重点是应用本身，以及在集群中部署的拓扑结构。运维时考虑的不再是到什么样的节点进行什么操作，而是对Kubernetes 对象进行调整，从而满足应用对性能、稳定性、存储、调度等的需求。

### 第9章 边缘计算

而集中式数据中心的一个显著的弊端是，所有服务集中部署在某一地域，这样的部署使得远程用户的请求需要经过长距离数据传输才能抵达数据中心。长距离的数据传输效率及互联网链路的不确定性，使得应用的速度和可用性无法得到保证。

它与中心化的云计算模式相比具有显著优势：● 计算尽可能靠近设备、靠近边缘，能有效降低网络延迟、提升应用性能。● 数据在边缘设备处理使得需要传输到云端的数据大大减少，有效地减少了网络传输的成本。

### 9.1 边缘数据中心

流量接入的实现依赖智能域名服务器、四层负载均衡及七层API 网关的协同工作，而这些组件均可通过Kubernetes 来实现。

9.1.1　智能域名服务（GSLB）

比如常见的 “两地三中心”，就是主备模式和多活模式的组合，该模式又分为同城主备和同城双活。同城单活模式下，如果主数据中心发生故障或灾难，业务应用可以在短时间内切换至其他数据中心。同城双活数据中心就是同一个城市部署两个数据中心同时提供服务，两个数据中心地位均等，一般是将统一业务拆分部署至两个数据中心，这样当一个数据中心出现故障，另一个数据中心依然可以提供服务。即使两个数据中心同时出现故障，还有异地数据中心可以从备份数据中恢复业务。为

针对多地部署的应用，若能基于访问应用的客户端IP 地址所属地域信息，返回物理位置接近的服务地址，则能有效地降低客户访问服务的网络延迟。智能域名服务承担的就是此职责。所谓GSLB，就是基于服务地域属性划分流量的智能域名服务器。主

假设其中有一个地址不可达或对应的服务不可用，普通DNS 是无法感知的，它依然按照之前配置的列表继续轮询并返回结果。而支持健康检查机制的智能域名服务，可以周期性检测目标地址的健康状况，若健康检查失败，则将不健康的目标从转发列表中剔除

GLSB 的存在使得按照地域进行负载均衡成为可能，GSLB 总是按照请求方的IP 地址信息返回相同地域的VIP，使得网络性能最佳。

目前大部分公有云都支持与GSLB 功能类似的全局流量管理，比如AWS 的Route 53，就是一个Smart DNS 实现，其因DNS 端口是53 而得名。还有微软Azure 的Traffic 也支持类似的功能。类似的全局流量管理在提供高可用生产应用和优化数据平面的场景中非常重要。

● 就近访问当前很多DNS 组件（如Kubernetes 采用的CoreDNS）支持通过自定义插件实现域名查询功能的扩展。为实现就近访问的功能，只需编写插件，并在处理域名查询请求时，根据客户端IP 地址返回就近的服务器地址。

跨地域的网络访问是非常慢的，主要有如下两下原因：1.TCP 开销TCP 是一种面向连接的、可靠的、基于字节流的传输层通信协议。该协议基于通信两端的协商，TCP 在建立连接时，需要三次握手。现代应用基本都基于TLS 协议发布，在基于TLS 协议建立连接时，在TCP 三次握手之后，还需要经过交换加密算法等协商阶段。对于TLS 协议，从客户端发起TCP 握手请求到SSL 连接建立，在这期间需要三次来回（RoundTrip）。

光速，光绕地球一圈约133ms。跨地域的网络传输一次需要100ms 是常见的情况，单向传输需要100ms 意味着一个Round Trip 需要200ms，SSL 连接建立需要600ms，HTTP 响应需要800ms，如图9-3 所示。

2.互联网路由的不确定性互联网是由无数路由器和交换机连接起来的一张大网，从一个点到另一个点有无数的路径。通常，路由器在转发请求时都会根据请求的目标地址查询本地路由表，选择最短路径进行转发。这些网络路径跟用户所处的地理位置及运营商都有关。通过traceroute 命令，可以查看访问某地址的完整网络路径。

假设边缘网关与访问的客户在同一城市，二者之间的延迟是10ms，边缘网关到数据中心的延迟是100ms，那么客户端发起请求到SSL 连接建立的整个时长会被压缩到60ms。

其次边缘网关到数据中心的线路可以是专线，其网络路径不再是普通的互联网路径，不用跟其他公网流量挤在同一通道。可以通过调节滑动窗口大小、MTU 大小等减少Route Trip 的数量，通过调节拥塞窗口等来增加并发传输数据量，从而提高整体数据传输的性能。

API 网关需要实现SSL 卸载的功能，SSL 卸载是一个需要大量CPU 进行加密解密的过程，相比专用硬件负载均衡器而言，基于普通服务器的SSL 卸载能力较差。如果并发请求较高导致CPU 太忙而影响转发效率，则可以购买专门的SSL 加速卡，或者对API 网关的Pod 进行横向扩展，以增加处理能力。

边缘网关不仅可以提升响应速度，借助应用层API 网关的访问控制能力，还能有效过滤用户请求，将非法请求阻止在边缘，减轻云计算数据中心的网关和骨干网的压力。

借助边缘计算能将数据分析功能下沉到边缘数据中心，帮助用户降低数据的存储和传输成本。大量数据能够得到预处理，仅将高度相关的数据上传到云端或企业内部自有的IT 基础设施即可。

4.视频监控与自动化控制
以交通违章监控为例，路口的摄像头可以时刻采集视频，但在无违章发生时，这些视频是无价值的，如果把这些视频信息无差别地全部传输到数据中心保存，那么对带宽和存储的占用是不可想象的。因此，在视频信息采集完成后，需要在边缘进行预处理，将涉嫌违规的视频片段上传保存。类似的应用场景还有自动驾驶、无人机、视频监控、智慧城市、智能家居等，物联网和边缘计算的应用场景有无限的想象空间。

### 9.2 KubeEdge

KubeEdge 是随着Kubernetes 的演进而产生的，其本质是一个基于CRD 对象的用于边缘节点管理、边缘应用编排、边缘设备监控和管理的定制版Kubernetes。KubeEdge 中的应用管理与在云中的应用管理体验一致，图9-8 展示了KubeEdge 架构。

WebSocket 是一种可以在单个TCP 连接上进行全双工通信的协议。WebSocket 使得客户端和服务器之间的数据交换变得更加简单，如图9-9 所示，客户端和服务器只需要完成一次握手，两者之间就可以建立持久性的连接，并进行双向数据传输。

Quic（Quick UDP Internet Connection）是由谷歌提出的使用UDP 进行多路并发传输的协议。
由于建立在UDP 的基础上，QUIC 支持在握手的同时传输数据，也就是说，握手的额外开销可以忽略，所以在大部分情况下，只需要0 个RTT 就能实现数据发送。

1.Modbus 协议
Modbus 是一种主从通信协议，支持多种电气接口，如RS-232、RS-485 甚至是以太网。特别是在RS-485 上的广泛应用，使Modbus 已经成为事实上的RS-485 通信标准。目前该协议在PLC、DCS 等各种智能仪表上得到了广泛采用。

KubeEdge 在每个边缘节点都维护一个轻量级数据库（SQLLite），用于保存CloudCore下发的资源信息，MetaManger 包含该数据库的DAO 层和所有消息处理逻辑。

当边缘侧的其他组件（比如 EdgeD）需要获取资源信息时，会发送 Query 消息到MetaManger，MetaManger 从数据库中查询所需信息，并将结果返回EdgeD。

### 第10章 应用落地

微服务架构中，有一个著名的指导理论——十二要素理论，该理论为微服务架构下的应用管理提供了十二条最佳实践指导原则。

应用落地主要有以下两方面内容：1.应用容器化

2.Kubernetes 集群中的应用管理应用的每个实例是否有独特配置或数据，决定了应用是有状态还是无状态。

遵循代码和配置分离的原则，配置文件不应该编译进容器镜像，如有密码证书等敏感信息，则应该定义在Secret 中，否则定义在ConfigMap 中。高可用保证的内容包括应用实例数、自动扩容策略、跨集群部署等。

根据上面两方面内容，可以得到最终应用需要的Kubernetes 资源清单，然后准备各个清单中列明的Kubernetes 对象，最终创建到Kubernetes 集群中，完成应用的部署。

### 10.1 应用容器化

在构建镜像的过程中，经常变化的内容和基本不会变化的内容要分离开来，我们称之为动静分离。把基本不会变化的内容放在下层，创建出不同的基础镜像供上层使用。比如，可以创建各种语言（例如Golang、Java 和Python 等）的基础镜像，这些基础镜像包含最基本的语言库。我们可以在此基础上继续构建应用级别的镜像。镜像的动静分离也会缩短连续多次构建的时间。不变化的内容所在的层在一次编译后就存在了。在后续构建过程中，可以直接使用已经存在的层，而不会重新运行一次。

● 多阶段构建
多阶段构建可以在Dockerfile 中使用多个FROM 语句，基于FROM 语句开始不同基础镜像的构建新阶段。我们可以有选择地将工件从一个阶段复制到另一个阶段，在最终镜像中只留下所需要的内容。多阶段构建的示例代码如下：

程序和配置分离
在构建镜像时，需要将应用的打包文件或者二进制文件拷贝到镜像的特定目录下，通过Configmap、Secret、环境变量等进行运行配置，配置文件无须打包到容器内，以达到灵活修改和管理的目的。在修改配置时，无须对容器再次打包和部署，减少对容器的重启，进而减少业务的中断时间。

以Java 为例，早期的Java JDK 版本并不能发现自己运行在容器之内并通过CGroup来获取应用的内存和CPU 限制，而是获取了主机的CPU 和内存资源。这样存在两个问题：

● JVM 的堆大小默认是发现的内存大小的¼，如果容器的CGroup 限制内存比该值小，那么JVM 进程会经常发生OOM（Out Of Memory，内存溢出）。


最理想的解决这两个问题的方案就是升级JDK 版本，但是受限于实际应用程序的需求等，JDK 版本有时无法升级到需求的版本，所以需要有其他解决方案。

1.top 和free 命令
容器和主机并非完全隔离，容器的proc 文件系统可以看到部分主机proc 文件系统的信息，例如，在容器内查看/proc/cpuinfo、/proc/meminfo 文件，可以获得主机的CPU 和内存信息。常用的top 和free 命令会从proc 文件系统中采集系统运行数据，因此在容器内运行时看到的是主机的资源使用信息，而不是容器内进程使用的CPU、内存和负载信息。

容器内可以通过查看CGroup 的统计来获取CPU 和内存的使用率。


Docker 下默认是阻塞模式，好处是不丢日志，但是如果有应用大量写日志，可能会导致应用一直处在阻塞状态，影响应用性能，甚至可能导致应用出现一些意想不到的错误。而非阻塞模式不会阻塞应用，代价就是没有发送给日志驱动的旧的日志可能被新的日志覆盖，从而导致丢失。

### 10.2 应用接入的最佳实践

emptyDir 卷需要设置sizeLimit，当写的数据超过sizeLimit 后，会被kubelet 驱逐。一般节点的根分区空间不会设置得很大，所以不建议用户通过emptyDir 卷存储大量的数据。

Init Container 可能会被重启、重试或重新执行，所以Init Container 的代码需要是幂等的。

（7）应用配置的传递。
用户可以选择Configmap、Secret、Downward API 对配置进行传递。

● startupProbe：指示容器中的应用是否已经启动。如果提供了启动探测（Startup Probe），则禁用所有其他探测，直到它成功。如果启动探测失败，kubelet 将杀死容器，容器服从其重启策略进行重启。如果容器没有提供启动探测，则默认状态为Success。

● 容器的优雅退出
删除pod 时，kubelet 会先发送SIGTERM 信号给Pod 内的容器PID 1 进程，如果在terminationGracePeriodSeconds 时间周期（默认为30s）内进程没有结束，则kubelet 会继续发送SIGKILL 信号来中止容器的PID 1 进程，从而实现硬退出。在容器的PID 1 进程中，通过处理SIGTERM 信号来实现退出前的处理，例如进行容器的反注册等清理操作，实现优雅退出。

另外，Pod Spec 还包含subdomain 可选字段，可以为Pod 设置子域。假如在Zoo Namespace 下的podhostname 为cat，subdomain 设置为dog，则Pod 具有如下的FQDN：“cat.dog.zoo.svc.cluster.local”

4.Downward API
Kubernetes 还支持通过Downward API 的形式对容器传递Pod 和容器的字段信息，而不需要通过Kubernetes 或API Server 来获取。DownwardAPI 将配置暴露给容器的方式有如下两种：

2）DownwardAPIVolumeFile
同Configmap 和Secret 一样，以文件的方式传递给容器

由于 Configmap 和 Secret 具有可动态更新的特性，所以应用可以在容器内监听Configmap 和Secret 的变化，再将变化的值重新加载，无须重启容器即可达到修改配置的目的。目前，Kubernetes 不支持通过环境变量或DownwardAPI 来修改配置。

### 10.3 应用管理

Kubernetes 副本集（ReplicaSet）对这些需求提供了强有力的支撑，其本质是定义了用户期望以某特定模板创建的Pod 的副本数量。Kubernetes 控制器会确保在节点资源足够的前提下，运行的Pod 数量和版本与用户的期望一致。

Deployment 对象是对副本集的进一步封装，用于不同版本副本集的升级和回滚，Deployment 可定义副本集的版本数量及当前版本号，以及版本升级策略。

Deployment Controller 创建副本集时，需要计算其包含的Pod 模板的哈希值，并以该哈希值作为副本集名称的一部分。当Pod 模板发生任何变更时，该模板的哈希值会发生变更。Deployment Controller 发现哈希值变更后，会以新的模板创建新的副本集，并逐渐增加新版本的副本数，同时减少旧版本的副本数，直到所有版本都变为新版本。这种滚动升级的机制能够确保业务在版本升级过程中（在使用得当的前提下）不会中断。

Deployment 是一个双刃剑，一方面，它利用PodTemplateHash 方便了部署，只需要将容器镜像更新至新版本，Kubernetes 即可完成一次没有业务影响的升级操作；另一方面，这限制了对Kubernetes Pod所做的任何变更，即使只是添加一个 Annotation 也会导致 Pod 重建，从这个层面看，Deployment 又显得过于灵活，制约了对Pod 的任何更改。

StatefulSet 中的serviceName 属性，可以引用一个Headless Service 名称，其作用是为每个Pod 创建一个独立且固定的域名，每个Pod 都可以作为独立个体提供服务。示例中的服务可以通过域名nginx.default.svc.[clusterdomain]来访问，每个Pod 可以通过其对应的固定域名web-[index].nginx.default.svc.[clusterdomain]来访问。

StatefulSet 允许用户定义volumeClaimTemplates，Pod 被创建的同时，Kubernetes 会以volumeClaimTemplates 中定义的模板创建存储卷，并挂载给Pod。这样每个Pod 就拥有了属于自己的存储空间，当Pod 被删除时，对应的存储卷不会被删除；当Pod 被重建时，相同的存储卷会被挂载给新Pod。

Operator 的本质是自定义对象和控制器的组合，既然内置对象无法满足业务需求，就基于CRD 定义扩展对象，并为该对象编写控制器。
Operator 最初由CoreOS 开创，为了实现对etcd 集群的管理，CoreOS 设计了etcd Operator。下面是一个EtcdCluster 的yaml 定义文件示例代码，它包含EtcdCluster 对象的定义，定义内容包括备份策略、容器镜像、副本数、部署反亲和性和版本信息等。

Kubernets 社区参考Etcd Operator 的工作方式，逐渐将其抽象成一种叫作Operator 的设计模式。Operator 模式借助Kubernetes 提供的控制器模式进行框架开发，这些自定义的控制器就像 Kubernetes 原生的组件一样。

随着 Operator 模式的广泛应用，Kubernetes 社区开始推出方便开发的 SDK，借助Operator，只需如下命令即可完成控制器手脚架代码的生成。

Controller 同时生成了控制逻辑的代码框架，只需将控制逻辑写入Reconcile 函数，即可实现Operator 的业务逻辑，

### 第11章 监控和自动修复

追求横向扩展的微服务时代，一个业务流由数十上百个微服务的成千上万个计算节点组成，人工监控显然已不够用，构建适宜的监控系统就成了系统运维的必要条件。有了监控，就有了 “上帝之眼”，就有了洞察过去和未来的能力；没了监控，平台运维就是 “盲人”，对事故的预见性为零。监控是平台生产化的必要条件，为运维提供数据支撑，培养数据驱动（Data Driven）的AIOps 文化。

一般来说，监控系统的数据分为两大类：指标（Metrics）和日志（Logs）

### 11.1 指标监控系统

kubelet 中内置cAdvisor，暴露出API。Heapster 通过访问这些API 得到容器的监控数据。

从Kubernetes 的1.8 版本开始，就由Metrics-Server 来向kubelet 的cAdvisor 收取资源指标，并通过Metrics API 在APIServer 中公开它们。这些指标可以被kubectl top、调度器、水平Pod 自动扩展器（Horizontal Pod Autoscaler，HPA）和垂直Pod 自动扩展器（Vertical Pod Autoscaler，VPA）观察并利用到。

● 各种Exporter + Prometheus + Grafana + Alertmanager通过各种Exporter 暴露不同维度的监控指标，Prometheus 定期向他们拉取指标数据，再用Grafana 进行展示。设定报警规则，异常情况利用Alertmanager 告警。这套方案目前是Kubernetes 平台应用得最为广泛的。接下来我们会着重介绍。

cAdvisor（Container Advisor）是一个开源的分析容器资源使用率和性能特性的工具。它能主动查找在其节点上的所有容器，采集它们的CPU、内存、文件系统和网络使用的统计信息。目前cAdvisor 运行在kubelet 中，指标暴露端口是kubelet 的只读端口，默认是10255，URL 是/metrics/cadvisor。cAdvisor 为客户提供了了解其容器的资源所用和性能特征的一种途径。对于每个容器，其历史资源和性能的数据对故障排除和性能调优都极具参考价值。

（2）kube-state-metricskube-state-metrics 监听API Server 处各个资源对象的变化事件，并生成有关资源对象的度量。它不关注单个Kubernetes 组件的运行状况，而是关注内部各种对象（例如Deployment、Node 和Pod 等资源对象）的运行状况，例如Pod 的启动、完成和终止时间，节点可分配资源和容量等。默认指标暴露端口是80，可通过--port 来指定，指标URL 是/metrics。

Prometheus 是数据收集和分析模块之间的核心。Prometheus 主动向各个 Job 或者Exporters 及Push Gateway 拉取它们所暴露的各种指标数据。它只会使用主动拉取的方式收集数据，但是某些Job 生命周期较短，可能无法等到Prometheus 来拉取时便已消亡，这个时候就需要Job 先将指标数据推送到Push Gateway。Prometheus 再从PushGateway 拉取数据，因此Push Gateway 就类似于一个中转站。

如何得知它们的指标数据路径呢？Prometheus 提供了多种服务发现选项，支持Kubernetes（kubernetes_sd_configs）、consul（consul_sd_config）等平台。每个Exporter 的服务发现和抓取配置等信息，都可以在Prometheus 配置文件的scrape_configs 中进行定义。在一般情况下，一个scrape_configs 指定一个 Exporter。下面是 Prometheus 的配置文件中关于node-exporter 的配置信息：

● relabel_configs：允许在抓取之前对任何目标及其标签进行高级修改。

Prometheus 还支持联邦机制，允许一个Prometheus 从其他多个Prometheus 中拉取某些指定的时序数据。也就是说，Prometheus 对集群联邦也有很好的支持。

Grafana 就是一个图形可视化展示平台，通过各种视图插件展示监控数据，例如热图、折线图、图表等。

Alertmanager 对收到的告警信息进行处理，包括去重、降噪、分组策略、路由告警通知，例如Email、Pugerduty 或者聊天平台（Slack）等。

Operator 本质上是一个自定义资源对象的控制器，其功能是将Prometheus 实例的部署与它们所监视的目标的配置进行分离，使监控真正成为集群本身的一部分，并且抽象出了所使用的不同系统的所有实现细节。Prometheus Operator 定义的CRD 有如下几个：
● Prometheus：定义一个预期的Prometheus 的Deployment 目标，包含数据保留时间、持久卷声明、副本数量、版本等信息。Operator 保证始终有一个 Prometheus Deployment 是满足此对象的定义。

● Alertmanager：与Prometheus 类似，定义了一个预期的Alertmanager Deployment，包含版本等信息。Operator 保证始终有一个Alertmanager 的Deployment 是满足此对象的定义。

### 11.2 日志管理系统

目前我们有如下日志管理系统的解决方案：
● Filebeats + Logstash + Elastic + Kibana：也就是比较有名的ELK 套件。如图11-7所示，每个节点上都装载了一个filebeats 的服务，它能够监控节点上的各种日志文件，将内容实时地推送到Logstash 中。Logstash 会对日志进行初步分析、过滤等操作，对日志形成结构化数据，并转发到Elasticsearch 中。Elasticsearch 则提供存储、分析和搜索功能。它是基于restful 风格、支持海量高并发的开源分布式搜索引擎。而Kibana 则用于为日志提供展示和分析的图表。

### 11.4 自动修复系统

为了解决这个问题，Node Problem Detector 诞生了。它使用Event 和NodeCondition 将问题报告给API Server，使调度器可以看到节点问题，从而避免将Pod 调度到此不良节点上。

11.4.1　Node Problem Detector
如图11-8 所示，Node Problem Detector 部署在每个节点上。多个Problem Daemon（问题守护线程）以Goroutine 的形式运行在其内，可以通过执行特定的脚本和监视特定的日志文件，发现特定类型的节点问题，并将其报告给Node Problem。

一开始Node Problem Detector 以DaemonSet 的形式部署，但是后来我们发现此方案不妥，特别是对于容器运行时问题的检测。当容器运行时出现某些问题时，会导致 Node Problem Detector 的容器退出，从而无法检测和上报该节点的问题。因此，我们建议Node Problem Detector 以系统服务的方式部署在节点上。

### 11.5 事件监控系统

事件（Events）也是Kubernetes 中的一种资源，常常被我们忽略。每个事件会关联到某个资源对象，用以记录这个资源对象在集群中的各个组件处所遇到的各种大事件，有正常的（Normal）有报错的（Warning）的。这些事件并不是永久存储的，默认集群中etcd只保留最近一个小时的事件。

EventRouter（GitHub 地址为heptiolabs/eventrouter）是GitHub 开源的一个事件转发器。其核心功能是以相对较低的开销监听集群中的事件资源对象，并将这些事件推送到用户指定的数据仓库持久化保留。利用这些事件进行系统调试或者对集群上运行的工作负载的长期行为进行分析。默认情况下，EventRouter 通过输出已包装的Json 对象来利用现有的ELK堆栈，便于在弹性搜索中索引这些对象。

### 11.6 状态监控系统

kubewatch 根据配置文件选择性监听API Server 处这些资源对象的变化，例如Pod 新建、更新、重启等状态变化，再通过handler 将这些变化以通知的形式发布到可用的渠道，例如 Slack、Hipchat 等。也可以在应用程序中通过Webhook 的方式来收听kubewatch 发出的状态变化通知。

### 第12章 DevOps

为满足软件快速迭代的需求，软件开发进行了一系列显著的变革：

● 系统架构
臃肿的单体系统被拆分为多个独立部署和独立发布的微服务子系统。独立发布意味着产品每次变更的风险可控制在部分子系统中，风险降低使得更频繁的变更成为可能。

● 组织架构
从单体架构到微服务架构的演进，不仅仅是系统架构的进化，组织架构的配合不可或缺。微服务架构下的每个子系统相对独立，一种常见的组织结构划分方式是，不同的子系统交由不同的开发团队完全负责。这使得系统的每个子系统职责包干到户，开发团队对子系统的可用性、可靠性、性能和新功能规划等完全负责。

● 工具链支持
为支持软件的快速发布，持续集成和持续发布是不可或缺的一环，由此出现了很多自动化工具，包括代码版本管理、代码构建、持续集成和自动部署等诸多环节。容器技术的出现，推动了这一进程的发展。容器镜像的 “一次编译、到处运行” 的特性使得测试环境编译的版本在生产环境也能运行；

DevOps 的目标是：
● 破除部门壁垒，使开发和运维合力交付高质量产品。
● 通过高度自动化持续发布，满足频繁发布的需求。
● 通过构建自动化工具集，提升开发、测试和运维自动化程度，降低重复性工作的比?例。

### 12.1 拥抱DevOps

DevOps 涵盖 “一个中心，两个基本点”——以业务敏捷为中心，构造适应快速发布软件的工具和文化。透过自动化透明的“软件交付” 和 “系统变更” 的流程，构建、测试、发布系统应用能够更加快捷、可靠。

DevOps 倡导运维一体化，本着谁开发、谁运维的原则，开发人员不仅要负责功能开发，还要负责生产系统中与之相关的运维工作。

传统模式下不同组织人员之间的 “隔阂” 被拆除，取而代之的是透明的开发、运维闭环。一方面部门之间的 “混乱之墙” 被打通，另一方面采用更先进的自动化和流程工具来提升软件质量和加速交付流程。

开发人员仍以功能交付为中心，运维人员仍然侧重维护和监控。但是发布环节将开发和运维联系在一起，开发人员的职责后移，除完成基本的功能开发外，功能上线后运维所需的指标收集、监控、性能优化、故障排查等也均由开发负责。事实上DevOps 扩展了研发的职责边界，使其在系统开发的全程都需要考虑后续运维成本。

● 压力与性能测试通过，定义性能指标，能满足用户的性能需求。
● 完成产品的健康检查和性能指标的暴露，提出或者实现监控系统构建的建议或设计。
● 完成设计文档归档，后期问题追溯有据可查。
● 完成自动化部署工具，可以傻瓜式部署软件。
● 完成运维手册，运维人员依照管理手册升级产品并解决现有问题。

DevOps 打破了不同角色的舒适区，开发和运维之间曾经明确的界限正在慢慢变得模糊，这也意味着与传统运维模式相比，对工程师个人的要求更高了

● 协调发布：采用电子数据表、电话会议、聊天工具和企业门户（Wiki、Sharepoint）等协作工具来确保所有相关人员理解变更的内容并全力合作。任何发布的流程需透明、公开。开发人员应遵守发布流程，不应私自将变更上线。一旦线上发生问题，“莫名” 的变更会导致运维人员提升定位和恢复的难度

### 12.2 自治跨职能团队

DevOps 强调人员和组织架构。传统的单体架构的系统，所有代码在同一个代码仓库，不同功能模块高度耦合，代码规模会随时间增长，新功能开发所引发的变更风险变得越来越难以控制。臃肿的代码结构使得个体开发人员很难了解整个系统的全貌，很容易在修复一个缺陷的时候引发更大的缺陷。微服务架构将系统划分成有独立生命周期的子系统后，每个子系统由不同团队全权负责，这使得高度自治的团队成为可能，团队只需关注自己负责的业务和与周边系统的交互，职责清晰。

企业在推进Kubernetes 项目生产化时，如开发团队规模较大，可参考社区兴趣小组的划分方式，设置多个职能小组，每个小组为特定领域完全负责。

● 产品负责人（Product Owner，即PO）负责需求管理、产品规划和定义产品路线图。产品负责人是整个产品成功的关键，他应该熟知客户需求和竞品细节，为自己负责的产品规划未来。定制了产品层面的愿景以后，该愿景会被细化到不同的职能小组，成为小组的执行目标。

对于Kubernetes 这种迅速进化的产品，架构师的一个主要价值是了解业界解决问题的不同选择，判断技术走向。从系统整体架构把握项目的技术路线，在需求、设计、实现和运维各个方面都能对团队成员进行指导。

开发团队负责需求设计、特性开发和部署；运维团队负责故障恢复、异常清理、处理用户请求。开发运维应走向一体化，即开发人员和运维人员在一起工作，亦或是开发人员即运维人员，可以让开发人员更直接地了解线上环境的痛点。这对用户需求的功能设计和优先级定义有极大帮助，解决了在设计时开发人员未过多考虑运维问题，导致后续部署及维护困难。具有凝聚力、领导力和竞争力的自治产品团队，能够把所有相关人员团结起来，真正解决用户的问题，能够先用户之忧而忧，将产品打造成最成功的产品。

### 12.3 敏捷开发

● 主张简单：尽最大可能减少不必要的工作。迅速构建满足用户需求的最小化产品。● 频繁交付：交付周期从数周到数月，越短越好。软件生产化就绪是衡量进度的首要标准。● 可持续化：持续地交付有价值的软件，维持张弛有度的步调，稳步向前。● 充分信任：充分信任团队或个人，为他们提供环境和支持。

瀑布式的主要问题是严格分级导致的自由度降低。由于开发模型是线性的，用户只有等到整个过程的末期才能见到开发成果。如果需求在中途发生变更，很难返回到先前阶段进行更改。越进行到后面阶段，需求变化所需的修改代价越大。如果一开始对需求不明确，则该模型效果较差。项目早期就需要对交付时间做出承诺，通常工作量预估不准确，与实际工作量相差较大，导致员工后期可能超负荷工作。因此，它适用于易于理解的小型项目。

Scrum 团队成员包括三个角色，除了产品负责人和开发团队，还有Scrum Master。Scrum Master 的工作是确保Scrum团队理解和遵循Scrum 的理论、实践和规则。Scrum Master属于服务式领导，服务于产品负责人和开发团队，旨在利用Scrum 实现团队生产力的变革，最大化团队所能创造的价值。产品负责人是管理产品待办事项列表的唯一责任人，清晰地将用户需求添加到产品代办事项条目，并根据优先级进行排序，确保开发团队所执行的工作是有价值的。

### 12.4 GitOps

对于具有依赖关系的功能模块的部署，沟通协调需要耗费大量时间和精力。每个功能模块由多个自治团队独立开发和部署。当功能模块提供的接口有改变时，需要协调依赖该接口的其他功能模块一同部署。如果某个功能模块未一起变更部署，则都有可能导致服务的不可用。

为了解决这些问题，提高交付速度和成功率，GitOps 诞生了，致力于通过零手动更改来实现Kubernetes 集群端到端的应用部署。GitOps 到底是什么？整个交付系统以声明式方式进行描述，使用版本控制系统（例如Git）作为唯一的部署来源，其内文件容纳应用部署于Kubernetes 的所有信息，然后使用Operator 将更改部署到集群。

GitOps 的声明式方式部署，和我们前面提到的Kubernetes 集群声明式管理方式类似。声明式意味着配置是由一组事实而不是一组指令来保证的。

GitOps是DevOps 的重要手段，提供以开发人员为中心的部署方式。应用开发团队可以接管一些运维工作，而运维团队可以更专注于平台的可用性和稳定性。

GitOps 对于开发人员来说很有吸引力，采用了开发人员熟悉的Git 工具来实现部署，增强了开发人员的部署体验，改变了团队之间协同和共享的方式。

所有的部署事项都在合并请求之后即时发生，每个团队每天都能多次发送部署请求，提高业务的敏捷度，快速地响应用户的需求。

当变更失败时，错误恢复就像发出git revert 命令一样，可以很容易地还原集群环境。

● 更强的安全保证得益于Git 内置的安全特性，保障了存放在Git 中的集群目标状态声明的安全性。使用Git 工作流管理集群，能够轻松获得集群所有变更的审计日志，满足合规性需求，提升系统的安全与稳定性。

GitOps 围绕应用源代码仓库和环境配置仓库而发生持续部署。应用源代码仓库包含了应用程序的源代码、配置参数及构建容器镜像所需的脚本等。环境配置仓库包括了部署不同环境所需的基础服务和应用程序的清单，以及它们运行的配置参数、容器镜像版本等信息。每当更新应用程序代码时，都会触发持续构建管道，编译出模块的最新容器镜像。在构建管道时使用新的容器镜像和配置参数更新环境配置仓库，并触发Operator将环境配置仓库的变化部署到集群环境中。

触发策略有两种：基于推送的部署和基于拉取的部署。基于推送的部署，即当环境配置仓库的请求合并后，主动推送部署请求到Operator，触发持续部署动作。

将各个应用程序源代码放在不同的仓库，配置文件信息放在同一个环境配置仓库。使用不同的构建管道来更新环境配置仓库，从环境配置仓库启动GitOps 并部署所有应用程序的工作流程。针对不同的生产环境，环境配置仓库可用不同的分支来区别。当观测到某个分支被更新时，Operator 就会启动部署流程将分支的变化应用到对应的集群环境中。

### 12.5 质量保证

质量被视为软件的生命，而质量保证（Quality Assurance）是软件生命的重要支撑。完善的、成熟的、高效的质量保证方法、规则和流程，能够前瞻性地预见产品的潜在性风险，保障软件产品的质量，赢得客户更多的信任。

● 5%!的(MISSING)时间用于编码，95%!的(MISSING)时间用于测试：不可测试的和没有经过测试的代码是不能交付的。开发人员应具备利用测试框架编写测试用例的能力，并在测试上投入更多的时间和精力。在积累了丰富的测试用例编写经验后，开发人员就能持续提升代码的质量，编写出具有良好可测试性的代码。

● 透明公布任何变更内容：每个版本的变更内容都需要通过某种公开的方式通知运维人员和项目负责人，并附带本次的更新说明、变更步骤说明、验证方案、回滚步骤、变更执行人、变更时间和变更的范围。任何变更都应由相关团队的负责人进行授权。主版本的发布和更新需要项目总负责人的确认才能进行。

● 万物皆可能失效：对于任何小的变更，都不应存在侥幸心理。任何意外都是有可能的。所有变更都应该完成所有的测试用例。对于复杂度特别高的变更，需在测试环境上多次演练。