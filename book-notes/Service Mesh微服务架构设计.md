## Service Mesh微服务架构设计
> 刘俊海

### 前言

Service Mesh技术有效地解决了当前微服务架构和治理过程中的痛点问题

本书从微服务架构和治理角度出发，聚焦Service Mesh的架构设计，试图从微服务技术演进的视角，全面揭开Service Mesh技术神秘的面纱。

### 1.1 为什么需要微服务

通过将单个服务拆分成多个微服务，多个微服务可以独立开发、独立测试、独立运维，不同团队可以并行开发，互不影响，可以有力地支撑业务的快速迭代，方便业务创新和试错。

### 1.2 微服务架构的挑战

监控上，单体服务的监控比较简单，微服务背景下，一个服务变为多个服务，需要对多个服务分别进行监控，对多个服务之间的通信也需要监控，监控管理变得非常复杂。

微服务架构最大的挑战在运维上，微服务的复杂性给微服务运维带来了很多难题。如果不能很好地解决微服务的运维问题，微服务架构的优势就无法体现，微服务改造也很容易陷入泥潭，因此，解决微服务的运维挑战，构建一体化的自动化运维基础设施，是微服务架构的关键一环。

### 2.4 可见可观测

可观测性就是用于上述微服务状态监控的工具集合，具体分为Logging（日志系统）、Metrics（度量系统）和Tracing（分布式跟踪系统）3个层次

日志是最常见、最通用的监控手段，但服务的日志监控和告警一般都需要人工添加，不仅效率低，也很容易遗漏。同时不同服务的日志格式和日志信息可能都会有差异，不太方便进行标准化，不仅日志收集、处理和展示比较麻烦，有太多个性化的需求，而且每个服务的日志都不统一，基于日志的全系统问题定位也非常麻烦。

Metric定义一套完善的日志收集、传输和处理标准，通过Metric可以实现日志和监控的标准化，同时基于Metric的日志聚合特性，聚合后的日志会小很多，减少日志系统的成本开销。

Metric系统侧重事件聚合，相比日志来说，Metric关注的对象相对小一些，比如服务QPS、成功率、耗时等，聚焦核心指标，基于Metric可以方便实现服务核心指标的监控告警自动化。Tracing关注请求级别的处理。使用过程中一般先关注日志监控是否有问题，如果有问题通过Metric查看具体的服务和指标异常，最后通过Tracing进行全链路排查。

### 3.1 Service Mesh基础

服务间通信是Service Mesh技术面对的问题域，对微服务屏蔽通信的复杂度，解决微服务的通信治理问题；请求的可靠传递是Service Mesh的目标；轻量级网络代理是Service Mesh的部署方式；对应用程序透明是Service Mesh的亮点和特色，Service Mesh接入对业务无侵入，可以非常方便地获取Service Mesh带来的便捷性，算是Service Mesh的一大优势。

有了Service Mesh之后，微服务也不再需要关注RPC通信（包含服务发现、负载均衡、流量调度、限流降级、监控统计等）的一切细节，真正像本地调用一样使用微服务，通信相关的一切工作直接交给Service Mesh。

Sidecar模式下，网络代理服务在微服务旁边，为微服务提供通信和链路治理功能。因此，数据平面代理服务也经常被简称为Sidecar。

SDN（Software Defined Network，软件定义网络）将数据平面和控制平面分离，控制平面具有可编程性，使得网络更加智能、灵活和易扩展，激发了网络技术的又一次革命。

第二代Service Mesh借鉴了SDN的思路，基于控制平面和数据平面分离思想，有了完善的控制平面：①所有的代理服务都由控制平面掌控，因为控制平面可以控制整个系统，所以提供了强大的控制能力和策略能力；②将具体的控制逻辑从数据平面移除，简化了数据平面的设计，数据平面不需要和外部系统进行交互，数据平面完全聚焦在变更频率很低的流量路由和转发逻辑上，提升了数据平面的稳定性。

数据平面负责代理微服务之间的通信，具体包含RPC通信、服务发现、负载均衡、降级熔断、限流容错等

控制平面负责对数据平面进行管理，定义服务发现、路由、流量控制、遥测统计等策略，这些策略可以是全局的，也可以通过配置某个数据平面节点单独指定。控制平面通过一定的机制将策略下发到各个数据平面节点，数据平面节点在通信时会使用这些策略。

### 3.2 Service Mesh的发展历程

2016年9月，开发Linkerd的Buoyant公司第一次提出了Service Mesh的概念。

### 3.3 Service Mesh项目Linkerd

1）高性能、高扩展性的集群通信能力。每秒以最小的时延及负载处理万级数量的请求，Linkerd 2.0的数据平面当前大小仅为10MB左右，并且99.9%!的(MISSING)请求耗时小于1ms；部署方便，业务代码和配置不需要任何修改，特别方便水平扩展。2）支持多种服务发现方式，并且扩展非常方便。支持各种服务发现机制，如基于文件（File-based）、ZooKeeper、Consul及Kubernetes；增加自定义的服务发现机制比较方便，扩展性强。3）强大的负载均衡算法支持。推荐使用基于感知时延的负载均衡算法，通过实时统计数据RPC延迟、要处理请求队列大小决定如何分发请求，反馈实时性强，能够根据当前的实时压力情况调整负载；内置多种负载均衡算法，可以根据实际场景灵活选用。

### 3.4 Service Mesh项目Istio

Envoy是一个用C++编写的云原生高性能边缘代理、中间代理和服务代理，作为专门为微服务架构设计的通信总线，定位是作为Service Mesh的数据平面，接管微服务通信的全部流量，对应用程序屏蔽网络和通信的复杂性。

架构上Envoy内部可以分为数据平面、控制平面和管理平面3个部分。其中，控制平面用于对流量路由和转发相关的策略与配置进行管理；控制平面通过标准API获取最新的流量配置信息；数据平面的流量转发就是基于控制平面下发的配置规则进行。

Envoy接收到Downstream客户端发过来的请求时，一般先经过类似Iptables的透明拦截机制，然后将流量交给Envoy。Envoy获取请求对应的原目的地址，根据目的地址信息获取对应的网络过滤链配置信息，根据网络过滤链配置建立新的连接，并通过网络过滤链对应的协议对请求进行解码和路由，选择合适的Upstream节点，将请求发送出去，同时建立Downstream连接和Upstream连接的对应关系，将Upstream返回的响应消息发送给Downstream客户端。Env

Envoy插件机制是Service Mesh数据转发和数据处理的基石，基于插件扩展性机制进行扩展和定制开发非常方便，可以建立起强大的Service Mesh生态。

配置动态化是Envoy架构层面最优雅的设计。基于配置动态化设计，Envoy集群管理与流量转发相关的所有配置均可以通过XDS协议动态下发和生效，不仅减少了运维复杂度，同时借助动态配置化能力，方便通过一定机制发现Envoy运行时的风险并快速调整，提高了Envoy的整体稳定性。

Envoy架构设计对性能的考虑随处可见。比如数据转发层面，Envoy采用异步事件驱动的方式，并且保证一个请求只会在一个线程内处理，减少了请求在线程间切换的开销；此外通过数据平面和控制平面分离，配置变更时，通过无锁机制保证数据转发的性能不受任何影响。

### 3.5 Service Mesh其他解决方案

华为公有云和腾讯云的PaaS微服务治理平台当前均支持Service Mesh技术，其中华为公有云采用自研的方式，腾讯云的TSF微服务平台采用基于开源的Istio/Envoy技术栈，进行扩展和定制。

### 3.6 Service Mesh云上产品

架构层面，AWS App Mesh基于开源的数据平面代理Envoy构建，使用Envoy管理AWS微服务的所有进出流量，控制平面是AWS自研，用于通过XDS API对Envoy进行配置和管理。AWS App Mesh支持Kubernetes、Amazon ECS、Amazon EKS等多个平台。

### 4.4 Envoy数据平面API

XDS API当前支持两种协议，V1版本的REST协议方式和V2版本的数据平面gRPC协议。V1版本的REST协议针对LDS、CDS、EDS和RDS等。每种XDS资源和Pilot-discovery均会建立一个连接，集群规模比较大时，不仅会有连接资源的浪费，Pilot-discovery服务也会有一定的连接压力。

通过CDS更新集群信息；

2）通过EDS更新集群节点信息；
3）通过LDS更新监听器相关信息；
4）通过RDS更新集群路由信息。

和REST协议相比，gRPC协议编码效率更高，同时，gRPC支持连接双向通信，带来了通信效率的极大提升。

### 4.6 Envoy与Nginx架构层面的对比

Nginx是Envoy出现之前网络通信中间件领域非常有代表性的开源系统，功能强大，性能出色，扩展性很强，已经形成了强大的生态，成为HTTP流量管理领域事实上的标杆。

Nginx最核心的功能是Web服务器和反向代理服务器，Web服务器完成对HTTP请求协议的解析和以HTTP协议格式响应请求、缓存、日志处理这些基本Web服务器功能；反向代理服务器完成对请求的转发、负载均衡、鉴权、限流、缓存、日志处理等代理常用功能。

除了对Nginx协议的支持外，Nginx还支持普通的TCP、UDP反向代理功能，同时以stream方式支持通用的基于4层协议的反向代理，比如MySQL代理、Memcached代理等。

Envoy的目标比较远大，定位是透明接管微服务之间的通信流量，将通信和服务治理功能从微服务中解耦，通过Envoy可以方便地增加对自定义协议的支持。
概括起来，Nginx的关键词是Web服务器和反向代理，Envoy是透明接管流量，更加体现对流量的控制和掌控力。

使用方式上看，微服务对Nginx是显式调用，通过Nginx完成负载均衡等相关功能，对Envoy是隐式调用，业务微服务不需要感知Envoy的存在，和使用Envoy使用相同的方式进行通信，只不过不再需要关注通信和链路治理的细节。

网络模型上，Nginx采用的是经典的多进程架构，由master进程和worker进程组成。其中，master进程负责对worker进程进行管理，具体包含监控worker进程的运行状态，根据外部输入的一些管理命令向worker进程发送处理信号以及worker进程退出时启动新的worker进程。

和Nginx不同，Envoy采用了多线程的网络架构，Envoy一般会根据当前CPU核数创建相同个数的worker线程，所有worker线程同时对Envoy配置的监听器进行监听，接受新的连接，为每个新连接实例化相应的过滤器处理链，处理该连接上的所有请求。和Nginx类似，Envoy的每个请求的处理全流程都在同一个线程下进行。

### 5.1 Istio整体架构

Istio在微服务之间建立连接，接管通信功能，对业务微服务屏蔽通信细节，同时通过流量控制、策略控制、遥测统计、Istio安全机制等对微服务进行监控和管理，使得微服务架构更加健壮、安全和易扩展。

数据平面除了负责实际流量转发工作的数据转发代理服务istio/proxy之外，还有几个关键组件：pilot-agent负责转发代理的启动和生命周期管理，istio-init负责流量透明拦截的配置，pilot-agent、istio-init和转发代理一同部署，共同为业务流量转发保驾护航。

### 5.2 Istio的Kubernetes基础

Kubernetes是Google 2014启动的开源项目，它基于Google长达15年生产级环境下的集群管理经验，同时结合来自社区的最佳创意和实践经验

API Server负责Kubernetes的对外交互，提供了操作Kubernetes资源的唯一入口，对Kubernetes集群进行的所有修改、查询和管理都需要通过API Server进行。

Scheduler（调度器）负责节点的资源管理和调度，主要工作是接收API Server的Pod管理任务，并分配到具体的节点上，并把调度结果写到etcd中。

Kubelet（节点管理器）实现对工作节点的管理，并定期将节点上容器的运行状态汇报给API Server，并通过cAdvisor监控容器和节点资源，节点管理器具体管理的资源包括Pod、容器、镜像、数据卷等。

Kube Proxy是Kubernetes在每个节点上运行的网络代理，负责Pod的访问。Kube Proxy负责为Pod创建代理服务，实现服务到Pod的路由和转发。

·metadata：唯一标识该对象的元数据，包括name、UID、可选的namespace。
·spec：标识对象的详细信息，不同对象的spec的格式不同，可以嵌套其他对象的字段。

Kubernetes当前通过Kube Proxy只能提供基本的通信能力，对于精细粒度的集群管理、流量调度、服务发现、负载均衡等，还需要业务服务自己解决。Kubernetes的定位是服务调度和服务生命周期管理，微服务通信本身不属于Kubernetes的重点支持范畴。为了减少微服务上云的复杂度，同时为了实现Kubernetes上微服务的标准化，需要有一套完善的基础设施，系统地解决微服务的通信问题，Istio正是解决Kubernetes集群的通信标准化问题，提供业务透明的通信解决方案。

基于Kubernetes提供的云化基础设施，有力地推动了Service Mesh的快速发展和落地。

### 第6章 Istio控制流设计

Istio提供的最核心能力是对业务流量进行透明转发和治理。其中，数据平面聚焦流量转发、链路管控和治理；控制平面扮演控制流的角色，为数据平面提供强大的管理和配置能力，为数据平面的简洁高效通信保驾护航。

### 6.1 Envoy生命周期管理

Envoy注入分为手工注入和自动注入两种方式。手工注入是指使用istioctl工具构建包含Sidecar镜像的模板，然后通过kubectl apply–f xxx.yaml将包含Sidecar镜像的模板文件注入到Pod中；自动注入基于Kubernetes的Admision Webhook机制，在Pod资源创建过程中自动触发。手工注入方式整体效率仍然偏低，而自动注入方式充分利用Kubernetes平台提供的机制和能力，实现了Sidecar透明接入，所以推荐使用自动注入的方式来完成Envoy的注入。

### 第8章 Istio微服务治理

链路的可观测性是服务治理非常重要的组成部分。基于链路可观测性，可以对系统的状态有全面的把控，也可以对链路进行相应的治理和控制。

### 第9章 Service Mesh架构的工程化设计

云计算时代，云计算基础设施的强大支持给容器化服务在架构开发、测试、运维等全流程均带来了革命性的变化。

### 9.5 API和SDK设计

讨论声明式API设计之前，先来了解“命令式”和“声明式”这两个相关的概念。“命令式”和“声明式”是来自于编程语言的两个概念，“命令式”有时也称为“指令式”，是命令机器做具体的事情（what），不管最后的结果是否是你的预期，都会按照你的命令来实现；“声明式”有时也称为“描述式”，是告诉“机器”你想要做什么，由“机器”想出如何去做。

### 10.1 Service Mesh和Serverless

Serverless中文可以翻译为“无服务器架构”，是最近几年兴起的一种新型架构理念，Serverless不是指不需要服务器，主要是强调在Serverless下，业务不再需要固定的服务器资源，而是按需分配相应的服务资源。当业务请求比较多时，会多分配相应的服务实例；当业务请求比较少甚至为零时，可以不分配任何服务器资源。Serverless架构和之前的架构相比，最大的差异是：业务服务不再是固定的常驻进程，而是真正按需启动和关闭的服务实例。因此Serverless架构可以提供如下两点优势。

在Serverless中，按需执行的代码片断称为“函数”，它是Serverless资源管理和调度的基本单位，因此Serverless有时也被称为FaaS（Function As A Service，函数即服务）。

Knative是一个以Kubernetes和Istio为基础的Serverless开源架构方案，目的是解决Kubernetes环境下Serverless应用的构建、部署和运行。

架构上，Knative由Build、Serving和Eventing 3个核心组件组成。其中Build负责构建工作，把用户定义的函数和应用构建成容器镜像；Serving负责计算工作，具体包含Serverless实例的路由和访问，以及Serverless实例的按需伸缩；Eventing负责事件工作，自动完成事件的绑定和触发执行。

Knative的Serving系统。Serving系统基于Kubernetes和Istio进行开发，利用Kubernetes强大的容器调度和生命周期管理能力以及Istio的通信和通信链路治理能力。另外，为了屏蔽Kubernetes和Istio的复杂度，Knative Serving自身有更高一层的抽象能力，提供一组用来对应用和通信进行管理的抽象概念（可使用Kubernetes CRD对这些抽象概念进行管理）。

）Route：工作负载的路由规则，对应Istio的VirtualService。2）Configuration：应用的最新配置，对应Kubernetes的Deployment。3）Revison：每次对工作负载进行修改的快照，对应Istio的Subset。4）Service：对工作负载的整个生命周期进行管理。

Knative的Serving系统做的事情和Kubernetes、Istio大体相同，但通过提供更高的抽象，屏蔽了Kubernetes、Istio的细节，自动帮应用管理好Deployment、VirtualService以及auto scaling之间的关系。

如果从云计算的发展趋势上看，通信功能会被Istio完全接管，并下沉为基础设施的一部分。在通信层面，Istio肯定更聚焦、更专业，Knative从大趋势出发，直接复用Istio的通信层功能，避免后续架构上的修改和返工，可以将自身聚焦到Serverless层面的功能和生态打造上。从架构的角度看，这也是Unix设计哲学的体现——整个系统采用模块化设计，每个模块只聚焦一件事情，并且做好做到极

### 10.2 东西向和南北向通信的统一

在微服务的整体架构中，存在着东西向通信和南北向通信两种方式。所谓的东西向通信是解决业务内部各个微服务之间的通信和链路治理；南北向通信解决的是集群和外部的交互问题，具体包含集群服务如何提供给外界访问，以及集群内部如何访问外界的服务。

Service Mesh本身主要是聚焦东西向的微服务内部通信。微服务内部服务通信的特点是流量不太大，QPS通常来说不太高，对性能总体要求不高，强调的是通信链路的精细化管控能力和稳定性保障能力，如何通过相应的机制减少业务对通信的关注度，给业务提供效率和稳定性支撑。

南北向通信有两个显著的特点。第一个是流量特别大，需要承载整个业务所有的对外流量，所以QPS非常高，对性能有着非常高的要求。第二个是南北向通信位于业务内网和外部网络之间，需要对业务内网进行保护和拦截，防止非法和异常请求对内网的攻击和影响，因此南北向通信对安全有着非常高的要求，需要不断对安全防控和安全策略进行升级，保护内网业务的安全和稳定性，因此对于南北向通信来说，性能和安全是关键词。

### 10.3 云原生时代的Service Mesh

云原生包含一组应用模式，用于帮助企业快速、持续、可靠、规模化地交付业务软件。云原生由微服务架构、DevOps和以容器为代表的敏捷基础架构组成。

### 10.4 Service Mesh现状和展望

采用基于Iptables的流量透明拦截，客户端与服务器端的服务调用次数和直接调用相比多2次，同时Iptables本身也有很大的性能损耗。针对流量透明拦截带来的网络和访问开销，Envoy已经尝试通过Cilium技术来解决。

### 附录 Service Mesh迁移的要点与原则

如果一定要在非Kubernetes环境下实施Service Mesh，可以在数据平面使用Envoy，控制平面根据XDS协议自己研发，这样也非常灵活，方便控制。

（2）先进行Kubernetes容器化改造，再接入Istio

对于近期有Kubernetes容器化需求的团队，可以在Kubernetes容器化完成之后，再进行Istio接入，Istio在很多方面利用了Kubernetes强大的基础设施能力，所以只有在Kubernetes下Istio才能最大限度地发挥作用，并且减少运维上的复杂度。

为了减少Service Mesh迁移过程中的风险，必须采用渐进式迁移原则，每次只迁移一个服务，迁移后观察足够长的时间，没有问题后再进行接下来的迁移。

需要针对每个迁移动作增加一键回滚开关。如果遇到线上问题，可以立即通过开关关闭迁移特性，回归到迁移之前的状态。