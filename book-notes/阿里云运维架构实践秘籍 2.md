## 阿里云运维架构实践秘籍
> 驻云科技 乔锐杰

### 推荐序

中国最早的云计算厂商阿里云于2009年成立，至今，云计算在中国大地上已经走过10多个年头，它作为互联网的基础设施，也已经在不知不觉中非常深刻地影响着整个社会、整个世界，并且这种影响还在不断发酵。上云这个词在2013年和2014年可能还是新鲜词汇，但今天还没有使用云计算的企业反而更像是稀有品种。

### 前言

所以说DevOps是运维行业的未来之路。云计算只是一个开始，不管是运维、开发还是架构等，未来你需要更多技能才能顺应互联网发展的潮流。

2009年的阿里云，2011年的盛大云，2012年的电信天翼云，2013年的腾讯云及华为云，2015年的百度云，国内的云计算市场可谓百花齐放。鉴于阿里云的领先地位，加上我本身也一直基于阿里云从事云端产品研发、云端迁移、云端运维、云端架构、云端安全、云端培训等方面的工作，

### 绪言 云计算带来的技术变革

未来，云计算将成为互联网的基础设施。作为技术过来人，我亲历了中国互联网时代的发展、云计算云时代的发展，以及技术人员眼中云计算所带来的技术变革。

1994年4月20日，中关村地区教育与科研示范网络（NCFC）通过美国Sprint公司接入国际互联网（Internet）的64K国际专线开通，标志着我国正式全功能接入国际互联网，这一年也是中国开启互联网时代的元年。

1994年4月20日中国实现了与国际互联网的全功能连接，但是直到1998年，我国的门户网站才兴起，中国真正意义上正式进入网络时代，这也是真正意义上的中国互联网的起点。

中国互联网巨头也大多于1998年左右成立：❑ 网易：1997年6月；❑ 搜狐：1998年；❑ 腾讯：1998年11月；❑ 新浪：1998年12月；❑ 阿里：1999年；❑ 盛大：1999年11月；

❑ 百度：2000年

“IOE”是这个时代的经典组合：服务器提供商IBM、数据库软件提供商Oracle、存储设备提供商EMC。

从工业时代到电气时代，再到信息网络时代，随着科技的发展，云时代是人类科技进步的阶段性时代。就如同云时代的下一个时代是人工智能，人工智能是不是人类科技的终点，答案还需探索。

2005年，Amozon宣布AWS（Amazon Web Service）云计算平台成立，这是全球云计算落地的起点。2009年，阿里云成立，这是中国互联网云计算的起点。随着这几年云计算从概念期转入落地期，云计算低成本、高稳定性、高效率、可灵活扩展等诸多优点所带来的红利让千千万万企业受益。

只不过它在虚拟化+分布式+自动化平台的基础上解决了以下问题：
❑ 稳定性：采用分布式集群部署，保障服务不宕机。
❑ 弹性扩展：按需索取，一键式秒级开通资源。
❑ 安全性：采用分布式多副本冗余部署，保障数据不丢失。

❑ 易用性：Web管理控制台，智能化便捷操作。

不同的是，云计算是一种新的互联网模式，这种模式本质上如同IDC的IT模式。传统计算资源的使用流程是采购→机器安装配→机器上架→机器后期机房托管。而在云端，在Web界面简易的一键式操作就能获取计算资源。就如同使用水和电一样方便，按需索取。

云计算是面向广大用户的，对应的是一个分布式云管控平台（简称云平台），用户可以在这个云平台上管理及使用对应的云资源。众所周知，云平台具有稳定性、可弹性扩展、安全性、低成本、易用性等优点。

平台化设计的一个前提就是产品封装性，封装性带来了定制灵活性的问题。比如最开始SLB七层并不支持虚拟主机的功能、RDS的MySQL不开放root权限、阿里云ECS不支持组播和广播。产品封装性带来的定制灵活性的问题在传统物理机器上是不存在的，所以这一点也是云平台的缺点。

云端产品封装性带来的灵活性的问题，可以在业务层、架构层、运维层进行协调解决：
❑ 比如，最开始对于SLB的虚拟主机的功能灵活性需求（此功能现在SLB已具备），可直接在ECS上搭建Nginx来满足。

云计算的普及，也促使最新热门的开源技术的普及使用。因为大家使用的大多数云产品都基于热门开源技术做了定制化开发及封装。比如，负载均衡（Server Load Balancer, SLB），其中，四层就是基于LVS（Linux Virtual Server, Linux虚拟服务器，它是一个虚拟的服务器集群系统。本项目在1998年5月由章文嵩博士成立，是中国国内最早出现的自由软件项目之一）、七层就是基于Tengine（Tengine是由淘宝网发起的Web服务器项目，它在Nginx基础上，针对有大访问量需求的网站，添加了很多高级功能和特性）等做了对应定制化开发及优化。

大多数运维人员都喜欢墨守成规。因为系统90%!的(MISSING)事故都是由变更导致的，所以我们担心新的技术、新的软件版本会影响系统稳定性，

这些技术已经在被逐步淘汰，在云上，我们可能再也接触不到物理服务器。相关软件的安装配置、调优、高可用等都会由云平台来完成。未来的技术方向是我们如何更好地使用、实践这些云产品及云技术。

运维自动化一般有以下几个阶段：❑ 人工阶段。日常运维全部靠人工来做。❑ 脚本及工具阶段。

❑ 平台化阶段。用平台界面智能化操作来完成日常运维命令或执行脚本/工具，以进一步提升效率。❑ 智能化阶段。智能运维，如自动扩容、故障自愈等，进一步减少人为参与，进一步提升运维效率。

2）云计算体系阶段。在传统硬件服务器的基础上，通过虚拟化及分布式技术形成对应的云资源平台。

3）容器体系阶段。

1）单机架构的阶段。

）集群架构的阶段。集群架构其实是单机架构的演变。集群架构的典型技术特点就是，一般采用虚拟VIP技术（如Keeplived、Hearbeat）解决单点故障问题，让架构高可用，如图1所示。而值得注意的是，在云端，对应的云产品底层都用集群架构来保障高可用。

图1 单机架构向集群架构演变

3）分布式架构阶段。分布式架构最重要的特点就是，不管是业务代码还是数据库，都是通过多台服务器以分布式模式部署的。如果业务压力增加，那么就增加对应的服务器资源。

这个架构对服务器单机的性能依赖不高，主要通过大量的云资源进行分布式快速部署，来满足业务发展和迭代需要。

集群的虚拟VIP技术只能将一台服务器作为热备（Backup），并且只有在主服务器故障的时候，才会切换到热备上，平时其都是处于空闲状态。而分布式架构的技术特点就是，负载均衡的引入，让不同服务器同时应对业务压力，而如图2所示。

4）微服务架构阶段。微服务架构本质上也是分布式架构，微服务其实是业务功能层面的一种切分，即切分成单个小型的独立业务功能。多个微服务通过API网关（Gateway）提供统一服务入口，对前台透明，而每个微服务也可以通过分布式架构进行部署，

微服务在云平台的基础上结合Docker容器技术进行部署，能让业务、运维、架构在技术和非技术方面的稳定性、成本、效率、可扩展性等都达到最优。

### 第一篇 云端选型篇

在2013年我在担任阿里云架构师一职时，

如不停地告知客户云计算在便捷性、安全性，以及成本的节省等方面具有怎样的优势。

都会依托于云计算，云计算将成为企业IT资源的必需品。

截至2018年12月，阿里云有两百款产品或服务。

### 1.1 全球云厂商占比

1）亚马逊2018年的市场占有率为30.4%!，(MISSING)依然占据无法动摇的主导地位，相信业内人员对亚马逊在云计算领域的地位也是能感同身受的。

### 1.2 国内云厂商的现状

用VMware等虚拟化技术虚拟出来的VPS（Virtual Private Server，虚拟专有服务器）。从技术角度来看，云主机是分布式的，所以在稳定性、资源隔离、数据安全性等方面都能得到保障，而VPS是单机版。

因为分布式的云平台一般采用三份数据冗余，所以某台物理机硬件有问题根本不会造成用户数据丢失。

阿里巴巴在电商领域拥有海量用户、海量数据、高访问量/高并发的业务场景，也拥有一流的技术实力。

特别是2014年阿里巴巴收购了万网，致使阿里云拥有了一键便捷式的域名备案服务。这方面的优势是国内其他云服务提供商甚至亚马逊都无法相媲美的。

腾讯作为游戏开发商，在游戏领域有着强大的渠道、丰富的经验。比如腾讯游戏在运维领域累积的“蓝鲸”运维自动化平台，就无缝对接了腾讯云。相比国内阿里云及其他云平台，这是腾讯云的优势和特点。

华为是全球领先的信息与通信解决方案供应商。华为的优势在于通信硬件设备，以前中国移动大多都是用思科的通信设备，后来全部换成华为的通信设备。

所以，结合华为在通信硬件设备方面的优势，华为云的优势与特点主要在于企业级私有化解决方案。在安全、大数据等云资源服务上，相比阿里云、腾讯云，华为云还存在一定的差距。

百度云的优势在于人工智能相关服务上

2012年，电信推出天翼云。其实电信在机房、硬件方面有着得天独厚的优势，但由于种种原因，天翼云在国内云市场一直无所作为。

Ucloud、青云也曾在国内云市场小有名气。相比阿里云、腾讯云这样的大云厂商，Ucloud、青云通过云产品功能细化以及服务质量提升来吸引客户。其中，Ucloud服务包括如下几项：1）在工单响应方面，Ucloud跟客户单独建立聊天群，里面有客服、技术人员，所以客户的问题每次都能得到快速响应及解决。2）Ucloud具有独有的服务器托管服务。而当时阿里云、腾讯云不能提供这项业务。

在海外的业务部署上，建议优先选择亚马逊。在国内互联网业务部署上，建议优先选择阿里云。在私有云、私有化输出方面，建议尝试使用华为云，毕竟华为是做硬件出身的。如若在游戏、运维等细分领域有更深的需求，建议尝试使用腾讯云。若对云产品功能特性、存储或者其他细分领域有需求，建议使用Ucloud、七牛云等云厂商的产品。

不同云平台的技术特性代表不同的技术方案，最终选择哪家云厂商还是取决于业务需求。

### 2.1 阿里云产品概要

ECS、RDS、SLB、OSS这是阿里云4款打江山的核心产品，简称“阿里云四大件”。

因为OSS的使用有个大门槛，就是需要通过API对文件进行增删查改。

比如，在Linux运维中可以通过“Crontab+OSS工具”实现文件的异地备份，这种运维方式在很大程度上替代了传统“备份服务器”的角色。当然OSS在图片、转码等领域甚至可以结合CDN来使用，应用场景也较多。

❑ Memcache、Redis相关缓存产品。真正需要引入缓存时，侧面说明企业业务是中大型的。在传统中小型企业应用中，业务代码大多就跑几台Tomcat、PHP就行了。

### 2.2 云产品的8/2选择原则

“使用RDS就不用你考虑安装配置、调优、备份、扩展等问题，拿来就用，为什么还要自己去折腾搭建！”

但使用云产品时，我们也不用关心冷备、热备方面的问题了，因为这一切云产品都默认帮你实现了，你只需要“傻瓜式”地操作即可，哪里不会点哪里。

1．责任方面在非技术方面，云产品有一个很大的优势，即如果是自己搭建维护的平台出了问题，那么对客户、对领导都要负责，对应的损失也得自己承担；相反，如果使用云产品出现了对应问题（当然主要是云产品稳定性、性能、安全性、高可用等本身问题，不包括因用户不会使用及对应误操作等导致的问题），那么对应的损失甚至可以找云厂商进行索赔。

RDS最主要的规格性能参数是：“最大连接数：4000; IOPS:8000”。所以在实际部署中，要让数据库达到如此性能。我们一般采用CPU和内存配比为1∶4的ECS配置（数据库偏向内存型应用，具体实践参考第5章），如4核16GB、8核32GB、16核64GB。在上述ECS的配置清单中，默认推荐选择8核32GB是为了保障自建数据库的性能和稳定性。而且RDS的高可用版是双机高可用版，我们在ECS上自建的MySQL是单机版，这里还需要再开一台做主从，以保障数据库数据的安全性和高可用。这样一来，成本就进一步增加了。

阿里云的很多云产品，如SLB、RDS、CDN等，其核心都是基于开源技术进行了封装并做了相应优化，然后形成对应的云产品。

### 3.2 硬件的天下

❑ C语言最主要的使用领域应该是Unix系统开发以及某些Unix系统软件的相关开发，所以常见的开源技术的源码，如Nginx，大多数都是用C语言编写的。

❑ C语言在嵌入式开发中，依然居于“舍我其谁”的霸主地位，主要是因为C语言跟底层硬件的结合度较高。

### 3.4 “胶水语言”

Python的主要优势还是在于代码量小、维护成本低、编程效率高。语言的特性决定了语言的应用场景，即同样一个需求，用不同语言来实现，代码量相差很大。一般情况下，Python代码量是Java的1/5。

说到运维、自动化运维、DevOps, Python一直是热度最高的开发语言。基本上Linux系统都会默认安装Python，加上Python在Linux OS层面有丰富的库，这是运维实现自动化运维而替代人工运维的重要手段。比如，通过Python实现备份、SSH登录、文件操作等，其实跟Shell一样简单。Python直接导入对应的OS库，用简单几行代码就可搞定。若想用Java来实现这些功能是很不容易的事，因为Java不擅长。所以在一些自动化的运维平台中，我们优先选择使用Python。在一些系统类的运维工具中（比如前面所说的Ossutil工具），我们也是优先选择Python。

我面试过很多资深的运维。他们在职业生涯发展到一定阶段、运维领域方面的技术已经学得差不多的时候大多都会遇到同样的瓶颈，就是开发能力的缺失，导致其没办法在自动化运维领域及技术领域做更多的事情，所以很多人希望有机会掌握Python这种技术。

如果没有用Python写够几万行代码，我觉得还是停留在学习Python语法的阶段。但这也是运维一直以来最大的痛点。在日常运维场景下，没有更多需求、复杂场景能让我们写更多的Python代码了，这是我们一直不具备研发能力的客观原因。所以想熟练掌握Python，别无他法，只能从事相关的职位或进行相应的开发。

我当初学习Python，也是因为在DevOps方面有需求。我花费了一个星期的时间学习Python的语法和特性，然后在OSS的客户端工具（Windows中基于OSS的一个类似FTP的工具，能同步Windows文件目录到OSS中）上面做二次开发，增加新的功能。通过这个项目，我熟练掌握了Python语法、特性、多线程编程。

业务场景的需求、开发进度的压力促使我掌握了这门技术，而且越来越熟练。

所以在数据分析、数据挖掘这方面，很多公司都会选择使用Python, Python在这方面有先天的语言优势。对于很多初学者来说，要想快速掌握Python，通过爬虫来做业务场景的演示是不错的选择。

### 3.5 “世界上最好”的语言

PHP是脚本语言，我们不需要做相关的编译操作。而Java的运行需要先把Java源代码文件编译成class二进制文件。所以在PHP的发布部署、变更中，我们只需要替换以前的文件就能即刻验证。但在Java应用中，比如Tomcat，变更代码后需要重启。

PHP和MySQL、Memcache、Redis的集成很简单，甚至直接在php.ini配置文件中简单配置一下就能将Session存放在本地磁盘或者Memcache、Redis中。而Java做Session的集中化管理就没这么简单了，需要通过代码来实现。

### 3.6 最适合高并发的语言

多人选择用Go语言来替换Java，以避免Java占用系统性能资源过多。

Go语言特别适合编写一些有性能瓶颈的业务，内存占用也非常少。其近C语言的执行性能、近解析型语言的开发效率以及近乎完美的编译速度，使其风靡全球。

### 3.7 唯一的前后端语言

后来Node推出了PM2——一款专门管理Node进程的工具（类似PHP的进程管理工具PHP-FPM），这让Node在云端的维护管理变得规范化。

### 3.8 不可替代的机器语言

汇编语言的传统应用场景主要有两个：❑ 在单片机编程里面，使用汇编语言，直接操作硬件。

### 第4章 系统技术选型

其核心内容包含云端网络、云端Web服务器、云端负载均衡、云端存储、云端缓存、云端数据库6个大类，基本覆盖了云端千万级运维架构常见的运维技术。

### 4.1 云端网络的三种选型策略

金融云网络也是基于VPC的专有网络，只不过在此基础上加入了很多安全规则及限制保障更高的安全性需求。

经典网络采用三层（网络层，即IP层）隔离，所有的经典网络类型实例都建立在一个共用的基础网络上。VPC采用二层隔离（数据链路层），相对经典网络而言，VPC具有更高的安全性和灵活性。在2014年以前，阿里云只有经典网络的

经典网络的内网IP是以10开头的随机IP，且内网IP只能随机分配，不能自定义。而在VPC网络中，每个客户都是独立的网络环境。客户可以自定义网络的IP段、网络架构等。

经典网络绑定公网的ECS（Linux系统），系统中网卡是两个网卡。eth0是内网网卡，eth1是公网网卡。如果没有绑定公网，则经典网络仅有一个内网eth0。

而VPC网络，即使绑定了弹性IP的ECS网卡也只有一个eth0，即不管有没有绑定弹性IP, VPC网络的ECS仅有一个eth0网卡（绑定弹性IP的时候，公网数据是通过阿里云内部NAT的方式流转到ECS的eth0网卡上的）。

经典网络客户和客户之间的数据通过安全组三层隔离。如果需要互通，安全组配置互访规则即可。

VPC和VPC之间默认二层隔离（不管是同阿里云账号下的不同VPC还是不同阿里云账号下的VPC），如果需要互通则只能走高速通道（专线）。
从上述对比中能看出来，经典网络在内网隔离的安全性方面不太好。

通过VPN（拨号VPN，如openVPN; Ipsec VPN，如StrongSwanVPN）或者专线，我们能将云端网络和线下办公网络或者数据中心实现内网互通，这是VPC企业级最佳实践方案。

经典网络：方便快捷，不需要设置VPC、vSwitch、网络规划等。它是随机内网，开通即用。一般适合部署个人应用、个人站点等。

VPC网络：企业默认的网络架构选择，网段划分、网络隔离及相关网络功能都是企业级网络所必需的。

4.1.2 策略二：入网请求选型的四种方法
在云端对ECS实现入网请求的功能，可以通过以下4种方法实现。

❑ DNAT：通过端口映射能直接将公网的请求流量映射到ECS的内网端口上。值得注意的是，在云端我们可以通过Iptables或者NAT网关实现DNAT的端口映射。

1）负载均衡类：DNAT端口映射的基本原理就是负载均衡，即DNAT和四层负载均衡的本质是一样的，在内核层修改访问者的目标IP和目标端口。负载均衡类，一般都是多台机器同时对外提供服务

其次，在很多场景中，给服务器开公网带宽只是为了让服务器能够远程SSH连接而已。我见到过这样的场景，一个客户有二三十台服务器，为每台服务器开个公网IP，目的就是方便远程，但这样使用的合理性、安全规范性有很大隐患。如果仅仅只是远程使用，在没有使用堡垒机的情况下，可以用SLB四层转发SSH的请求到一台ECS上（或者直接绑定EIP，这台机器类似跳板机，跟堡垒机的原理差不多），然后将这台机器与后端服务器的“SSH key”打通，这样登录目标服务器就都是一键式，如图4-2所

对于服务器需要去公网请求第三方服务及接口的需求，采用NAT网关的SNAT功能即可，根本不需要单独给ECS绑定公网。
最后，给ECS绑定公网存在很大安全隐患，这等同于给黑客开了一道能触达ECS的门，方便通过端口扫描、漏洞嗅探实现入侵。

直接绑定公网一般适用于在服务器上部署了FTP等功能，由于服务协议的关系等，没办法直接通过SLB负载均衡的方式把服务暴露给公网。

实践中，不建议用户自己搭建SNAT，自己搭建的SNAT容易存在单点故障。比如有个客户有一百多台机器部署的服务，有很多服务器都需要公网请求第三方接口。有一次重启SNAT那台机器后，Iptables的防火墙规则没有启动，导致线下业务大面积受损。

所以在云端，应优先使用对应云产品（NAT网关）来解决我们对SNAT的需求，以有效避免单点故障等方面的问题。

比如，5Mbps的带宽峰值是指出口带宽峰值，并不是入口带宽峰值，相反，入口流量峰值是不受限制的

所以，有些客户担心DDoS攻击会产生额外的流量费用。其实这无须担心，DDoS产生的瞬时几十GB的流量都是入口流量，是不收费的。在这种流量攻击中，唯一可能产生费用的就是CC攻击，就是类似刷网页，导致我们的出口流量增加，从而产生额外的流量费用。

### 4.2 云端Web服务器的五点选型考虑

❑ Web1.0：本质是联合，以静态、单向阅读为主，如早期的新浪、搜狐门户网站。内容一般由站长更新维护，缺乏用户分享互动。而这时候的Web服务一般偏向静态内容的展示

❑ Web2.0：本质就是互动，它让网民更多地参与信息产品的创造、传播和分享，如天涯论坛、微博都是Web2.0典型的应用。而这时候的Web服务一般偏向复杂的动态内容的应用（如PHP、JSP），不仅仅是单纯的静态内容了。

Nginx官网表示，Nginx保持10000个没有活动的连接，而这些连接只占用2.5MB内存。而在3万并发连接下，开启10个Nginx线程消耗的内存不到200MB。因此，类似DOS这样的攻击对Nginx来说基本上是没有任何作用的

在以前的版本中，Nginx只支持七层的负载均衡。后来从1.9.0版本开始，Nginx也支持四层的负载均衡。这意味着，Nginx能对MySQL、Redis等服务做负载均衡了。这一功能的出现使之前优秀的七层/四层负载均衡HAProxy受到极大挑战。因为Nginx不仅能作为七层/四层方面的负载均衡，在Web服务器、静态缓存、丰富的第三方插件等其他方面的功能也很优秀，这让用户选择HAProxy的可能性降低。


Nginx源代码采用C语言开发，这将使我们的二次开发变得非常复杂。为了方便开发人员，OpenResty整合了Nginx和Lua的框架，它帮我们实现用Lua的规范开发实现各种业务，并且厘清各个模块的编译顺序。而同时OpenResty在开源WAF上使用得非常广泛，中小型应用出于成本考虑，觉得使用云端WAF产品偏贵，用OpenResty也是一个不错的方案（

通过OpenResty在七层WAF中的运用可知，Nginx+Lua主要实现七层复杂逻辑控制，主要针对七层HTTP头、IP访问判断、SQL注入判断等。

云诀窍
OpenResty是云端开源WAF的最佳实践。

### 4.3 云端负载均衡选型的五个方面

负载均衡既是分布式架构的基础也是其核心，负载均衡的使用可以实现会话同步，消除服务器单独故障；也可以进行请求流量分流，提升冗余，保证服务器的稳定性。

在开源的软件负载均衡中，应用最为广泛的为LVS、Nginx、HAProxy，甚至阿里云的SLB也是基于LVS及Nginx的。

云端实践中不支持在ECS中部署使用LVS。因为LVS基于虚拟VIP，阿里云当前底层限制，经典网络和VPC专有云网络都不支持虚拟VIP的功能。

❑ 应用范围比较广，因为LVS工作在二层/三层/四层最底层，所以它几乎可以对所有应用做负载均衡，包括HTTP、数据库、在线聊天室等，并且LVS的3种工作模式、10种调度算法使其在负载均衡端有更灵活的策略选择。

❑ 云端ECS不支持LVS的部署，所以对二层/三层/四层负载均衡需求，只能使用云产品SLB的四层负载均衡功能替代，或者自行部署Nginx/HAProxy。


4）Nginx现在作为Web反向加速静态缓存越来越成熟，Nginx也可作为静态网页和图片服务器。其速度比传统的Squid服务器更快，基本上CDN在底层静态缓存服务器的选择，如今Nginx是一个成熟的解决方案。

Nginx的缺点如下。
1）对后端服务器的健康检查，只支持通过端口来监测，不支持通过URL来监测。这就会导致，在七层的健康监测中，很多情况下端口是正常的，但应用URL访问异常，Nginx却不能识别，反而主动剔除这个有问题的服务器，最终导致客户端继续访问有异常的服务器。

ip_hash在实践中虽然解决了会话保持的问题。其实并不能在七层中做到很好的流量均衡，比如我们驻云上海办公网络（有200人办公）去请求Nginx，如果Nginx配置了ip_hash参数，这200人的请求都会被Nginx转发到后端的某一台服务器上，并不能很好地进行流量负载均衡。Nginx可以通过第三方模块向客户端植入Cookie达到会话保持的目的，它解决了ip_hash的缺陷，但是这种方式需要重新编译Nginx增加nginx-sticky-module模块。

1）HAProxy是一款专注在七层/四层的软负载均衡软件，但相比于Nginx少了相应的Web服务器、静态缓存、丰富的第三方插件等功能。

同时在健康检查中，支持通过获取指定的URL来检测后端服务器的状态。以下是对应配置参数的说明。

但其实本质上，如今Nginx在四层负载均衡上的性能和稳定性已不比HAProxy差。


❑ 四层采用开源软件LVS（Linux Virtual Server）+Keepalived的方式实现负载均衡，并根据云计算需求对其进行了个性化定制。
❑ 七层采用Tengine实现负载均衡。Tengine是由淘宝网发起的Web服务器项目，在Nginx的基础上，针对有大访问量的网站需求添加了很多高级功能和特性。

基于OSI七层模型的底层原理，我们把常见的负载均衡具体划分为以下几个类型：二层、三层、四层、七层。最后再加上一个除OSI七层模型以外的DNS（如表4-2所示），通过DNS做负载均衡的场景也是非常常见的。

当前二层、三层负载均衡的实现只有LVS能做到。

LVS负载均衡技术的实现，主要由IPVS和Ipvsadm实现。
❑ IPVS：是LVS集群系统的核心部分，是基于Linux Netf ilter框架实现的一个内核模块，主要工作于内核空间的INPUT链上。其钩子函数分别HOOK在LOCAL_IN和FORWARD两个HOOK点，如图4-5所示。

需要特别注意的是，IPVS是直接作用在内核空间进行数据包的修改及转发的。而Nginx/HAProxy作用在用户空间，这使得LVS的性能更为强悍（能够赶上硬件负载均衡的性能），而Nginx/HAProxy根本不是一个量级。并且现在IPVS已经是Linux内核标准的一部分，在早期的Linux系统版本中，安装LVS还需要重新编译内核，当然现在已经不需要了。

❑ Ipvsadm：而Ipvsadm是工作在用户空间，主要用于用户定义和管理集群服务的工具。所以实际在安装配置LVS时，主要是安装配置Ipvsadm。

IPVS虚拟出一个对外提供访问的IP地址，用户必须通过这个虚拟的VIP地址访问服务器。由于LVS是采用VIP（三层IP地址）作为请求入口的，这也是很多人喜欢把LVS统称为IP负载均衡的原因，即也是三层负载均衡。但LVS有DR/IP TUN/NAT 3种模式，每种模式的核心原理分别作用在二层/三层/四层，所以把LVS称为二层/三层/四层负载均衡更为恰当些。

2）PREROUTING链在接收到用户请求后，会判断目标IP，确定是本机IP，将数据包发往INPUT链。

如果用户请求的就是定义的集群服务，那么此时IPVS会强行修改数据包，并将新的数据包发往POSTROUTING链。

2）负载均衡会将客户端请求数据包报文的源MAC地址改为自己DIP的MAC地址，目标MAC改为了RIP的MAC地址，并将此包发送给后端服务器。这里要求所有后端服务器和负载均衡所在服务器只能在一个VLAN（局域网）里面，即不能跨VLAN。

3）后端服务器发现请求数据包报文中的目的MAC是自己，会将数据包报文接收下来。由于数据包的MAC地址被修改，因此后端服务器需要在lo网口绑定VIP，这样数据包才会有“归属感”。处理完请求报文后，将响应报文通过lo接口送给eth0网卡直接发送给客户端。由于数据包由后端服务器直接返回给客户端，因此也会要求后端服务器必须绑定公网IP。

2）负载均衡将客户端请求数据包报文首部再封装一层IP报文，将源地址改为DIP，目标地址改为RIP，并将此数据包发送给后端服务器。与二层负载均衡不同的是，包通信通过TUNNEL模式实现，因此不管是内网还是外网都能通信，所以不需要LVS VIP与后端服务器在同一个网段内，即能跨VLAN。但三层负载均衡原理导致在后端服务器中不能直接获取客户端的源IP地址

同样，根据LVS的NAT模式修改数据包报文的原理，在后端服务器中能直接获取客户端的源IP地址，netstat -n能直接查看客户端请求通信的源IP。

每台内部的后端服务器的网关地址必须是负载均衡所在服务器的内网地址，即要配置SNAT，这样数据包才能经过LVS返回给客户端。

我们可以看到，目标服务器处理完数据包返回给客户端的时候，会经过LVS，然后再把数据包回给客户端。由此可以看到，LVS NAT模式存在很大的性能瓶颈（就是在于LVS这一端），而相比于DR及IP-TUN模式，数据包后端服务器直接返回给客户端就不存在这个问题。

LVS的NAT模式对外是以虚拟VIP作为请求入口（IP层为三层），然后在三层负载均衡的基础之上用IP+PORT接收请求，再转发到对应后端服务器，所以LVS的NAT模式是三层负载均衡+四层负载均衡。而Nginx/HAProxy的四层对外直接暴露的是DIP+TCP/UDP IP端口服务。

但负载均衡和后端服务器是建立新的TCP连接，所以后端服务器和负载均衡所在服务器可以进行跨VLAN（局域网）通信。

Nginx/HAProxy下的四层负载均衡无须将每台内部的后端服务器的网关地址设为负载均衡所在服务器的内网地址，即无须配置SNAT。因为是负载均衡和后端服务器建立了新的TCP连接，不必担心数据包不会返回给负载均衡。

而七层和四层负载均衡最主要的区别是，在七层负载均衡中能获取用户请求的HTTP头信息

而七层负载均衡的原理就是根据HTTP请求头来做判断转发，在七层负载均衡中应用最为广泛的当数虚拟主机功能。其实虚拟主机功能的核心就是获取HTTP请求头中的HOST字段来对应匹配转发。

❑ 可以根据请求的域名来做转发。比如请求者访问A域名，转发到后端A服务器；请求访问B域名，转发到后端B服务器。这个功能，在七层叫虚拟主机功能，是七层应用中最为热门的应用实践。

❑ 可以根据请求的URL来做转发。比如请求者访问的URL包含A目录，就转发到A服务器；请求访问的URL包含B目录，就转发到B服务器。

而四层只能获取访问的目标/源IP地址和端口。所以四层的负载均衡，单纯地是将请求轮询转发到后端目标服务器。并不能跟七层一样，做相应的逻辑判断，然后最终再转发给符合要求的后端目标服务器。

因为LVS在四层和二层，没办法识别封装在七层中的数据包内容。

实现的核心配置就是获取七层头信息里面的数据来做对比判断，如果符合，执行对应的操作即可。这个是四层/二层负载均衡无法实现的功能。

LVS能够做到“一次连接”的本质原因是LVS工作在内核空间。LVS 3种模式都是工作在内核空间，数据包的处理也仅在内核空间，这也是LVS轻量高效、高性能的最为本质的原因，如图4-13所示。

而Nginx/HAProxy七层的二次连接，在客户端和负载均衡进行TCP三次握手后，还需要等客户端Pushdata传输数据，之后负载均衡和后端服务器才会建立新的TCP三次握手。由此可见，Nginx/HAProxy四层的二次连接转发效率会更高。加上Nginx/HAProxy七层会进行一些Rewrite规则的判断，会损耗一些CPU和内存的性能。所以相较而言，Nginx/HAProxy四层的性能要高许多。

所以在后端服务器中，HTTP头的remote_addr虽然代表客户端的IP，但实际值是负载均衡的IP。为了避免这个情况，七层负载均衡通常会增加一个叫作x_forwarded_for的头信息，把连接它的客户端IP（上网机器IP）加到这个头信息里，这样就能保障后端服务器可以获取到客户端真实的IP。

但我们发现，设置DNS的负载均衡，落到不同源IP（也可以是负载均衡的VIP地址）的请求流量往往分布得很不均匀。有可能是某个后端地址的请求量很大，而另一个后端地址的请求量却很小。
2）由于客户端往往有DNS相应缓存，如若DNS解析的某个源IP服务异常，一般它不会主动剔除这个有异常的源IP解析。这可能会导致部分客户的解析访问还是这个有异常的服务地址。

但是我们唯一不能把控的就是客户端的DNS缓存，大部分客户端的电脑DNS都有缓存。有可能是DNS已经解析到最新的IP，但这时候客户端的DNS缓存还是会获取解析到的旧的IP，这会导致这个客户端可能一段时间内一直解析访问到那个有异常服务的IP。

之所以把Nginx/HAProxy七层/四层也称为反向代理，是因为Nginx/HAProxy的七层/四层应用有如下特性。
1）在转发客户端请求的过程中建立了“两次连接”。在接受了客户端请求后会将请求重新封装，以类似“代理”的角色重新将其转发给后端。

在深圳联通机房的一台服务器上部署了Nginx作为反向代理服务器。广东地域相关联通客户访问这台深圳代理服务器的速度肯定能得到保障，同时深圳联通这台机器网络由于直接在主干线路上，所以访问上海电信机房的速度也能一定程度上得到了保障。

此外，需要在代理服务器上，在HOST中配置强制将qiaobangzhu.cn的IP解析至上海电信机房的源IP。这样在深圳的客户访问Nginx反向代理服务器时，能及时将请求转发至最终目标服务器。

之所以把LVS称为负载均衡而不是反向代理，是因为LVS的以下特性：
❑ 在转发客户端请求的过程中，只建立了“一次连接”，对客户端请求数据包只做纯转发。
❑ 与后端服务器只能在一个VLAN中，不能跨VLAN。
由以上两点可以看到，LVS单纯只做分流转发，只具备负载均衡的功能。

正向代理与SNAT：可以通过一台有公网的代理机将几台内网机器对公网的请求转发出来，实现内网机器对公网的访问。这是正向代理和SNAT的核心功能，只不过正向代理主要是七层的HTTP代理。即一般我们要在客户端的浏览器中主动设置代理IP

所以可以看到，SNAT在企业级应用中非常普遍，基本上在公司内网网络（有个独立IP）都是通过SNAT方式共享上网的。

而在反向代理中，Proxy和Server同属于一个LAN，对Client透明，不需要客户端进行配置。实际上，对比正向代理和反向代理中的Proxy，其所做的事情都是一样的，都是代为转发请求和响应。

无可厚非的是，硬件负载均衡在抗并发能力及稳定性方面是最强的。虽然它的性能是最好的但同时也是最贵的，售价几万元到几十万元不等。
LVS工作在内核空间，用于处理客户端请求仅“一次连接”，单纯进行网络数据包流量转发，对CPU和内存消耗极低，基本上抗并发的能力取决于具体的网络带宽，其性能基本上跟硬件负载均衡媲美。

而IIS、Apache、Tomcat服务器都是Web服务器，底层基于Select网络模型，擅长进行逻辑处理，适合处理Web类请求应用，不适合高并发场景，基本上极限并发能力在千这个量级别。而Nginx/HAProxy底层基于Epoll网络模型，擅长进行网络流量转发及I/O处理，所以适合高并发场景。

事实上，在80%!的(MISSING)企业级HTTP负载均衡应用中，只有单纯转发的功能，没有对虚拟主机的转发需求。所以这里优先选择的不是七层，而是四层（相关内容将在第7章中进行详细介绍）。因为四层在性能方面更加强悍，在应用入口采用四层负载均衡进行分流是标准且成熟的企业级架构。

如果需要前端SLB挂证书SSL，也就是HTTPS，那么只能选择七层负载均衡。但这种架构在电商高流量高并发的场景下也会出现力不从心的性能问题，还是建议前端采用四层负载均衡，证书放在后端ECS中的Nginx进行配置。但是在常规的中小型Web应用中，这方面的性能等是完全没问题的。

### 4.4 云端存储的四种类型

存储的数据结构类型也决定了我们选择什么样类型的存储类产品。数据的结构类型一般有以下3种。
❑ 结构化数据：类似包含预定义的数据类型、格式和结构的数据，常见的如关系型数据库中的数据表里的数据。

❑ 非结构化数据：顾名思义，没有固定结构的数据。通常为不同类型的文件，比如文本文档、图片、视频、日志文件、代码文件等。


❑ 数据库类云产品（主要为云RDS、云MongoDB、云Redis、云Memcache）主要用于解决结构化数据及半结构化数据的持久化存储的问题。
❑ 块存储（云盘）、共享块存储（共享云盘）、共享文件存储、OSS对象存储主要用于解决非结构化数据的持久化存储的问题。

由于本地磁盘其实就是物理磁盘，没有云平台的高可靠性的特点，某种程度上来说，这款产品的推出就是在走传统VPS虚拟机的套路。

随着KVM的推出，当前阿里云基于KVM自主研发的飞天系统已经支持两万台物理机进行集群。在磁盘I/O上，推出高效云盘、SSD云盘，彻底解决了之前云盘I/O低的问题，并且成为当前云盘默认的主流标配产品。

ECS共享块存储是一种支持多台ECS实例并发读写访问的数据块级存储设备，即常规云盘只支持同时挂载在一台ECS上，但共享块存储支持同时挂载在多台ECS上。

（1）磁盘空间分配冲突
如果一块共享块存储挂载到多个实例上，当实例A写文件时，会查询文件系统和可用的磁盘空间，文件写入后会修改实例A上的空间分配记录，但不会修改其他实例的记录。因此，当实例B写入文件时，可能会对实例A已经分配出去的磁盘空间进行再次分配，造成磁盘空间分配冲突。

2）数据文件不一致
当实例A读取数据并将其记录在缓存中时，实例A上另一个进程来访问同样的数据就会直接从缓存中进行读取。但如果此时实例B修改了该数据，而实例A并不知道，其依旧会从缓存中读取数据，从而造成业务数据不一致。
正确使用共享块存储的方式是采用集群文件系统进行块设备的统一管理，如GFS、GPFS等。典型的Oracle RAC业务场景中推荐采用ASM统一管理存储卷和文件系统。

云文件存储NAS（Network Attached Storage）产品，都是共享文件存储，也叫共享文件系统存储。

相比于共享块存储，共享文件存储是基于网络+文件系统级别的共享。而共享块存储是基于网络+块存储级别的共享，

在云端使用共享文件存储（NFS/SMB/NAS）的典型应用场景如下：
❑ 负载均衡中的典型场景：使用负载均衡+多台ECS（如Web服务器）部署的业务。多台ECS需要访问同一个存储空间，以便多台ECS共享数据。

❑ 日志共享场景：多台ECS应用，需要将日志写到同一个存储空间，以方便做集中的日志数据处理与分析。

但云端NAS产品的出现彻底解决了传统自建NFS服务的这个问题。虽然产品刚刚出来，NAS的I/O性能确实不尽如人意。但其具有的特性，免去NFS服务的自建维护，我们相信，随着后续产品的优化，NAS的I/O性能也会逐步得到改善，这使得我们在云端共享文件存储中将其作为默认的选择。

与块存储和文件存储管理数据的方式不同，对象存储是以对象的形式管理数据的。

相比于块存储和文件存储，使用对象存储（分布式文件系统）访问数据有很大不同。它可以通过RESTful API接口对对象进行访问操作，并且可以用HTTP动词（GET、POST、PUT、DELETE等）描述操作。

❑ 对象存储主要应对的是海量数据存储，不必担心存储容量空间的问题，并且能应对高并发的场景。而块存储和文件存储都存在容量空间扩容的问题，因此在高并发场景下性能较差，所以可以看到对象存储偏向大型应用。

❑ 网页或者移动应用的静态和动态资源分离：利用BGP带宽，OSS可以实现超低延时的数据直接下载。OSS也可以配合阿里云CDN加速服务，为图片、音视频、移动应用的更新分发提供最佳体验。

❑ 云端数据备份：可以将ECS、RDS的备份或者线下IDC的数据很方便地同城或异地备份至OSS中。

### 4.5 云端缓存的两大选型秘籍

可以想见，直接去缓存中获取数据，省去了对后端数据源进行复杂逻辑处理后再获得数据的步骤，其性能得到了很大提升。但弊端是，缓存中的数据都有时效性，

这里建议参考一下“I/O 5分钟法则”，我觉得其在缓存领域也非常适用：“即如果一条记录频繁被访问，就应该考虑放到缓存里。否则的话，客户端就按需要直接去访问数据源，而这个临界点就是5分钟。”

❑ 动态页面的缓存。例如，对.do、.jsp、.asp/.aspx、.PHP、.js(nodejs)等动态页面进行缓存。
❑ 对数据库频繁访问查询的热点数据内容进行缓存。

静态缓存主要解决了网络带宽压力、服务器I/O压力等性能问题，而应用动态缓存的技术主要是为了减少对数据库的压力。动态缓存是典型的牺牲数据的实时性换取性能的技术。如果业务对数据的实时性要求不太高，且业务访问量又很大，那么这时候特别适用采用动态缓存

在动态缓存应用中，最为重要的就是缓存时间。缓存时间不宜设置过长，否则这段时间内，客户端访问的都是缓存数据；也不宜设置过短，否则就不能有效抵挡压力。

1．动态缓存常用技术
动态缓存的实现技术普遍采用Memcache、Redis。它们都是一款基于内存的Key/Value数据库。

❑ Redis不仅支持简单的k/v类型的数据，同时还提供LIST、SET、HASH等数据结构的存储。

云数据库Memcache版、云数据库Redis版基本上解决了高可用、持久化、性能等方面的问题。即我们无须担心Redis、Memcache搭建、高可用、后期扩展等问题。

对数据结构和处理有高级要求的应用选择Redis，其他简单的Key/Value存储选择Memcache。

### 4.6 云端数据库选型的三个方面

指出对于一个分布式系统来说，其不可能同时满足以下3点。
❑ Consistency（一致性）：所有节点在同一时间具有相同的数据。
❑ Availability（可用性）：保证每个请求不管成功或者失败都得到响应。
❑ Partition tolerance（分区容错性）：系统中任意信息的丢失或失败不会影响系统的继续运作。

PostgreSQL的主要优点是，基于坚实的RDBMS实现（实现了CAP定理中的Consistency（一致性）和Availability（可用性）），因其稳定性和功能集而倍受青睐。特别是对JSON数据类型和运算符的支持，以及在新版本中提高了分布式数据库的性能和支持，让其在数据库领域格外耀眼。

Memcache被排在了第24名。这主要是因为Memcache的功能太单一，只提供简单的K/V类型的数据结构，不支持数据库持久化、不支持主从及Sharding集群分配功能。

传统关系型数据库已满足不了云计算/大数据/人工智能对海量数据存储及高并发读写的性能需求，而这些也是非关系型数据库（NoSQL）的优势。

在QPS方面，关系型数据库的极限在万级别，而非关系型数据库，根据集群规模不同，能轻松应对10万级别以上的高并发读写需求。

❑ 需要考虑一些热点数据查询需不需要借助Key-Value内存数据库做缓存，从架构上减少数据库压力，提升业务系统的性能效率及稳定性。
❑ 在代码连接池方面，设计合理的连接池，连接数不宜过大或过小，以免导致后端数据库连接数不释放等性能问题。
❑ 在一些对数据库高并发的场景加入队列的控制，也是有效减轻对数据库直接压力冲击的一种方式。

❑ 选择什么配置的服务器（数据库对服务器性能的需求是IO >内存> CPU

❑ 80%!的(MISSING)数据库性能问题都出在SQL语句上。
❑ 80%!的(MISSING)SQL语句性能问题都是由索引引起的。

❑ 备份采用冷备还是热备？采用物理备份（数据库底层磁盘文件备份）还是逻辑备份（数据SQL语句DUMP导出）？相应的备份方案也与业务情况及状态相关。

❑ 云数据库的运维架构层考量及规划：云数据库底层已经通过DNS+LB（负载均衡）+HA（高可用）的技术保障数据库高可用，增加主从节点、增加Sharding都可以在控制台界面一键式操作。

❑ 云数据库的SQL语句优化：云数据库在控制台直接给出慢查询的明细及优化建议。
❑ 云数据库的日常备份及恢复：云数据库自带热备及冷备，只需要手动在控制台设置对应的备份策略即可。

### 5.2 业务访问量与性能压力指标的转换

得到业务访问量的指标数据后，我们要将其转换成为系统压力的指标数据。这一步让抽象的业务访问量的指标数据，变成技术人员熟悉的性能压力指标数据，是容量配置规划的关键一步。

最终转换成对系统的每秒请求数（某一秒内同时向服务器发送的请求数量，以下简称每秒请求数）这个指标，

在实践中可以发现，一天中的80%!业(MISSING)务请求量主要发生在40%!的(MISSING)时间内，这成为我们计算PV值对应请求压力的重要依据。24小时的40%!是(MISSING)9.6小时，即80%!的(MISSING)请求发生一天的9.6个小时当中。基本与绝大多数的业务场景吻合，业务请求主要集中在白天，晚上则相对较少。

即服务器一秒能处理23.1个请求，就可以每天承受100万PV的业务量。服务器一秒能处理115.7个请求，就可以每天承受500万PV的业务量。

### 5.3 云端服务器配置模型

据监控数据显示，互联网企业的服务器CPU利用率平均在10%!～(MISSING)20%!，(MISSING)磁盘空间的利用率在20%!～(MISSING)30%!。(MISSING)在云端，有80%!企(MISSING)业存在计算资源和存储资源闲置浪费的现象。

1∶2的处理器与内存配比可以获得最优计算资源性价比，不管是线下IDC的物理服务器，还是云端ECS服务器的配置，1∶2均为黄金比例。

❑ Nginx是轻量级的高性能中间件，Nginx基于Epoll网络I/O模型，在高并发场景下对性能消耗很低。如果在服务器上仅部署Nginx做转发，2核4G/4核8G的配置足够了，无须8核16G的高配。如果Nginx+PHP做FastCGI，更多性能的消耗在于PHP，所以一般用4核8GB/8核16GB的经典配置。

3．CPU与内存资源配比：1∶4
1∶4的配比，比如，2核8GB、4核16GB、8核32GB。这类配比的配置偏向内存，特别适合部署数据库类的应用。

数据库对服务器性能的需求首先是I/O，因为数据库是个存储类应用，涉及数据持久化，所以对I/O性能的要求是最高的。其次才是内存，因为高内存会有效提升数据库的缓存性能，很大程度上提升数据库的性能。

4．CPU与内存资源配比：1∶8
处理器与内存资源配比为1∶8，比如，2核16GB、4核32GB、8核64GB，这类是高内存资源占比。尤其适用于数据库类中的内存型应用，比如，Redis、Memcache的部署。

Redis是单进程单线程模式，对多核利用得不太好，所以适合偏向用1∶4/1∶8的2核8GB/2核16GB高内存资源占比的配置。

### 5.4 云端带宽配置选型

在云端带宽配置的选择中：
❑ 80%!的(MISSING)应用默认选择按量带宽，即按量带宽是云端带宽类型选择的最佳实践。
❑ 20%!的(MISSING)应用选择固定带宽。这个特定的条件就是，如若每天按量下载的量合计费用超过带宽平均每天费用，则使用固定带宽

### 第6章 云主机实践

但是Kubernetes技术的日益成熟以及普遍应用，让我们可以从中看到未来代码运行技术的趋势，对用户而言，肯定会是无服务器、无操作系统的全新概念。

### 6.1 云网络下的业务新架构

在如红警、魔兽等游戏流行的年代，其实也是IDC物理硬件盛行的年代。那时候打游戏“进入房间”时能明显地感觉到，电信专区、网通专区、移动专区的速度是不一样的

这是IDC网络架构的通病，即中国基础网络的运营商、用户都具有区域性，联通用户多分布在北方，而电信用户多分布在华东、华南，这就导致了经典的南北互通问题，即南方电信与北方联通线路之间的互访很慢

云平台使用BGP线路，具有较高的网络质量及速度，这是非常具有吸引力的实用特性。

### 6.2 云的技术本质优势

而云计算下的技术架构的特点是群体作战能力势如破竹，如群狼，最终猛虎也难以招架住群狼。而“群狼战术”的本质就是如今分布式架构的特点，云计算的普及，也推进了分布式架构的发展。

图6-3 共享型实例底层物理资源分配
共享型实例系统采用的是随机的更贪婪的调度CPU的模式，一个核的使用可能会调度分配给不同的云主机实例。所以如若一个实例占用导致CPU飙高，可能也会导致其他实例出现CPU负载飙高且性能不稳定，其实这类性能异常在实践中我们也经常遇到。

相比于共享型实例，独享型实例用系统固定调度CPU模式，一个核的使用会固定调度给某云主机实例，不会因为其他用户的资源使用繁忙或空闲而产生波动，独享型实例如图6-4所示。

但独享型实例系统的本质还是共享资源，只不过是在细分的核数上看似为独享。独享型是在底层硬件计算资源上，将计算资源（核数、磁盘、内存、网络）划分给一个公共的“大池子”（云计算平台），用户按需索取相应的计算资源即可，多个用户共同使用这个大的计算资源池。

云的发展，其实也是分布式架构的普及以及发展。技术架构的发展，已经从单机架构到分布式架构，再到如今的微服务架构，所以如若业务对单机性能要求很高，只能进行垂直扩展（即升级服务器配置）来解决性能问题，而不能进行水平扩展（通过简单加服务器），显然这样的业务不能真正发挥云平台的优势，甚至不适合云平台。

分布式架构，去除对底层高配性能硬件的依赖，在架构上考虑采用海量低配硬件资源来满足庞大的业务量需求。

### 6.3 云时代下的资源自动化管理

比如在红米手机的秒杀活动中，会瞬间开启200台机器且持续两小时来应对，然而IT资源才消费了600元人民币：

不过对于此类功能在云端已经做成成熟的产品了，如ESS弹性伸缩，它能动态调整弹性计算资源，实现真正意义上的无须人工干预、资源动态伸缩。

### 第7章 云端负载均衡实践

集群架构主要是热备的高可用架构，它通常采用虚拟VIP技术（如Keeplived、Hearbeat）来解决单点故障的问题，让架构高可用。集群的虚拟VIP技术只能让一台服务器平时作为Backup热备，只有在出现故障的时候才会切换到Backup上，平时Backup热备都处于空闲状态。分布式架构的技术特点就是引入了负载均衡，让不同服务器来同时处理业务压力。负载均衡是分布式架构的起点，高可用架构底层最需要及最依赖的就是负载均衡。

### 7.2 单机+SLB架构的必要性

单机版的时候，业务的域名IP直接绑定在ECS的公网IP上。如若后期业务量增加，没办法采用一台服务器来应对业务量，那么就得引入SLB，或者在ECS上自建Nginx等负载均衡。这时候域名要重新解析到新的IP地址上，因此需要中断业务。相比较而言，单机+SLB的方式，虽然SLB后面只有一台机器，但业务域名解析的IP地址用的是SLB的IP地址。如果后期业务量增加，只需要在SLB中加入新的服务器即可，而且对用户请求端来讲，这完全是无感知的，也不需要中断业务。

### 7.3 被LBHA误导的架构

客户最后提出的需求：Session文件不要放在磁盘上，而是放在内存文件系统中（tmpfs），以保障业务访问速度。

大家都知道负载均衡（LB）和高可用（HA）的概念。但是什么时候用分布式架构，什么时候用高可用架构，其实需要根据业务情况来定。业务没有到达金融证券、淘宝这样的量级，对业务中断时间的担忧是过分的敏感和担心。

所以对简单业务而言，没必要花费额外成本在高可用、负载均衡方面，还给后期管理维护平添麻烦。相反，如果业务规模、业务重要性要求能在秒级内恢复等，就需要额外的成本投入在高可用的架构保障中，而不要单纯为了分布式而做分布式，为了架构而做架构。

### 7.4 DNS的两大主流实践

DNS的A解析记录IP地址配置为SLB的IP地址，这样暴露给DNS的解析IP是集群IP地址，负载均衡后面往往挂着多台服务器。相比DNS直接把IP解析到ECS上，高可用的问题就解决了。但是在这里我们发现负载不均衡的问题还是未能解决。即SLB1上、SLB2上流入的流量不均衡，可能导致一个流量偏高，一个流量偏低，最终后面的ECS计算资源并不能得到有效利用。

图7-4中，ECS分别挂载到了不同的SLB上，即使前端两个SLB的流入流量不同，但最终请求流量都是转到后端ECS服务器进行处理。所以采用DNS引起的负载不均衡的问题就被解决了。

现在变成两个SLB，增加了维护管理成本，将原本简单的事情复杂化，这显然是不合理的。所以中小型架构并不适合以DNS作为负载均衡。DNS的负载均衡架构，主要适合大规模应用。当后端有一两百台ECS，而一台SLB的性能又有限时，此时采用多个SLB, DNS调用的优势和必要性就体现出来了。

DNS的智能解析功能是跨地域的分布式架构中不可缺少的核心功能。在CDN或跨国际应用中，目前应该没有能替代它的解析方案。

域名智能解析是指域名解析服务器根据来访者的IP类型，对同一域名相应地做出不同的解析。智能解析的原理是，有一个全世界现有的IP地址库，每个地址对应哪个地域下的运营商线路都会记录在这个IP地址库中。

不过在云端，如今DNS在运营商线路智能解析这方面基本上派不上用场。主要是因为云端网络都是BGP线路，一个IP对应电信、网通、移动、教育网等多家运营商线路。所以云解析变成这样填写记录：

智能解析的最佳实践当属CDN。CDN虽然本质上是提供静态缓存加速，但静态访问加速的“就近访问”的核心，就是智能解析的核心功能

5）阿里云CDN的DNS调度系统根据终端用户的源IP判断出该终端属于北京地域，并为该终端用户请求分配北京缓存节点IP, LDNS获取DNS返回的解析IP地址1.1.1.1。

CDN所谓“就近访问”的核心在于DNS的智能解析能分辨不同终端用户源IP的地域，然后再返回该地域的CDN缓存节点IP地址，从而实现“就近访问”。

在云端实践中，也经常遇到跨国际用户请求的需求：❑ 应用部署在国内，海外用户访问不畅。❑ 应用部署在海外，国内用户访问不畅。用什么架构可以实现海外用户、国内用户访问通畅？下面将给出实践中总结出的两套成熟的标准方案。

方案1：多个域名+多个站点

方案2（最佳实践）：单个域名（DNS智能解析）+多个站点

通过智能解析把不同地域的请求引流至各自不同地域部署的节点中

虽然业务代码层的跨地域部署问题解决了，但是还有个问题没有解决，即数据库也可能需要进行跨地域部署。所以底层数据库跨地域进行同步时所带来的数据同步的延时问题，就成了跨地域分布式架构的第二大难题。

当前在云端可以通过高速通道进行专线打通。当前实际案例中，许多客户已经将海外部署节点和国内部署节点通过高速通道打通，从而保障网络传输速度及传输质量。然后结合DTS工具进行数据库底层的实时同步。

DTS数据传输（Data Transmission）

通过数据传输可实现不停服数据迁移、数据异地灾备、跨境数据同步、缓存更新策略等多种业务应用场景，帮助用户构建安全、可扩展、高可用的数据架构。DTS将会成为云端数据同步、跨地域数据同步的最佳实践方案。

### 7.5 企业级Web架构实践

所以反向代理是企业级Web架构的最佳实践

### 7.6 企业级负载均衡架构实践

七层的负载均衡，在中大型项目中并不适合作为业务请求的流量入口，特别是在电商高并发的场景下，七层SLB显得格外力不从心。

对于用户请求流量的入口，经过大量实践，一般采用二层/四层或者硬件负载均衡来实现，从而实现抗并发及分流。然后将请求转发给后端七层负载均衡，在七层负载均衡上做Rewrite、虚拟主机等业务功能上的控制。图7-13所示为标准的互联网企业级应用架构流程。

我们并不会采用七层Nginx、七层Haproxy作为流量入口，也不会采用Nginx四层、Haproxy四层作为流量入口，主要是因为Nginx/Haproxy的“二次连接”对性能有损耗。采用LVS或者负载均衡作为互联网架构的流量入口，可以保障入口流量最大性能及效率被负载均衡转发，这是中大型项目的标准应用。

所以中大型应用中，前端用七层负载均衡，几乎都会成为瓶颈点。所以入口流量处仅通过硬件或LVS负载均衡做流量分发。而七层虚拟主机、Rewrite等功能需求，就放在后端服务器上通过搭建Nginx/Apache来实现。

如果是中小型应用，用户请求的QPS量级远远达不到50000，这时候其实还是优先推荐使用七层的SLB作为流量入口。

使用七层SLB作为流量入口，虽然没办法应对高并发的场景。但是在功能应用、管理维护等方面要比四层SLB方便且简单许多。

采用七层SLB集中化管理证书，虽然在维护管理上提升了效率，但是在性能上损耗严重。实践发现，前端使用七层SLB进行证书集中管理，性能会损耗20%!～(MISSING)30%!。(MISSING)在高并发场景下尤为明显，我们在其他的客户实践中，专门用PTS压测得到了这个性能数据，并对其进行了验证。

如果是中大型应用，存在高并发访问场景。在云端，结合传统互联网企业标准架构所设计的符合中大型应用的云端标准负载均衡架构如图7-16所示。

图7-16 云端中大型互联网企业负载均衡架构

前端采用SLB四层作为请求流量入口，以应对高并发场景。如果需要配置https证书的话，可将证书放在后端ECS中用Nginx来配置。若还需要虚拟主机、Rewrite等功能，可一并放在后端ECS中通过Nginx来实现。

### 7.7 通过代理+VPN提速跨国际网络访问

跨国际网络访问延时，一直是网络架构中的疑难杂症。比较暴力直接的解决办法，就是通过专线（阿里云的云企业网及高速通道服务）来解决网络连通的质量问题。但是专线也仅仅只解决了内部应用服务之间调用（比如数据库跨国际地域同步）的网络问题，没办法解决用户跨国际地域访问带来的网络延时问题。通过代理+VPN提速跨国际网络访问，在云端算是经典的应用实践。

为了解决这个跨国际网络延时的问题，我们引入了代理节点。其核心原理跟前面介绍的负载均衡中为实现南北互通以Nginx反向代理作为中转的原理基本类似。

国内用户直接访问德国、美国东部节点，网络延时较严重。但若在新加坡放置一台代理服务器，那么直接访问这台代理服务器的速度一般都很快，这台代理服务器直接通过国际网络去访问德国节点、美国东部节点，其速度也很快。

同样，重点是网络稳定，不会出现丢包的情况。所以结合对比测试，加入新加坡的代理节点，网络的传输延时不仅会减少，还会使网络质量更稳定，不会出现丢包、不稳定等问题了。同样，美国东部节点、德国节点的CSOS代理要跟我国杭州节点的CSOS服务器通信时，也让请求通过新加坡代理转发至我国杭州节点上。将CSOS代理中要跟csos.cloudcare. cn通信的请求通过配置VPN强制走新加坡代理即可实现。通过以上架构优化，基本上可以避免延时抖动的情况。

代理是跨国际网络应用中不可多得的成熟解决方案，但要确保代理节点的网络通畅，否则就失去了代理节点中转的意义，即国内到国外代理节点的网络要通畅，国外代理节点到目标服务器的网络也要通畅。

### 7.8 通过反向代理提速跨国际网站访问

接着上文跨国际网络访问的延时话题，还有另外一个场景，就是国内客户跨国际访问国外部署的网站（主要为亚洲地域外的其他地域）延时很高的问题（主要受访问出口带宽限制）。

怎么解决国内用户访问国外网站慢的问题呢？同样我们可以借助新加坡的服务器通过反向代理解决

除了要依靠新加坡的服务器外，核心技术还需要在该代理服务器上部署Nginx来中转用户请求。Nginx是百年难遇的“神一般”的软件，只有你想不到的，没有它做不到的。本文主要介绍Nginx七层反向代理的技术特性，我们主要有两个方案实现，明细如下文。

我们访问sg.qiaobangzhu.cn被重定向跳转到qiaobangzhu.cn上了，本质上最终还是要直接访问qiaobangzhu.cn，这并没有解决访问网络延时的问题。

那能不能只通过一个域名结合反向代理，来解决跨国际网站访问延时的问题呢？答应是可以的，我们需要借助DNS智能解析的核心功能。

通过DNS智能解析设置后，国内用户访问qiaobangzhu.cn会自动解析到新加坡服务器的IP161.117.229.111上，而海外用户访问qiaobangzhu.cn会自动解析到美国服务器的IP47.90.209.124上。

值得注意的是，新加坡的服务器将国内用户访问qiaobangzhu.cn的请求反向代理给qiaobangzhu.cn，这时候新加坡的服务器解析qiaobangzhu.cn的IP地址得到的已经是美国服务器的IP地址47.90.209.124了。为了保险起见，我们可以在新加坡的代理服务器上的host配置文件（/etc/hosts）中将qiaobangzhu.cn的解析本地强制绑定到美国服务器的IP地址47.90.209.124上，这样就可以保障请求最终被转到美国服务器的源站上了。

### 第8章 云端存储实践

非结构化类型数据的持久化存储实践，即块存储（云盘）、共享块存储（共享云盘）、共享文件存储、OSS对象存储的云端实践。而结构化数据类型及半结构化数据类型的存储实践，即数据库类实践，将在第10章重点介绍。

### 8.1 云端块存储八大实践技巧

块存储（云盘）是非结构化数据（比如文本文档、图片、视频、日志文件、代码文件等）存储的重要手段

我们发现把一台机器拆分到不同低配机器磁盘上，其I/O性能呈线性增长。所以在云端可采用大量机器进行分布式部署，此架构是提升I/O性能最有效的做法。

在云平台中，夜间的资源竞争少了很多，这时候也是I/O性能最好的时候。所以在进行架构设计的时候，可把需要I/O、需要CPU计划的任务放在晚间进行。

技巧三：云盘的拆分及合并云盘的拆分是指将一块云盘拆分成不同云盘来购买，把一块大空间分为几块小空间购买，比如需要200GB磁盘空间，可以分两次进行购买，一次购买100GB。而且两次购买的时间间隔尽量拉长一点，这样多数情况下，这两块云盘的空间会被分配在底层不同的物理硬盘上。在两块物理硬盘上读写总比在一块物理硬盘上读写性能要好，尤其是将不同的应用部署在不同的磁盘空间上时效果更加明显。

技巧一：在云盘中不建议进行分区在云端实践中，建议直接格式化一块云盘，不建议进行分区。后续如若这块云盘要升级扩容，只需要使用resize2fs指令扩大文件系统，然后重新挂载即可。相反，如果这块云盘提前进行了分区。后续进行升级扩容时，就需要针对不同的分区来进行额外的扩容，操作烦琐且复杂。

在开ECS的时候，建议系统盘只选择40GB。系统盘的空间仅用于存储系统日志、一些系统中间件软件的主目录数据，不建议存储业务代码、业务日志、数据库数据等。

在实践中，我们经常遇到服务器重启后，虽然上面的服务都启动了，但是业务还是异常的情况，最后发现云盘、NFS或NAS在本地没有挂载，导致业务数据读取不到。

在磁盘挂载的开机自启动中，默认需要把挂载命令放在/etc/fstab中。

### 8.2 云端共享文件存储的五种方法

需要将不同服务器之间的文件进行共享或者集中管理。

Rsync是Linux系统下的数据镜像备份工具，在数据增量备份、数据迁移、数据同步等方面有着无可替代的地位。Rsync自带MD5传输校验、文件传输压缩、文件传输限流等很多实用功能。

❑ Rsync不是实时同步的，每次同步需要手动执行rsync命令，或者结合crontab进行定时执行。❑ Rsync只适合进行两台服务器之间的数据同步。当服务器较多的时候，比如有3台服务器，同步就成为环状，

Rsync只是实现了定时同步数据，但是定时任务的同步时间粒度并不能达到实时同步的要求。在Linux Kernel 2.6.13后提供了Inotify文件系统监控机制，通过Rsync+Inotify组合可以实现实时数据同步。

NFS是标准的文件共享解决方案，

1．NFS的高可用问题NFS的Server存在单点问题，如若Server出现异常，会导致客户端共享的数据都不可用。不过结合7.3节所提到的知识可知，一些中小型应用中没必要过多考虑NFS的高可用性，因为这些设计需要投入额外的成本。

NAS（Network Attached Storage，阿里云文件存储）的推出，彻底解决了NFS安装配置、维护管理、高可用等方面的运维问题。它实现了“开箱即用”，这使得ECS之间进行文件共享变得非常方便快捷。

### 8.3 OSS文件管理的六大技巧

所以阿里云OSS管理控制台只适合进行一些简单的日常文件管理，当数据量较多较大时，使用效率就会很低。

OSSFS能在Linux系统中把OSS Bucket挂载在本地文件系统中，能够像操作本地文件一样操作OSS对象，实现数据共享。

但OSSFS使用有如下缺陷：❑ 多个客户端挂载同一个OSS Bucket或重命名/文件夹时，可能会出错，导致数据不一致。❑ 不适合高并发读写场景，性能及稳定性不足。所以并不推荐在业务场景中使用OSSFS，但可以将其作为一个运维类工具，用在数据备份、数据同步等方面，方便用户日常管理OSS中的文件数据。

相比于OSSFS，建议优先使用2019年新推出的阿里云云存储网关产品。而且商业产品，也适用于生成环境，OSS存储资源会以Bucket为基础映射成本地文件夹或者磁盘。

对OSS进行管理的命令行工具，官方推荐使用OSSUTIL。OSSUTIL是一个二进制Binary的工具包，不需要依赖Python环境，直接下载就能运行，同时OSSUTIL支持在Windows、Linux、MAC下运行。

### 8.4 四招搞定OSS数据迁移

OSS在线迁移只需要在Web控制台进行简单配置即可，不需要安装配置，也不用理解每个参数的含义等。

跨区域复制是OSS自带的一个功能，通过配置跨区域复制能完成同阿里云账号下不同Bucket之间的数据迁移

### 8.5 运维容灾备份新篇章

而在云端，对象存储OSS彻底替代了传统备份服务器的角色。云端运维容灾备份架构如图8-16所示。

在云端，快照基本上能满足我们ECS的数据备份需求。不过，快照是针对磁盘级别进行备份的，若针对单个文件等更细粒度的备份，可以结合OSSFTP、OSSFS、OSSUTIL、OSS客户端、OSS网关服务直接将数据从内网备份至OSS中。

### 第9章 云端缓存实践

缓存是一种典型的以牺牲数据时效性换取访问性能的技术，是中大型架构中应用最为广泛的技术。静态缓存，主要应用在Web应用中，提供网站访问性能。而动态缓存，除了常见的数据库热点数据缓存外，还应用在Session缓存及动态页面缓存中。除了大家常见的CDN静态缓存、Memcache/Redis数据库动态缓存技术实践外

### 9.1 使用静态缓存提升网站性能的四种方法

静态缓存，一般是指Web类应用中，将HTML、JS、CSS、图片、音视频等静态文件/资源通过磁盘/内存等缓存方式来存储，从而提高资源响应方式，减少服务器压力/资源开销。

浏览器缓存，也称为客户端缓存，是静态缓存中最常见、最直接的表现形式，然而往往会被人忽略掉。

第一次访问时，响应的状态是200，当第二次及后续访问的时候，响应状态变成了304，如图9-2所示，这表示客户端已经开始获取浏览器缓存内容，不需要去服务器端获取对应的请求内容了

在Nginx中设置expires，并不是指把静态内容缓存在Nginx中，而是设置客户端浏览器缓存的时间，这是很多人理解的误区所在。

所以不难看出，Nginx把HTML内容和图片二进制全部缓存到本地磁盘上了。下次用户再次来访问test.html的时候，Nginx会直接将缓存在本地磁盘的文件返回给用户。特别是后端，如若部署的是Tomcat、IIS等，Nginx强大的静态缓存能力将有效减少服务器压力。

内存缓存，顾名思义，就是把静态文件缓存在服务器端的内存中。在这种缓存方式下，如若命中缓存的话，取内存中的缓存数据返回比取磁盘中的缓存数据返回性能要高很多。以Varnish为例

事实上，CDN不仅是云端静态缓存的最佳实践，更是动静分离的最佳实践。

在这个时候，不必过早引入OSS，除非是出于业务上的规划需求等，否则会增加应用的维护成本和复杂度，没太大必要。中型应用中，静态资源一般会和应用代码一起部署。要实现静态加速，前端只需要加个CDN, CDN会针对客户端请求，进行回源并主动缓存静态资源

在大型应用中，我们怎么使用CDN进行动静分离呢？如果想进一步提升静态资源的访问速度，可以单独针对静态资源服务器加上CDN，即将CDN的回源地址解析绑定到静态资源的域名上。在云端实践中，我们可以把静态资源单独放在OSS中，以OSS作为静态资源服务器，替代还需要单独部署Web服务器来管理静态资源的传统方式。

### 9.2 动态缓存的三种应用场景实践

在动态缓存应用中，最为常见的就是数据库缓存了。

数据库缓存的第一个技术特点就是提高性能，所以数据库缓存的数据基本上都是存储在内存中的，相比I/O读写的速度，数据访问能更快速返回。

 Session是程序中用于和客户端请求进行会话保持的一种技术。PHP会默认将Session数据存在本机磁盘文件上，而Tomcat会默认将Session数据存放至JVM分配的内存中。

所以在负载均衡的分布式应用中，当请求转到服务器2上，也能获取到服务器1上的Session数据，我们需要在服务器端采用Session共享的相应技术来集中管理，

策略一：基于源IP的会话保持
基于源IP的会话保持，是指负载均衡识别客户端请求的源IP地址，将同一IP地址的请求转发到同一台后端服务器进行处理。若将客户端的请求绑定在后端某一台服务器上，不让请求轮询到其他服务器上，那么就不会出现Session会话的问题。

再如，四层的SLB会话保持，也是基于源IP地址的会话保持

1）基于源IP的会话保持在实践中虽然解决了会话保持的问题，但流量均衡做得并不是很理想，比如驻云上海办公网络（有200人办公）去请求Nginx时，如果Nginx配置了ip_hash参数，那么这200人的请求都会被Nginx转发到后端的某一台服务器上，并不能很好地进行流量负载均衡。

这时候会让客户端重新登录系统，但用户端并不知道后端这台服务器宕机。所以会感觉莫名其妙，怎么又让我登录？

策略二：基于浏览器Cookie的会话保持
为了解决基于源IP会话保持所带来的后端流量不均衡的问题，出现了基于浏览器Cookie的会话保持，它很好地解决了该问题。哪怕再多人共用一个IP也不用担心，因为客户端是根据客户端请求的Cookie进行转发的。七层SLB的会话保持，就是基于Cookie的会话保持的

七层SLB Cookie处理主要分为以下两种。
1）植入Cookie：指客户端第一次访问时，负载均衡会在返回请求中植入Cookie，下次客户端携带此Cookie访问，负载均衡会将请求定向转发给之前记录到的后端服务器上。

策略三：数据库存放Session
虽然基于浏览器Cookie的会话保持解决了基于源IP的会话保持中转发给后端流量不均衡的问题，但是没有解决基于源IP的会话保持中，后端某一台服务器宕机了，存储在这台服务器上的Session丢失的问题。所以为了解决这个问题，我们就不能把Session存放在服务器本地的内存或者文件中。在实际应用中有一种解决办法，就是把Session存放在数据库中

虽然这种方式解决了后端服务器宕机Session丢失的问题，但是带来了更严重的性能问题，在实际应用中强烈建议不要这么使用。因为数据库往往是业务性能瓶颈点，如果将Session存储到数据库表中，频繁的数据库操作会影响业务正常访问，得不偿失。

但是一到活动期间，用户访问量增加，导致数据库操作频繁，数据库成为性能的瓶颈，从而导致业务性能严重跟不上。

何为动态页面缓存？即对.do、.jsp、.asp/.aspx、.php、.js(nodejs)等动态页面进行缓存。可以看出，动态页面一般都会涉及动态计算、数据库缓存、数据库等操作，所以每一次访问同一个页面，所获得的数据都可能有所不同。如若应用对数据及时性要求较高，则可能不太适合动态缓存。

不过，Nginx内置Proxy模板proxy_cache实现动静态缓存，缓存文件是存放在磁盘文件中的。在高并发访问的场景中，磁盘I/O可能是性能的瓶颈点。

其实对于Nginx而言，动态缓存和静态缓存的配置基本一致。唯一的区别就是，静态缓存的location配置中，正则匹配的是静态访问请求。而动态缓存的location配置中，正则匹配的是动态访问请求。

FastCGI跟proxy_cache模块一样，也是将缓存文件存放在磁盘文件中的。同样，在高并发访问的场景中，磁盘I/O也可能是性能的瓶颈点。

第三招：通过Nginx内置Memcache模块实现动态页面缓存
proxy_cache和FastCGI都是将缓存存放在磁盘文件中的，高并发场景下，磁盘I/O会成为性能瓶颈。有没有一种缓存技术能用内存缓存动态页面？答案是肯定的，那就是Nginx内置的Memcached模块。Nginx内置的Memcached模块ngx_http_memcached_module，可以很轻松地实现对Memcached的访问

这个配置会把所有请求URI后缀为php的访问，用Memcached模块来读取，同时使用请求URI作为Memcached的key。当缓存没有命中或者出错时，可使用@fallback进行处理

其实动静缓存的配置，后端采用的技术都一样，具体取决于在Nginx location中匹配动态还是静态来让其缓存。

### 10.1 垂直拆库三大应用场景实践

纵向的垂直拆分，可以针对数据库的库级别拆分，也就是说把数据分别放在不同的数据库中，然后把不同的数据库放在不同的服务器中。我们也可以针对数据库读写操作的纵向垂直拆分，这就演变成大家熟知的经典主从（读写分离）架构。在主从（读写分离）基础上加上高可用，就是数据库的集群技术应用，本质上也是数据库垂直拆分的应用。

可通过主从的读写分离架构，即将读写请求进行垂直拆分，引流到不同的数据库上来解决对数据库性能的诉求。
方案一：经典主从架构的实践
Replication主从复制，又称之为AB复制。是数据库中最为经典、应用最为广泛的架构。从库通过实时同步复制主库，保障从库中的数据和主库中的数据一致，比如MySQL的Replication主要通过binlog来同步主库数据。

所以针对数据库增删查改，我们把对数据的查找放在从库上。把数据的增删改操作即写数据放在主库上，可见读写分离是针对数据库连接操作的垂直拆分。

❑云数据库RDS:MySQL版，可以添加只读实例，用于读写分离。也可以添加灾备实例，用于数据容灾。
❑云数据库Redis版的读写分离版，也是标准的主从应用。

个从库可以进一步分担查询的压力。因为在业务中，80%!的(MISSING)数据库压力都来源于查询压力。特别是业务中后台的一些分析功能，数据库查询分析会消耗很多数据库的性能。多个从库不仅能进一步分担数据库查询的压力，数据库的热备、冷备等也都可以放在不同从库上，这样，数据的备份就可以高枕无忧了。

在高并发场景下，增加多个从库能有效解决数据库大量查询的压力，但是没办法解决数据库的写（增删改）压力，这是读写分离的瓶颈点。

另外，主从复制的机制，从库会造成短期的数据不一致。主要是因为最新写入的数据只会存储在主库中，想要在备库中读取到新数据就必须先从主库复制过来，这会带来一定的延迟，造成短期的数据不一致。

在生成环境中，为了进一步减少写的压力，有时也会采用主主主的环状读写分离，如图10-6所示。

而相比传统主从架构，MongoDB副本集（如图10-7所示），集群中的任何节点都可能成为Master节点。一旦Master节点发生故障，副本集可以自动投票，则会在其余节点中选举出一个新的Master节点。并引导剩余节点连接到新的Master节点，这个过程对于应用是透明的。目前MongoDB官方已不推荐使用主从模式，取而代之的是副本集模式。

在Java连接驱动中，Server的连接地址是一个集合，集中包含副本集所有的IP地址。而客户端连接到副本集，不关心具体哪一台机器是否挂掉。主服务器负责整个副本集的读写，副本集会通过oplog定期同步数据备份。一旦主节点挂掉，副本节点就会选举一个新的主服务器。这一切对于应用代码服务器不需要关心，我们也不需要因为主从节点挂掉，而去修改应用代码的连接配置文件，以及重启应用代码服务器。只需要通过连接副本集的参数指明是在主节点读，还是在从节点读，其他一切底层会帮忙搞定。

（2）集中配置管理
为了解决这种人工干预配置文件的低效率做法，可以采用Zookeeper、Consul、Etcd来做配置集中管理。这便是第二种方式，即配置集中化管理，如图10-9所示。

代码配置管理、集中配置管理，都是通过代码层的控制来选择对应的主从读写分离库的。我们也可以采用读写分离的中间件来实现，应用业务代码连接不用关心后端有哪些主库及从库，真正意义上对应用是透明的，

业务代码连接读写分离的中间件Proxy用来分辨是写请求还是读请求，然后再将请求转发到后端对应的主从库来处理。

引入读写分离中间件能很好地对程序端和数据库进行解耦，这样程序端只需关注数据库中间件的地址，而无须知晓底层实际上有几个数据库及如何提供服务。

❑MySQL Proxy : MySQL-Proxy是MySQL官方提供的中间件服务，是一个处于Client端和MySQL server端之间类似代理的程序，它可以监测、分析或改变它们的通信。它使用灵活，没有限制，常见的用途包括：读写分离、负载平衡、故障、查询分析，查询过滤和修改等。

❑ Consul用于心跳检测，当主库正常，让DNS解析主库的IP地址。当主库异常，让DNS解析从库的IP地址。

### 10.2 水平拆表三大应用场景实践

水平拆表意味着我们可以将同一数据表中的记录通过特定的算法进行分离，分别保存在不同的数据表中，从而可以部署在不同的数据库服务器上，来应对大数据存储及读写的场景。

与垂直拆分最大的不同是，水平拆分能把一份数据拆分成

### 11.1 上云迁移的实践

❑ 服务器端、前端采用Nginx+Varnish作为二级缓存，主要减少CDN回源访问的压力。

❑ 图片源文件等，主要通过NFS进行磁盘挂载共享，图片数据量2T+。
❑ 数据库缓存端主要采用Redis作为数据库缓存，减少数据库压力。

但是，对云产品、云架构的灵活运用，是有一定技术门槛的。关于如何利用云资源设计出低成本高性能的架构，是个经验性的技术活。

双方最终在12月底确定了合作意向。我们为客户提供上云架构方案+上云迁移+7*24监控+7*24运维服务（我方运维为主，客户运维为辅）来解决客户痛点。

挑战三：零配置文档、零规范。由这个挑战点可知，客户在某些方面是很不规范的。例如，一个经历了8年运维的系统，居然在运维配置文档、运维手册方面没有建立一份文档，仅仅有几张零碎的架构图。另外，在主机名、防火墙、配置文件规范等方面更是杂乱无章。在迁移期间还遇到过一件不可思议的事情——客户忘记了机房交换机密码，这给我们的迁移工作带来了极大的难度和挑战。

2015年1月4日，作为运维负责人的我，和2名架构师、1名DBA、1名高级运维及2名中级运维，在当天下午开车前往杭州，进行项目周期为两周的上云迁移工作。

内容主要是确定双方参与项目人员的职责，制订项目通讯录，确定项目实施计划及项目周期（项目周期为12天）。

即将所有域名先绑定到SLB上，然后转到后端Nginx，通过Nginx做虚拟主机等七层更灵活的控制。

（2）采用TCP层SLB保障性能
在实践中，面对高并发性能要求的场景时，我们发现HTTP层的负载均衡，相比TCP层的负载均衡，在性能上面有很大差距。HTTP层负载均衡只能达到万级别并发，而TCP层的负载均衡能达到几十万级，甚至上百万级的并发量。所以在电商等网站应用中，对于SLB，我们优先选择TCP层。

最后，在迁移方案中，我们确认了客户云上资源清单（23台ECS、2台RDS、1台SLB）及具体的服务器配置。

● 上云实践4：域名备案要先行


● 上云实践5：通过镜像提升云端部署效率
刚开始我们开通了一台ECS，并对这台ECS做了运维规范方面的系统调优、安全加固等措施。然后我们又把这台ECS做成了一个基础镜像，批量开通了22台同样环境的服务器，大大提升了部署效率。

● 上云实践6：自动化运维工具的应用
关于对应软件的安装脚本，我们内部团队都统一存在在内部的GitLab中。我们通过Ansible工具，定制对应的PlayBook，推送对应的安装脚本到目标机器上。5分钟内完成了对应Java、PHP、Python等环境的安装。

第五步：迁移测试
此阶段的工作内容主要为功能测试、性能测试，其主要集中在客户的测试团队（时间节点：2016年1月9日—2016年1月11日）。


● 上云实践7:Nginx反向代理将老用户请求引流至阿里云
由于DNS解析切换，部分用户由于本地DNS缓存的原因，导致请求访问的仍旧是老的IDC环境，而出现异常错误提示等。所以我们需要在IDC机房前端Nginx上做302重定向跳转，将依旧访问IDC的客户引流到阿里云，这将大大提高用户的访问体验性

云时代给运维行业带来不小的冲击，很多运维人员将面临失业。因为传统中小型互联网公司不再需要运维人员来做一些琐事，这些问题在云平台中都能得以解决。从另一方面来讲，云时代也将给我们带来新的机遇及挑战，这就要求技术人员的知识要更加全面。这也是很多人说DevOps是未来之路的根本原因！


### 11.2 混合云八大运维架构实践

云上上海地域的资源通过百兆专线（高速通道）和全华机房打通，存放云上MySQL数据库的从库热备数据，以及云上MongoDB的副本热备数据。迁移完成后，云上的环境为线上业务环境，IDC机房的环境改为测试环境。这样基于VPC+专线，便将云端和线下IDC数据中心内网打通，即构建了混合云架构。

由于云上MongoDB的副本集不能和IDC机房自建的MongoDB组成一个副本集，所以我们选择自建云上的MongoDB，主要是为了满足混合云中数据库跨平台冗余的架构。MongoDB副本集，两个节点在云上，还有一个Secondary节点在IDC机房，

1．更改/etc/resolv.conf，重启服务器后会被还原
我们发现/etc/resolv.conf其实是软连接到/run/resolvconf/resolv.conf的文件，进而才确认Ubuntu下本地DNS的管理是采用resolvconf命令的。这与Red Hat相比有很大区别，在Red Hat下直接编辑/etc/resolv.conf，便可进行DNS Server的设置。在Ubuntu系统中，我们可以看到/etc/resolvconf/resolv.conf.d目录下有base、head、tail三个配置文件，最终用来生成resolv.conf配置文件。

即把IP获取由DHCP改成手动静态配置，然后重启服务器，/run/resolvconf/interface/中生成的DNS不再由DHCP获取。我们配置在/run/resolvconf/interface/目录中的自定义DNS生效。

11.2.8 混合云实践8：关于DNS的高可用
虽然通过DNS+Consul解决了Redis及MySQL的高可用问题，但我们发现搭建的DNS本身就是一个单点。如果DNS宕机了，就意味着它无法解析IP地址了，也意味着连接的Redis、MySQL将全部宕机，这就使得DNS的高可用保障尤为重要。

DNSmasq做DNS，它并没有主从对应的方案。不过，由于客户端/etc/resolv. conf能配置多个nameserver的DNS Server。客户端可以根据配置的多个nameserver自上而下的选择一个可用的DNS Server来完成本地的域名解析，这样我们就可以配置多个自建的DNS Server，也就解决了自建DNS的高可用问题。

如若172.16.1.1宕机，客户端会自上而下发现172.16.1.1请求不通，然后自动去172.16.1.2的DNS Server进行解析查询。

### 11.3 云端运维架构五大优化

❑ 在云端有80%!的(MISSING)企业存在计算资源和存储资源闲置及浪费的普遍现象。云你真的会用吗？真的用好了吗？
❑ 随着业务发展，对应的资源越来越多，如何高效地对云端资源进行监控运维？

线上90%!的(MISSING)服务器的硬盘空间使用率在1%!～(MISSING)3%!之(MISSING)间，磁盘空间使用严重不饱和。建议统一从500GB降配至100GB。

由以上案例，我们可以看到年消费两百万的资源费用，大部分都是浪费。通过合理优化，每年的成本预算可以控制在三十万到五十万之间。并且通过很多类似案例我们也可以总结出以下经验，即根据业务的实际需求，结合系统实际的性能压力及峰值，在云端做出合理的配置规划。

❑ 每台机器都开通公网，安全性风险非常高。
❑ 服务器太多，在安全组上控制有些机器的端口服务只允许指定的公网IP访问，安全配置管理烦琐。

❑ 云端网络采用VPC，不同的业务放在不同的网段中，不同的网段用不同的安全组隔离。架构层次清晰，非常方便管理。

❑ 基于VPC，在一台绑定公网的ECS上通过IPTABLES设置SNAT。后端ECS不绑定公网EIP（只绑定内网），访问公网第三方服务的接口，通过这台有公网的ECS SNAT去实现对出公网请求的访问。

❑ 同样基于VPC，通过VPN网关服务的IPSec链路把云端VPC内网和公司内网VPC打通，我们不用再去设置繁杂的公网安全组规则，在公司直接访问云端内网端口服务。如果想要访问公司以外的网络，可以在公司内部，或者云端放个拨号VPN，在非公司办公网络，直接拨号到内部网络。这种网络架构，不管是IDC还是云端，都是成熟企业的混合云架构模式。

但重启后IPTABLES的规则没有加载，导致SNAT功能不可用，从而导致线上业务连接公网第三方服务接口不通，对应出现线上业务异常。后来添加了这条IPTABLES的SNAT规则，才恢复对应服务。

❑ ECS上绑定的EIP的带宽峰值仅有200Mbps，在该客户电商活动期间，由于请求压力较大，对出口带宽的压力也变大，导致200Mbps的带宽被跑满，从而线上业务出现访问超时、白屏的问题。当时我们临时的解决方案是在出口带宽压力较大的ECS上紧急绑定EIP，让数据的返回直接通过EIP，而不从SNAT出去，以减少那台ECS的SNAT压力，这样故障才得到解决。

将自建SNAT迁移到NAT网关服务上，其核心注意点就是在控制台中删除VPC中添加的路由条目，使路由器不再把公网访问请求转发给自建SNAT，而是转发给NAT网关。

❑ 前端用四层SLB进行流量转发，不用担心面对活动等高并发的业务场景，SLB表现出性能不足的问题。基本上四层SLB能抗10w～50w的并发，常规互联网高并发场景也基本上都能应付得来。

❑ 给域名申请对应证书，我们把证书放在后端Nginx中配置管理。不采用七层SLB在前端做证书集中管理，其实主要也是从性能方面考虑，虽然采用四层SLB，后端每台ECS上都需要配置证书。但是Nginx管理证书的配置也只有两行，然后结合Ansible自动化工具，这对运维侧来说也是非常方便的。

❑ 业务侧使用单独的二级域名来管理静态资源，静态资源统一存放至OSS中。
❑ 静态资源的二级域名直接将CNAME绑定在OSS的URL地址上，这样就直接跳过“使用Nginx做反向代理”这个冗余的步骤了

❑ 主机名规范：统一采用《业务名》-《用途》-《编号》来进行，比如prod-web-01。
❑ 开机自启动规范：梳理Nginx/Node/Java服务的启动命令至/etc/rc.local中，统一采用rc.local是为了方便了解每台服务器上运行的服务进程清单。

❑ 运维自动化：采用Ansible+Rundeck实现服务器日常集中化管理，Ansible+Rundeck都是基础的自动化运维工具。运维人员可以直接登录服务器通过自动化工具集中管理服务器，也可以登录DevOps平台管理服务器。

### 第12章 云端监控实践

监控是业务的“眼睛”，能让对应的异常问题在第一时间被发现，只有这样我们才能第一时间去解决问题。运维工作做的好不好，更多的是看监控有没有加好。

### 12.1 物理机体系：三大监控方案实践

缺点：缺乏中间件、应用层监控。缺乏监控数据存储、数据查看等监控集中化管理平台（Dashboard）。

### 12.2 云计算体系：四大监控方案实践

Zabbix被称作企业级解决方案，从这一点就能看出来Zabbix不仅仅能做Nagios主机、网络设备层面的监控，还能满足企业级其他方面的监控需求，比如中间件、日志等。
功能特点：
❑ 主机、网络、中间件、日志等性能监控（非常成熟完善）。
❑ 详细的报表图标绘制，如Dashboard、Graphs、Screen、Map等功能。
❑ 支持自动发现网络设备和服务器，支持分布式集中管理、管理监控点。

缺点：
❑ Zabbix的分布式体现在监控节点上，即客户端（Agent）监控数据的收集，Zabbix有Proxy模式，能满足大多数企业跨地域、跨内外网等网络结构下的监控需求。但是Server端的数据存储用的是以MySQL为主的关系型数据库，Server端存在很严重的性能问题。
❑ 需要在监控的目标主机中安装Agent，这样将会存在安全隐患。

云监控（CloudMonitor）是一项针对阿里云资源和互联网应用进行监控的服务。随着云平台功能的逐步完善及成熟，云平台中自带的云监控也逐渐日益完善及成熟。当前云监控不仅能完成针对云平台层面的云产品服务监控，也能完成主机层面、中间件层面等的定制化监控。产

一般情况下，云监控一键式的监控设置，能满足80%!互(MISSING)联网企业的监控需求。

Zabbix优化如下：
❑ CPU load > ‘cpu核数’ && CPU利用率> 90%!。(MISSING)
❑ 磁盘空间> 90%!&(MISSING)& 磁盘空间≤5GB。
❑ Zabbix针对阿里云的资源过期监控（通过调用API Python SDK）。

### 12.3 容器体系：四大监控方案实践

我们就需要有一套新的解决方案，来解决容器时代下容器的监控需求。而Prometheus的出现，便成为容器监控开源类的最佳解决方案。

这是因为任何监控目标，只要暴露出标准的HTTP协议的Metric数据，Prometheus就都能监控到。这也就意味着Prometheus的监控对象，不仅仅局限于IT系统。类似于容器监控需求，通过Prometheus很方便地就能实现，因此Prometheus被誉为容器开源解决方案的最佳实践。

所以Prometheus也适用于未来物联网（IOT）领域的海量数据存储及监控。

而Prometheus没有Agent的概念，只需要监控目标能按照标准的HTTP协议暴露出Metric数据，即可完成监控。

我们激动地把Alertmanager称为“报警神器”。Alertmanager是Prometheus下的一个独立的报警模块（需要单独部署、独立运行），主要用于接收Prometheus发送的告警信息，它支持丰富的告警通知渠道，而且很容易做到对告警信息进行去重、降噪、分组等操作，是一款前卫的告警通知系统。

❑ 需要每个中间件或者监控目标都要单独安装Export，如果有多个监控目标的话，多个监控目标对应暴露HTTP服务端口，在维护管理等方面非常不便。

❑ Prometheus的监控项值只能为浮点数据类型，不能为字符串数据类型，这个就具有局限性了。为什么不能存储字符串数据类型呢？因为官方觉得涉及不到字符串类型数据采集，应该属于日志采集的范畴，不属于监控的范畴。所以使用Prometheus，并不能做字符串类型数据采集，以及字符串类型数据的规则报警。

### 第13章 云端容器/DevOps实践

IT体系的发展经历了物理机体系阶段、云计算体系阶段，现在即将进入容器体系阶段。预计到2020年，将有超过50%!的(MISSING)全球企业在生产环境中运行容器。

计算资源的开通、时间耗费周期都是按分钟来计算的。相信随着云计算机的发展，容器技术也会逐渐普及。到那时，计算资源已经创建好，我们可直接从容器中索取计算资源，且时间耗费周期将达到秒级（Docker容器一般启动的速度是1～3秒）。相应地，对应的系统架构也从以前的单机/集群架构转换到如今云普及的分布式架构，并且即将进入微服务架构阶段。

而微服务架构给我们研发的灵活性、业务后期迭代带来了极大的可扩展性，对应大量的微服务也能更加方便、更加高效、更加适合地运行在容器中。

### 13.1 云端容器技术的十二大实践

但值得注意的是，Docker技术其实是一个单机版技术。而类似K8S（Kuberneters缩写，K和S之间有8个字母，所以简称K8S）的容器编排工具的核心是解决应用部署在单机Docker中时，所存在的扩容、缩容、故障自愈等容器资源管理问题。

总的来说，如果对容器的可靠性要求不高，Swarm比较合适；如果是对外服务，或者是需要提供高可靠服务的场景，K8S更合适。基本上80%!的(MISSING)中小型企业，其业务都较简单，因此在资源编码技术的选型方面，我个人还是推荐使用Swarm，因为它可以快速入手

我们通过GitLab来管理代码，通过Jenkins+Maven对代码进行全自动编译及打包，最终生成Docker镜像并传到私有仓库中。然后再结合K8S，把Docker镜像推到测试环境/预生产环境/生产环境中运行。

传统的DevOps流程中，是在操作系统中通过执行git pull命令来拉取代码进行更新发布的。而结合容器的DevOps流程，是先将代码打包成Docker镜像，然后把Docker镜像推到容器中运行，从而完成更新发布，如

相反，如若未部署Dashboard，则需要通过部署kubectl命令行工具来对K8S集群进行日常的管理及维护，这对部署人员的专业要求较高。K8S自带的Dashboard支持密码、证书认证

我们在Dashboard前加上Nginx做反向代理，Nginx中的第三方LDAP模块就完美地解决了我们对LDAP的需求。Nginx核心配置如下：
￼

2．Rancher对K8S的管理
Rancher是一个开源的企业级容器管理平台。有了Rancher，企业再也不必自己使用一系列的开源软件从头搭建容器服务平台了。Rancher提供了在生产环境中使用的管理Docker和K8S的全栈化容器部署与管理平台。

所以为了保障连接的Master节点高可用，我们最后还需要在APIServer前面加个四层的SLB，

但是在实践中，SLB的方案未通过。因为在APIServer中报错，集群对于SLB的IP不识别。而这主要是因为我们集群内通信采用的是SSL加密，在集群的kubernetes.pem证书中我们看到以下配置：

阿里云容器服务K8S版基于原生K8S进行适配和增强，简化了集群的搭建和扩容等工作，它通过整合阿里云的虚拟化、存储、网络和安全等能力来打造云端最佳的K8S容器化应用运行环境。

### 13.2 DevOps发展的四个阶段

对于传统运维，我们经常要做如下工作。
（1）和硬件相关的工作
和硬件相关的工作主要分为以下两类：
❑ 搬服务器，网络部署，机器上架。
❑ 硬件日常维护：联系机房重启机器，更换磁盘，更换内存，解决硬件故障等问题。
（2）和系统相关的运维工作
和系统相关的运维工作主要分为两大类：
❑ 系统环境运维状态维护：环境安装配置、性能调优、安全加固、日常运维故障处理、运行状态监控及巡检等。
❑ 业务代码变更发布：代码部署、代码日志查看、代码版本回滚、代码运行环境重启等。

❑ 研发经常会有变更发布的需求，即使是简单的变更发布也需要运维亲自完成。这种频繁简单的操作不仅浪费时间，也不利于及时变更发布。

针对运维自动化1.0版本下的自动化面临的问题及挑战，我们把日常运维经常用到的重复性的命令（如重启、版本更新、版本回滚等）通过Rundeck图形化管理工具封装成一个Web控制台。运维、研发只需要点击Web控制台中已经封装好的功能模块，就可以完成日常运维及日常代码变更发布等工作，如图13-17所示

在Ansible+Shell基础之上加入Rundeck后，可实现集中图形化管理，并且还可以根据不同角色来进行权限划分。

DevOps的出现就是为了解决软件开发人员（Development, Dev）和IT运维技术人员（Operations, Ops）之间的沟通协作问题，以使得构建、测试、发布软件能够更加快捷、频繁和可靠。

运维自动化主要解决资源集中管理方面的问题以提升运维效率，其关注系统维护层面。而DevOps则主要解决持续集成（Continuous Integration, CI）及持续交付（Continuous Delivery, CD）方面的问题以提升业务快速集成及交付，其关注业务迭代层面。

在运维自动化2.0版本中，研发能通过运维提供的工具平台快速完成代码部署交付到测试环境、预发环境，甚至到生产环境，这已经属于DevOps体系中CI/CD中的CD功能。但是如何做到持续集成，即完成代码的自动编译及自动测试，这是DevOps体系要解决的问题。

Jenkins是开源CI&CD软件的领导者，提供超过1000个插件来支持构建、部署、自动化，以便满足任何项目的需要。Jenkins出现后很快成为DevOps体系主流的最佳实践工具。通过Jenkins我们很快解决了软件代码的自动编译及自动测试，并且对应的插件也能帮助Jenkins通过SSH无缝打通后端服务器，通过Jenkins也能完成软件代码的部署交付

运维自动化从脚本及工具阶段演变至平台化阶段，即运维自动化的终极目标就是不登录服务器，通过Web界面进行简单设置操作就能完成所有日常运维管理工作。即我们需要在运维自动化2.0基础之上进一步以平台界面“傻瓜式”操作来替代日常运维执行命令、执行脚本/工具的方式，从而进一步提升运维效率。

随着云平台的普及，你会发现，云平台能对IT资源做自动化管理，所以本身云平台就是一个大的运维自动化平台。也是因为云平台的出现，我们的运维技术直接跳过人工阶段、脚本及工具阶段，到了平台化这个阶段。

而在云端，CMDB、资源管理早已被云自动化及平台化。甚至传统运维做不到的硬件资源管理及维护，在云端都已不是问题。云平台的出现，我称之为运维自动化3.0。图13-19所示为云平台中ECS产品的运维自动化管理控制台界面。
￼
图13-19 ECS运维自动化管理控制台界面

在云端很少还有人在讲运维自动化，这是因为本身云平台已实现了平台化的运维自动化。所以我们已将更多的精力由传统底层资源的运维管理转向业务层的运维管理，这也是大家如今提到的更多的是DevOps的原因。

但由于每个公司的业务流程不同，我们的运维规范标准也有所不同。比如，金融类业务的运维更多偏向安全性方面；电商类业务的运维偏向快速扩容等资源管理方面；而政企类业务的运维更多偏向以审批为导向的流程化管理方面。所以云平台通用的运维自动化管理可能满足不了中大型公司对定制化的自动化诉求。

图13-20 驻云运维平台
平台功能模块概要如下：
❑ 账号管理：运维平台的登录，通过公司内部的LDAP集中进行用户认证管理。
❑ 客户管理：包含我们运维所有客户信息、运维档案信息，如我方架构师、运维人员、销售人员、客户对接人员等，以及客户拜访记录等。


3．DevOps2.0
正如上文所述，云平台的普及，让我们把更多的精力由传统运维的资源管理运维自动化转移到业务管理DevOps（持续集成/持续交付）自动化上。即通过Jenkins一键式完成编译打包、部署上线，从而实现业务的快速迭代及交付，这是DevOps的1.0阶段。

采用微服务架构，其实是业务层的解耦拆分，主要带来业务上的重组及快速迭代等弹性优势。但随着微服务越来越多，维护的服务器也日益增加。这给我们带来了极大的运维挑战，我们在以下几个方面的运维工作越来越困难：

❑ CI/CD（可持续集成/可持续交付）的主要压力表现在可持续交付上。

即DevOps1.0主要通过Jenkins等技术解决可持续集成方面的问题，但在可持续交付部署方面仍有不足

通过容器资源编排技术Kuberneters+Jenkins+微服务架构，便成为如今的DevOps2.0体系，如图1

❑ CI/CD（可持续集成/可持续交付）上，代码提交到GitLab中，通过Jenkins进行编译，然后打包成一个Docker镜像上传至私有镜像仓库。线上的变更，只需要拉这个镜像即可；新增某个应用的台数时，只需要定义YAML配置文件增加对应的副本即可。整个线上的更新迭代，容器启动在秒级即可完成发布。

❑ 整个DevOps流程，也可以结合JIRA+Conf luence做项目管理及知识库管理。
所以引入容器资源编排的架构，让我们在CI/CD、运维管理、资源管理上的效率都提升了一个档次。

GitLab+Jenkins+K8S+微服务是云端DevOps的最佳实践。

智能运维，即当前如火如荼的人工智能和运维的结合——AIOps。智能扩容、故障自愈、智能分析等进一步减少了人为参与，也进一步提升了运维效率。AIOps的核心在于AI算法在运维领域的运用，但当前在市面上类似的成熟解决方案及产品少之又少，还处于概念期

通过将多年的云服务经验数字化，并结合基于人工智能的诊断平台，CloudCare针对用户的IT资源进行检测与诊断，从云平台、云主机到集群，为用户提供监控、安全、费用、优化等多方位的提醒、告警及实施方案，帮助用户大幅地缩短IT系统的平均诊断时间，提升IT系统管理效率，如图13-22所示。

### 14.1 云端安全问题的现状

随着云的普及，云技术更加成熟及稳定。如今如果有人还在对比云技术的优势，企业还在问要不要上云，那必然会被互联网浪潮淘汰，必然会被云时代淘汰。

依靠云本身的灵活性等技术特点，我们在云端可选择对应的安全产品一键式解决安全类问题，也就是说，云让安全问题更加快捷高效地被解决。例如，很多用户就是为了云盾免费的5G DDoS流量防御而来，即买一台1核1GB的服务器，一个月的成本才50元左右，但能免费获得5GB的DDoS流量防御，何乐而不为呢？

### 14.2 云端安全面临的三大挑战

最大的安全问题，并不是黑客有多厉害，而是我们的安全意识低下。

明明系统安全做得足够好了，为什么还是会被入侵？事实上，总有人把很多系统部署在一起或者同一局域网内，黑客利用“旁注”的攻击手段，只需要攻击“弱小”的那个系统，以此为突破口，再通过“旁注”入侵另外的系统即可。

整个市场的安全人员基本都集中在大型公司中。中小型企业安全人员的招聘，一直是瓶颈。后来我们甚至放弃招聘，直接在从内部向安全方向培养相关人才，这成为我们招聘安全相关人才的主要渠道。

安全成本一直是个严峻的挑战。安全领域会涉及复杂的高端技术，这也决定了安全领域攻防的成本同样高昂（这也是黑色安全产业链暴利的核心原因）

### 15.5 黑客常见应用层攻击

应用层常见攻击排行最前面的，当属SQL注入攻击。所谓SQL注入，就是通过把SQL命令插入Web表单提交或输入域名、页面请求的查询字符串，最终达到欺骗服务器执行恶意的SQL命令。

### 15.6 黑客常见网络层攻击

分布式拒绝服务（Distributed Denial of Service, DDoS）是当今网络上危害最大的黑客攻击，能让网络、系统业务、服务器全部瘫痪，无法提供正常服务。

黑客通过控制大量的客户端（“肉鸡”，一般通过木马入侵控制）向目标服务器发送大量网络请求包，导致目标服务器网络堵塞、服务器性能飙升，不能对外正常提供服务。

当前防御DDoS，都是侧面通过抗流量的高性能的硬件防火墙进行流量清洗，这是唯一有效的措施。

而DDoS核心攻击原理主要是针对TCP三次握手，即属于OSI七层模型中四层（TCP层）的攻击。因为原理不同，它们在云端的防御手段也是完全不同的。CC攻击需要通过WAF应用防火墙进行防御，而DDoS攻击需要通过高防IP进行防御。

### 16.1 云端常见黑客攻击的防御

2．WAF对应用层攻击的防御
对于应用层的攻击，主要是因为代码存在安全问题，成为黑客入侵的漏洞。在云端，最为有效的解决办法就是使用云盾Web应用防火墙（Web Application Firewall, WAF）。
WAF基于云安全大数据能力，用于防御SQL注入、XSS跨站脚本、常见Web服务器插件漏洞、木马上传、非授权核心资源访问等OWASP常见攻击，并过滤海量恶意CC攻击，避免网站资产数据泄露，保障网站的安全与可用性。

DDoS高防IP产品使用专门的高防机房提供DDoS防护服务，通过引流、清洗、回注的方式将正常业务流量转发至源站服务器，确保源站服务器的稳定可用。

必须将WAF与安骑士结合起来使用。WAF保障HTTP拦截具有安全风险的请求，避免网站再次被挂载木马。而安骑士主要检测已经通过Web流入系统中的木马文件。

实践1:DDoS防御治标不治本
监控报警，发现业务不可达。阿里云通知SLB地址被拉入黑洞，控制台提示攻击流量达到11GB。但由于客户成本方面的问题不使用DDoS高防。
基于以上原因我们给出了如下的临时解决方法：
1）更换SLB（等同于更换IP）。
2）开启DDoS基础防护中的“安全信誉开关”。

在云端我们遇到某金融类客户，每天都会出现PC站、手机站、移动App等多处业务无法访问的情况，每次大约1～6小时，被黑客DDoS攻击峰值达到200GB以上，致使网站完全瘫痪，每小时损失近百万元。

域名统一解析到DDoS高防IP上，DDoS高防IP过滤到三四层网络流量攻击，然后再将请求转发至WAF产品上，最终WAF将正常请求转回至源站。

实践3:综合流量攻击中的实践
并不是所有流量攻击都用DDoS高防IP，DDoS流量主要针对三四层网络流量攻击，而CC流量攻击主要针对七层网络流量攻击。所以面对DDoS流量攻击时，我们需要用DDoS高防IP防御，而CC流量攻击则用WAF防御。

再比如，客户被DDoS攻击，瞬时流量高达50Gbps，同时CC攻击流量高达1Gbps。请问要怎么使用云盾安全产品？答案是这时候需要用DDoS+WAF结合起来防御。

### 16.2 云端安全架构四大策略实践

一些大型企业就会选择专有云（私有化部署）来满足自身定制化需求。这时候可以考虑使用阿里专有云或者传统OpenStack来部署，并采用驻云的CBIS系统进行云产品资源管理。

16.2.2 策略二：分布式架构是安全保障的基石
在前面“1+1 > 2”的案例中也曾提到，分布式架构是安全保障，也是性能、稳定性的保障，同时还能冗余防御DDoS方面的攻击请求。正所谓“鸡蛋不要放在一个篮子里”，通过分布式跨地域部署，能够进一步提高安全性，

图16-6跨地域、跨平台分布式架构


进入云机房后会最先经过DDoS流量清洗集群，然后经过CDN，之后数据包会流向Web应用防火墙（WAF），然后再经过云平台的安全组网络规则，再到负载均衡SLB上。SLB会将数据包转到ECS，数据包通过网卡流入内核，会先经过Iptables。通过Iptables后，数据包会由内核空间流入用户空间，流入到Nginx中。Nginx通过反向代理，会向应用服务器发起数据包转发。应用服务器（Java、PHP、Node）处理数据包请求，期间可能会调用数据库。发起新的请求至RDS数据库中，数据库处理完成后会返回数据结果至应用服务器。应用服务器封装数据结果，将结果返回给客户端。

16.2.4 策略四：基于VPC的企业级安全架构
在云端企业级安全架构中，核心是基于VPC网络，能有效减轻或避免黑客针对网络层攻击

我们可以采用VPN，把云上环境和公司内网环境做IPSEC内网打通，公司AD/OA/ERP/邮箱/测试环境等全部可以放在云端。云端不仅能和企业内网打通，也能和IDC、数据中心的内网打通。普通VPN由于基于公网加密传输，如若对传输速度、质量有网络要求，可将线路改为专线，以保障传输速度及质量。

### 16.3 云端运维安全实践十则

堡垒机是远程控制的必要安全手段，即我们需通过堡垒机来对服务器进行远程管理。堡垒机具备安全审计、权限管理等核心安全管理功能，也可以避免黑客恶意远程登录等安全问题。在云端堡垒机实践中，有三种实践方案：
1）阿里云自带堡垒机（推荐，原因是与ECS云产品的无缝结合更紧密）；
2）驻云CSOS云安全运维系统；
3）在ECS上自建JumpServer。

通过安全组控制，ECS不能直接对外提供SSH/RDP远程连接，只能通过JumpServer实现。即想要SSH/RDP登录ECS，只能通过JumpServer进行。这样就能在JumpServer上做安全审计、权限控制等安全管理了。
JumpServer支持客户端及Web界面连接登录，基本上主流堡垒机，包括阿里云自带堡垒机都支持客户端和Web界面登录的方式。

通过安全组+VPN，把云端和公司网络的内网打通。在公司内网中的办公网络，能直接远程登录云端服务器。而在非公司网络，比如家庭网络，需要提前拨号VPN登录公司网络，然后才能远程登录云端服务器。这是当前云端最为成熟的网络架构。

首先，程序采用非root用户运行，特别是PHP等中间件，安全漏洞较多，如若采用root用户进程，root权限较大，则很可能出现利用漏洞直接执行一些恶意代码或者程序的情况，存在较大安全隐患。

安全组主要针对属于同个安全组下ECS共用的端口安全规则设置，而Iptables主要针对ECS访问级别能做更细粒度的安全规则设置。

Iptables的四表五链，针对内核级别的数据包控制，更适合做更细粒度的安全规则控制。

我们来看一个具体案例，Iptables能根据进过内核的数据包state状态来做控制，甚至能控制每秒经过内核数据包的个数。

数据库类服务居然绑定在本机0.0.0.0上，表示本机内网、公网、localhost等都能访问。这是个小细节，但也能体现较大的安全问题。

甚至很多商业的WAF产品，都是基于OpenResty做二次开发得来的。OpenResty其实就是Nginx通过Lua进行了二次开发，然后可以随心所欲地做复杂的控制及安全监测。

在实际应用场景中，我们一般把证书存放在流量入口处。

2）调优操作系统内核参数，冗余更多TCP连接。
￼
        net.ipv4.ip_local_port_range=102465000
表示用于向外连接的端口范围。默认情况下很小，只有32768～61000，建议改为1024～65000。
￼
        net.ipv4.tcp_max_syn_backlog=8192
表示SYN队列的长度，默认为1024，加大队列长度为8192，可以容纳更多等待连接的网络连接数。

  net.ipv4.tcp_tw_reuse=1
表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭。

Rsync作为开源领域一款热门的增量文件同步工具、文件迁移工具、文件备份工具，实在想不出有谁能替代这个好用且功能强大的工具，如图16-13所示。甚至一些商业的迁移、备份工具，感觉都没有Rsync好用。

最终发现，采用Rsync提前做热备份迁移，可以完全无缝地实时迁移。

Rsync有个缺陷，就是并不是实时的，但Rsync+Inotify的结合能实现实时同步。这在前面的共享文件实践中有详细介绍。

定期进行安全巡检，及时进行安全补丁更新、漏洞修复。

### 16.4 云端防御综合案例总结

第二步，对安全架构进行优化。立刻将该网站切换成分布式架构，前端通过SLB将请求转发给后端4台4核8GB的服务器实施业务请求。提升业务系统性能，使其能够支撑更多请求、冗余更多攻击请求流量。

### 第四篇 云端架构篇

向大家分享一个小型网站如何逐步演变到千万级架构的过程。

### 第17章 云端千万级架构的演变

一个好的架构是靠演变而来的，而不是单纯靠设计。刚开始做架构设计时，我们不可能全方位地考虑到架构的高性能、高扩展性、高安全等各方面的因素。随着业务需求越来越多、业务访问压力越来越大，架构不断地演变及进化，进而造就了一个成熟稳定的大型架构。淘宝网、Facebook等大型网站的架构，无一不是从小型规模架构，不断进化及演变成一个大型网站架构的。

### 17.1 架构原始阶段：万能的单机

架构的最原始阶段，即一台ECS服务器搞定一切。传统官网、论坛等应用，只需要一台ECS即可，对应的Web服务器、数据库、静态文件资源等，都可以部署到一台ECS上。一般5万PV到30万PV访问量，结合内核参数调优、Web应用性能参数、数据库调优，基本上能够在一台ECS上稳定运行。

### 17.2 架构基础阶段：物理分离Web和数据库

将Web应用和数据库物理分离单独部署，即可解决对应性能问题。这里的架构采用ECS+RDS

### 17.3 架构动静分离阶段：静态缓存+对象存储

通过将动态请求、静态请求的访问分离（“动静分离”），有效解决服务器在磁盘I/O、带宽方面的访问压力。
该架构采用CDN+ECS+OSS+RDS的形式，如图17-3所示。


❑ CDN可以直接进行动静请求分离，但在此用户量规模下，可能很多人觉得没必要用OSS。不过我还是建议将静态资源采用独立的二级域名集中部署在OSS中，方便后续架构扩展，也方便CDN回源请求加速。

### 17.4 架构分布式阶段：负载均衡

但是动态请求的压力已经让服务器“吃不消”。最直观的表现是前端访问堵塞、延迟、服务器进程增多、CPU100%!，(MISSING)并且出现常见的502、503、504等错误码。

显然单台Web服务器已经满足不了需求，这里需要通过负载均衡技术增加多台Web服务器（对应ECS可以选择不同可用区，进一步保障高可用）。因而告别单机时代，转变到分布式架构阶段。
该架构采用CDN+SLB+ECS+OSS+RDS的形式，如图17-4所示。


图17-4 负载均衡的分布式架构
● 

此阶段的架构中，采用七层SLB即可满足绝大多数应用场景及业务压力。但在高并发场景下，我们优先考虑四层SLB，但是这可能会增加架构的运维配置管理工作量。

### 17.5 架构数据缓存阶段：数据库缓存

该架构采用CDN+SLB+ECS+OSS+云数据库Memcache+RDS的形式，如图17-5所示。


数据库缓存，需要业务代码改造及支持。哪些热点数据需要缓存，这需要业务方面重点规划。并且云端数据库缓存，已不推荐在ECS中自行搭建Redis、Memcache等。

### 17.6 架构扩展阶段：垂直扩展

Web服务器和数据库仍然是瓶颈。在此我们通过垂直扩展，进一步切分Web服务器和数据库的压力，解决性能问题。
“何为垂直扩展，按照不同的业务（或者数据库）切分到不同的服务器（或者数据库）之上，这种切分称之为垂直扩展。”

垂直扩展第一招：业务拆分
在业务层，可以把不同的功能模块拆分到不同的服务器上进行单独部署。比如，将用户模块、订单模块、商品模块等拆分到不同服务器上部署。

垂直扩展第二招：读写分离
在数据库层，如果结合数据库缓存，数据库压力还是很大，那么可以通过读写分离的方式，进一步切分及降低数据库的压力。

垂直扩展第三招：分库
结合业务拆分、读写分离，在数据库层同样可以把用户模块、订单模块、商品模块所涉及的数据库表（用户模块表、订单模块表、商品模块表）等，拆分出来分别存放到不同数据库中，如用户模块库、订单模块库、商品模块库等。然后再把不同数据库分别部署到不同服务器中。

该架构采用CDN+SLB+ECS+OSS+云数据库Memcache+RDS读写分离的形式，如图17-6所示。

图17-6 数据库垂直扩展的架构

❑ 此阶段的架构中，由于业务请求有一定压力，建议采用四层SLB，虚拟主机、证书配置等七层功能需要放在后端服务器ECS搭建的Nginx上来控制，但是这可能会增加架构的运维配置管理工作量。

### 17.7 架构分布式+大数据阶段：水平扩展

比如，读写分离仅解决读的压力，面对高访问量，在数据库“写”的压力上面“力不从心”，会出现性能瓶颈。另外，分库虽然将压力拆分到不同数据库中。但当数据量达到TB级别以上，显然已经达到传统关系型数据库处理的极限。

水平扩展第二招：增加更多的SLB
单台SLB也存在单点故障的风险，并且SLB也存在性能极限，如七层SLB的QPS最大值为50000。可通过DNS轮询，将请求轮询转发至不同可用区的SLB上面，实现SLB水平扩展。


水平扩展第四招：Sharding+NoSQL
面对高并发、大数据的需求，传统的关系型数据库已不再适合。需要采用DRDS（MySQL Sharding分布式解决方案）+OTS（基于列存储的分布式数据库）对应的分布式数据库来从根本上解决问题。
该架构采用CDN+DNS轮询+SLB+ECS+OSS+云数据库Memcache+DRDS+OTS的形式，如图17-7所示。

❑ 大型应用中，海量业务数据的存储、分析给数据库带来了巨大挑战。传统的关系型数据库明显已经“力不从心”，分布式数据库NoSQL是适用技术发展的未来趋势。
❑ 有了海量业务数据的基础，我们可以结合云端MaxCompute大数据分析服务，来辅助业务进行价值创造。

### 第18章 云端架构的应用

云端最热门的互联网应用，当属电商、游戏、移动社交、金融等。回归到技术层面，无非还是用Java、PHP、Python等开发语言，Oracle、MySQL、SQLServer等关系型数据库，以及MongoDB、Redis、Memcache等非关系型数据库。回归到架构层面，则无非是用负载均衡、数据库、对象存储等分布式架构。

### 18.1 云端电商架构应用

高并发是电商业务最为明显的业务特点。所以针对电商类业务架构，高并发是要重点解决的问题。

弹性伸缩可以根据我们定义的伸缩规则，自动开通相应的ECS来应对业务压力

电商业务的另一个特点就是静态资源几乎占了业务请求流量的80%!。(MISSING)因为在电商类业务中，有大量商品图片，而这些商品图片往往是商品详情介绍最为核心的一部分。

### 18.2 云端游戏架构应用

业务特点：分区
相比传统B/S、C/S应用架构，游戏架构的特点在于其特有的“分区”概念。
为了解决玩家的延时性，传统IDC机房的游戏架构，一般分为电信和联通两大分区，其下又分为“北京”“上海”“江苏”等分区

这样部署主要是为了解决游戏玩家的延时性。例如，北京电信的玩家登录上海电信的服务器，延时一定很高。

服务器部署数据库即可。如果用户量增加，比如上海电信的用户量增加，可以再独立部署一台服务器和一台数据库。因而就有了上海电信一区、上海电信二区这样的区分。

游戏类业务偏向计算，需要中高配类型服务器配置。型号优先选择CPU与内存资源配比为1:2的类型，即8核16GB的配置基本上是游戏类经典配置。

1．页游与手游、端游的架构区别
页游是网页游戏的简称，这类Web游戏应用，都属于B/S架构的范畴。

手游是手机游戏的简称，端游也就是客户端游戏的简称，手游和端游的特点是需要在手机、电脑上安装客户端。所以手游和端游就是典型的C/S架构。

2．页游、手游、端游的技术区别
页游、手游、端游后端的技术可能相差不大，主要技术区别在于前端。页游采用HTML5、Flash等前端技术。手游，如果是Android系统采用Java技术；如果是iOS系统主要采用Object C。端游首选是C++了。

### 18.3 云端移动社交架构应用

所以移动社交类，用户量大也是业务一大特点。以用户量大为基础，结合聊天、朋友圈等功能，有海量的文字信息、音视频、图片内容存储。所以针对移动类这一业务特点，我们在架构设计上有两个侧重点

一方面，是大量的信息存储。传统关系型数据库已经应付不过来，需要通过MongoDB、列存储等分布式数据库来进行存储。

另一方面，音视频、图片等半结构化海量数据，必然需要通过OSS对象存储分布式文件系统来解决。特别是OSS中的图片处理、音视频转码等功能，这是移动社交类中必不可少的业务需求，所以OSS几乎成为移动社交类的标配。当然也可以在OSS的基础上，结合CDN，对OSS中存储的音视频、图片等数据进行访问加速。


### 18.4 云端金融架构应用

业务特点：安全
安全是金融类业务应用的代名词。

明显的趋势是传统金融也在向互联网金融转型。

为了满足银行、证券、保险、基金等金融机构对安全性的要求，云端采用独立于公有云的机房集群来部署金融云平台，相比于传统的公有云平台，在安全性、服务可用性和数据可靠性等方面作了大幅增强。

金融云按照人民银行和银保监会的合规标准建设，在安全性、服务可用性和数据可靠性等方面做了大幅增强。

3．金融云可以提供专享产品与服务
❑ 独立的资源集群
❑ 更严格的机房管理
❑ 更高的安全容灾能力
❑ 更严格的网络安全隔离要求
❑ 更严格的访问控制
❑ 遵从银行级的安全监管及合规要求
❑ 专门的金融云行业安全运营团队、安全合规团队、安全解决方案团队

在IT方面金融公司会有比较严格的安全流程控制管理，因此在实施具体技术时，相应安全规范、安全加固也会细致许多。所以相应地也要加强相关工作人员的安全意识，做好安全培训和安全管理工作。