## Ceph分布式存储实战
> Ceph中国社区

### 本书赞誉

正如OpenStack日渐成为开源云计算的标准软件栈，Ceph也被誉为软件定义存储开源项目的领头羊。

Ceph是主流的开源分布式存储操作系统。我们看到越来越多的云服务商和企业用户开始考察Ceph，把它作为构建“统一存储”和“软件定义存储”的可信赖解决方案。

在开源软件定义存储（SDS）领域，Ceph是当之无愧的王者项目。随着IaaS技术的火热发展，越来越多的用户开始在生产环境中部署SDS。

### 前言

目前Ceph已经成为整个开源存储行业最热门的软件定义存储技术（Software Defined Storage, SDS）。它为块存储、文件存储和对象存储提供了统一的软件定义解决方案。Ceph旨在提供一个扩展性强大、性能优越且无单点故障的分布式存储系统。从一开始，Ceph就被设计为能在通用商业硬件上高度扩展。

由于其开放性、可扩展性和可靠性，Ceph成为了存储行业中的翘楚。这是云计算和软件定义基础设施的时代，我们需要一个完全软件定义的存储，更重要的是它要为云做好准备。无论运行的是公有云、私有云还是混合云，Ceph都非常合适。国内外有不少的Ceph应用方案，例如美国雅虎公司使用Ceph构建对象存储系统，用于Flickr、雅虎邮箱和Tumblr（轻量博客）的后端存储；国内不少公有云和私有云商选择Ceph作为云主机后端存储解决方案。

Ceph已被不断完善，并融入以下建设性理念。❑每个组件能够线性扩展。❑无任何单故障点。❑解决方案必须是基于软件的、开源的、适应性强的。❑运行于现有商业硬件之上。❑每个组件必须尽可能拥有自我管理和自我修复能力。

在Ceph中，对象是不依赖于物理路径的，这使其独立于物理位置。这种灵活性使Ceph能实现从PB（petabyte）级到EB（exabyte）级的线性扩展。

❑云平台运维工程师。
❑存储系统工程师。

### 1.1 Ceph概述

Ceph消除了对系统单一中心节点的依赖，从而实现了真正的无中心结构的设计思想，这也是其他分布式存储系统所不能比的。

OpenStack是目前最为流行的开源云平台软件。Ceph的飞速发展离不开OpenStack的带动。目前而言，Ceph已经成为OpenStack的标配开源存储方案之一，其实际应用主要涉及块存储和对象存储，并且开始向文件系统领域扩展。

2010年，Linus Torvalds将Ceph Client合并到内核2.6.34中，使Linux与Ceph磨合度更高。
2012年，拥抱OpenStack，进入Cinder项目，成为重要的存储驱动。
2014年，Ceph正赶上OpenStack大热，受到各大厂商的“待见”，吸引来自不同厂商越来越多的开发者加入

2016年，OpenStack社区调查报告公布，Ceph仍为存储首选，这已经是Ceph第5次位居调查的首位了。

Ceph可以提供对象存储、块设备存储和文件系统服务，其对象存储可以对接网盘（owncloud）应用业务等；其块设备存储可以对接（IaaS

Ceph作为开源项目，其遵循LGPL协议，使用C++语言开发，目前Ceph已经成为最广泛的全球开源软件定义存储项目，拥有得到众多IT厂商支持的协同开发模式。

3）互联网企业：腾讯、京东、新浪微博、乐视、完美世界、平安科技、联想、唯品会、福彩网和魅族等国内互联网企业。

代码提交都需要经过单元测试，模块维护者审核，并通过QA测试子集后才能合并到主线。社区维护一个较大规模的测试集群来保证代码质量，丰富的测试案例和错误注入机制保证了项目的稳定可靠。

在Ceph存储中，包含了几个重要的核心组件，分别是Ceph OSD、Ceph Monitor和Ceph MDS。一个Ceph的存储集群至少需要一个Ceph Monitor和至少两个Ceph的OSD。运行Ceph文件系统的客户端时，Ceph的元数据服务器（MDS）是必不可少的。下面来详细介绍一下各个核心组件。❑Ceph OSD：全称是Object Storage Device，主要功能包括存储数据，处理数据的复制、恢复、回补、平衡数据分布，并将一些相关数据提供给Ceph Monitor，例如Ceph OSD心跳等。一个Ceph的存储集群，至少需要两个Ceph OSD来实现active +clean健康状态和有效的保存数据的双副本（默认情况下是双副本，可以调整）。注意：每一个Disk、分区都可以成为一个OSD。

❑Ceph Monitor:Ceph的监控器，主要功能是维护整个集群健康状态，提供一致性的决策，包含了Monitormap、OSD map、PG（Placement Group）map和CRUSH map。

❑Ceph MDS：全称是Ceph Metadata Server，主要保存的是Ceph文件系统（File System）的元数据（metadata）。温馨提示：Ceph的块存储和Ceph的对象存储都不需要Ceph MDS。Ceph MDS为基于POSIX文件系统的用户提供了一些基础命令，例如ls、find等命令。

RADOSGW功能特性基于LIBRADOS之上，提供当前流行的RESTful协议的网关，并且兼容S3和Swift接口，作为对象存储，可以对接网盘类应用以及HLS流媒体应用等。

通过LIBRBD创建一个块设备，通过QEMU/KVM附加到VM上，作为传统的块设备来用。目前OpenStack、CloudStack等都是采用这种方式来为VM提供块设备，同时也支持快照、COW（Copy On Write）等功能。

Ceph FS（Ceph File System）功能特性是基于RADOS来实现分布式的文件系统，引入了MDS（MetadataServer），主要为兼容POSIX文件系统提供元数据。一般都是当做文件系统来挂载。

Ceph最初针对的应用场景，就是大规模的、分布式的存储系统。所谓“大规模”和“分布式”，至少是能够承载PB级别的数据和成千上万的存储节点组成的存储集群。

❑去除所有的中心点：搞IT的最忌讳的就是单点故障，如果系统中出现中心点，一方面会引入单点故障，另一方面也必然面临着当系统规模扩大时的可扩展性和性能瓶颈。除此之外，如果中心点出现在数据访问的关键路径上，也必然导致数据访问的延迟增

### 4.1 Ceph FS文件系统

RGW是Ceph对象存储网关服务RADOS Gateway的简称，是一套基于LIBRADOS接口封装而实现的FastCGI服务，对外提供RESTful风格的对象存储数据访问和管理接口。RGW基于HTTP协议标准，因此非常适用于Web类的互联网应用场景，用户通过使用SDK或者其他客户端工具，能够很方便地接入RGW进行图片、视频以及各类文件的上传或下载，并设置相应的访问权限，共享给其他用户，形成最简单的网盘分享，

5）Object（对象，文件）：在S3中，用户操作的基本数据单元是Object。单个Object允许存储0～5TB的数据。Object包含key和data。其中，key是Object的名字；data是Object的数据。key为UTF-8编码，且编码后的长度不得超过1024个字节。

### 7.1 Ceph与KVM

OpenStack是一个由NASA（美国国家航空航天局）和Rackspace合作研发并发起的开源云计算管理平台项目，由几个主要的组件组合起来完成具体工作。OpenStack支持几乎所有类型的云环境，项目目标是提供实施简单、可大规模扩展、组件丰富、标准统一的云计算管理平台。OpenStack通过各种互补的服务提供了“基础设施”即服务（IaaS）的解决方案，每个服务提供API以进行集成。

### 8.1 网盘方案：RGW与OwnCloud的整合

对象存储服务基于HTTP协议，在互联网以及移动互联网中有着得天独厚的优势，特别是存储多媒体数据（如图片、音频、视频等）。在实际应用方面，大家熟知的各类云盘、图片和音视频云存储服务，基本上都是利用对象存储技术，因此对象存储早已经遍布在各类互联网服务中，成为互联网不可分割的一部分。

5）配置OwnCloud来将Ceph用作一种S3外部存储。

### 9.1 需求模型与设计

Cgroups是Control groups的缩写，是Linux内核提供的一种可以限制、记录、隔离进程组（Process Groups）所使用的物理资源（如CPU、Memory、IO等）的机制。最初由Google的工程师提出，后来被整合进Linux内核。Cgroups也是LXC为实现虚拟化所使用的资源管理手段，可以说没有Cgroups就没有LXC。Cgroups内容非常丰富，展开讨论完全可以单独写一章，这里我们简单谈一下Cgroups在Ceph中的应用。

前面提到了当我们要进行网络IO时，会触发系统中断。默认情况下，所有的网卡中断都交由CPU0处理，当大量网络IO出现时，处理大量网络IO中断会导致CPU0长时间处于满负载状态，以致无法处理更多IO导致网络丢包等并发问题，产生系统瓶颈

### 13.1 Ceph集群运维

首先从Ceph集群的扩展、维护与监控3方面来讲解一下Ceph日常运维的一些经验。