## 互联网企业容器技术实践
> 龚曦主编

### 前言

随着互联网行业的不断发展，企业管理的服务器数量大幅增加，业务系统越来越复杂，用户体验要求也越来越高，运维工作面临的挑战和难度也越来越大。
2013年，Docker作为一个开源项目横空出世，解决了服务器应用快速构建、部署和分享的问题，能够把服务器应用像 App 一样简单地安装到各种平台环境中，而不受真实环境的影响。对于开发者和运维工程师来说，Docker 是提高开发和部署效率的一大利器；而对于企业来说，能够节省大量的基础设施投入和降低维护成本。

而Kubernetes作为后起之秀，在Docker技术的基础上，为容器化的实施提供了部署运行、资源调度、服务发现和动态伸缩等一系列完整功能，使得大规模容器集群管理更加便捷。虽然Kubernetes的出现比Docker晚一些，但其影响力同样巨大，作为一个容器编排管理平台，其受关注度甚至超过了容器本身。

容器技术的兴起，不仅使应用开发部署发生了深刻变革，也让应用设计架构和运维部署发生了新的变化。企业进行容器化改造，通常根据企业的实际应用和业务需求选择容器化管理平台，并进行应用容器化改造和DevOps建设。

网易云计算工程师

云计算架构师

### 作者简介

目前工作聚焦在美联容器云平台的研发和容器DevOps项目的落地上，在Docker容器、Kubernetes、DevOps、Cloud Native、微服务化等方面都有丰富的实战经验。

### 1.1 容器简介与Docker容器引擎

容器又称为容器虚拟化技术，从名称可以看出容器也是一种虚拟化技术。容器是基于操作系统的轻量级的虚拟化技术。容器的英文名称是 Container，该单词的另一个意思是集装箱

容器就是将软件打包到一个平台中，用来开发、发布和部署的一种工具。容器镜像是一个轻量级、独立、可执行的软件包，包含运行它所需的所有内容：代码、运行时、系统工具、系统库、设置等。

因为Docker是基于LXC（Linux Containers），并使用了一些已经存在了很长的时间并且被广泛应用的技术来实现的。Docker 主要是将这些复杂、琐碎的底层技术服务化了。之所以能够获得如此巨大的成功，受到如此多用户的青睐，因为它能够更加了解用户的需求，且足够的简单方便。

docker save命令将一个本地存在的镜像打包成一个tar文件

### 1.2 Docker核心原理

Docker 客户端与 Docker 守护进程通信，Docker守护进程负责构建、运行和分发Docker容器。Docker客户端和守护进程可以在同一个系统上运行，也可以将Docker客户端连接到远程Docker守护进程。Docker客户端和守护进程使用REST API通过UNIX套接字或网络接口进行通信。

Docker 主要是使用了一些已有的技术实现的，主要的核心技术是 Cgroup+Namespaces和UnionFS。

命名空间（Namespaces）是 Linux 系统提供的一种内核级别的环境隔离方法，Docker 就是利用的这种技术来实现容器环境隔离的。Linux Namespace提供了对UTS、IPC、mount、PID、network、User等的隔离机制。

IPC Namespace主要提供了进程间通信的隔离能力。同样可以用Go语言实现IPC Namespace的创建。代码与创建 UTS Namespace 的基本相同，只要把标识符 CLONE_NEWUTS 换成CLONE_NEWIPC即可。

PID Namespace用来隔离进程。同样的进程在不同的PID Namespace下拥有不同的PID，通过代码创建一个新的PID Namespace，同样只需要把UTS Namespace的代码做少量修改，把标识符修改为CLONE_NEWPID即可。

Network Namespace在Docker中被用来隔离网络。Network Namespace可以让每个容器拥有自己的网络设备、端口、IP地址等。因为其网络是完全隔离的，所以每个Namespace下的端口不会产生任何冲突。

Mount Namespace用来隔离各个进程看到的挂载点视图。在不同Namespace中的进程看到的文件系统层次是不一样的，不同的 Namespace 进行 mount 和 umount 操作只会对自己的Namespace内的文件系统产生影响，对其他的Namespace没有影响。

User Namespace 可以用来隔离用户的用户组ID。在不同的User Namespace下进程的User ID和Group ID是不同的。

Cgroups是Linux系统中提供的对一组进程及其子进程进行资源（CPU、内存、存储和网络等）限制、控制和统计的能力。Cgroup可以直接通过操作Cgroup文件系统的方式完成使用。

UnionFS（联合文件系统）是把不同物理位置的目录合并到同一个目录中的文件系统服务。

写时复制是一种可以有效节约资源的技术，它被很好地应用在Docker镜像上。其思想是如果有一个资源需要被重复利用，在没有任何修改的情况下，新旧实例会共享资源，并不需要进行复制，如果有实例需要对资源进行任何的修改，并不会直接修改资源，而是会创建一个新的资源并在其上进行修改，这样原来的资源并不会进行任何修改，而是与新创建的资源结合在外，表现为修改后的资源状态。这样做可以显著地减轻对未修改资源的复制而带来的资源消耗问题。下

Docker镜像是由一层层的只读层组合而成的。镜像层的内容存储在/var/lib/docker/aufs/diff目录下，在/var/lib/docker/aufs/layers目录下则存放着对应的metadata，描述镜像需要的层。

再看第二条命令中一共修改了/usr、/sbin、/etc、/var这些目录文件，正好和第二层修改的文件目录相同，剩下的每一层的目录也正好和Dockerfile中修改的文件目录一一对应。由此可以确定，在Build镜像中对文件的每一次修改都会增加一个文件层包含着对应修改后的文件。而当进行删除文件操作时，也会创建一个新的层，在新层里创建一个特殊名称隐藏文件，这样的一个隐藏文件对应着一个文件的删除。

发现多了两个文件层，其中的init文件层中主要存放与容器内环境相关的内容，这是一个只读文件层，另外一个文件层是一个可读写文件层，用来存储容器的写操作。当容器内部文件发生任何改变时，改变后的文件就会存储在这层文件系统中，当容器停止时它们仍然是存在的，只有容器被删除时才会删除这两个文件层。

### 1.3 Docker镜像及镜像仓库

Docker 镜像是由一层层的文件系统组成的特殊文件系统，为容器提供了运行时所需要的程序、配置、环境等。镜像属于只读文件系统，不能包含任何的动态数据，在构建完成后就不会被改变。

构建Docker镜像的方式有两种，一种是通过docker commit方式，另一种是通过Dockerfile文件构建的。相比之下，推荐使用Dockerfile的方式构建，因为Dockerfile方式简化了镜像的构建过程，并且可以更好地进行版本控制。

Libnetwork提供了Bridge、Host、Overlay、Remote和Null五种网络模式。
1）Bridge驱动：此驱动为Docker的默认驱动，使用该驱动时，Libnetwork将创建出来的Docker容器连接到Docker网桥上，能够满足容器的基本使用性需求。
2）Host驱动：在该驱动模式下，Docker容器直接使用宿主机的网络，与宿主机享有完全相同的网络环境。

5）Null驱动：在这种网络模式下，Docker仅仅为容器创建自己的Network Namespace，提供网络隔离的功能，但并不会为容器创建任何网络配置，需要用户根据自己的需求为容器添加网络，并配置网络环境。

但是因为每增加一个端口映射，宿主机就会多出一个docker-proxy进程，一旦需要过多的端口映射，就需要增加过多的docker-proxy进程，这样将会消耗大量的资源。因此，在1.7及更高版本中，Docker提供了一种完全由iptables DNAT实现的端口映射，但docker-proxy的方式依旧是默认方式，通过配置-userland-proxy=false来选择iptables DNAT模式。

### 2.1 Kubernetes概述

直到1998年，VMware成立并首次引入X86的虚拟技术，真正迎来了云计算的时代。亚马逊云计算服务于2006年推出，在随后的十年间，它可以说趁着云计算趋势改变了整个IT行业。2010年，开源云计算IaaS平台OpenStack发布，在多家主流公司的推动下迎来了爆发，越来越多的企业利用OpenStack搭建公有云和私有云平台。2013年，dotCloud公司开源了容器引擎Docker，相比于虚拟机，Docker的秒级启动、环境标准化、轻量级、资源利用率高、快速扩缩容等优势立刻引起了行业的关注。

使用Kubernetes究竟有什么好处呢？
➢ 拥抱微服务。微服务架构将巨大单体式应用分解为多个服务，每个服务通过RPC或者API进行通信，具备了各个自服务容易开发、维护的优点，另外子服务还具备独立部署、快速扩展等优点。Kubernetes 对微服务本身有很好的支持，应用本身通过Deployment进行部署，各个服务运行在Pod中，Pod之间服务通过Service具备的服务发现实现相互间的通信。同时微服务架构也恰好是云原生应用的一种体现。

➢ 容器编排。Kubernetes帮助使用者通过简单易用的API高效地管理成千上万个运行在容器中的微服务。

Kubernetes 还具备包括监控、日志、包管理等各种完善而专业的工具链，这将大大减轻运维和开发人员的负担，传统动辄十几人的团队，采用Kubernetes方案之后只需精湛的小团队就能应付自如。

Kubernetes 同时提供了 DNS 和环境变量两种方式，帮助实现服务的注册和发现，Kubernetes早期版本中使用的环境变量方式实现，现在Kubernetes则默认使用DNS，通过使用DNS将服务名称解析为服务的IP地址，然后Proxy转到对应的Pod。

### 2.4 Kubernetes网络

Kubernetes是一个跨主机的容器编排管理平台，

➢ Serverless 架构属于平台即服务，针对事件驱动、短暂性的工作负载。它结合了 FaaS与其他云服务（数据库、消息队列等）构建复杂的系统，并依赖Faas提供的计算服务功能。

2）Serverless具备以下特性。
➢ 无运维管理：部署代码无须进行预先和完成之后的操作，整个部署过程无须关注网络、监控、调度、资源等各个层面。
➢ 自动伸缩：无须预先写定脚本实现扩容或者缩容，平台具备根据请求负载的情况自动伸缩函数实例。
➢ 按需付费：传统的方式应用部署上去，无论访问量或者负载高低，都需要计费。而使用Serverless，用户仅需要为函数真正调用到的次数和执行时间付费。
➢ 更高的效率：由于只关注代码本身，部署将更加快速；以函数作为实例，粒度更小，

3）Serverless面临的困难。
➢ 无状态：函数实例执行完会被销毁，需要借助外部数据库或网络存储管理状态。
➢ 函数执行时间限制：例如，AWS Lambda限制函数执行时间最长为5分钟。
➢ 启动延迟：针对应用不活跃以及对瞬时流量增长的情况延迟会更明显。
➢ 平台依赖：比如服务发现、监控、调试、API 网关等都依赖于 Serverless 平台提供的功能。

Kubeless是基于Kubernetes之上的Serverless平台，依托于Kubernetes强大的功能和生态，Kubeless 在 Serverless 架构平台中备受关注，同时 Kubernetes 用户也很容易使用 Kubeless。Kubeless 采用函数实例部署、事件驱动的模式。Kubeless 使用 Kubernetes Custom Resource Definition（CRD）实现了Kubeless自定义的资源，Kubeless通过运行一个控制器来管理自定义资源，用于runtime的启动，代码动态地注入。

Kubeless支持的触发器包括HTTP触发器、Kafka消息队列触发器、定时触发器、nats触发器。

➢ 运行时：表示函数运行的代码语言类型和代码运行所需要的特定环境。目前 Kubeless支持的语言类型包括Python、Node.js、Ruby、PHP、Golang。
Kubernetes上如何承载Kubeless：
➢ 使用CRD定义函数对象。
➢ 每个事件驱动源封装成单独的触发器CRD对象。
➢ 独立的CRD controller处理Kubeless中CRD的增删改查操作。
➢ 使用Deployment/Pod运行Kubeless运行时。
➢ 使用ConfigMap存放函数的代码给运行时的Pod。
➢ 使用init-container处理将函数需要的依赖。

➢ 使用Service暴露函数，使用Ingress将函数对外部暴露。

### 第3章 美丽联合容器云实践

基于Kubernetes和Docker容器云平台的技术方案、架构演进的三个阶段，以及在稳定性、效率和成本三方面所做的工作

### 3.1 “从零到一”：容器云平台的技术演进

目前，全站超过 90%!的(MISSING)业务都运行在虚拟化平台上，正朝着全容器化的目标迈进。虚拟化团队也是美联全站业务最底层的基础平台团队之一。

美联从2014年年底开始建设私有云平台，希望能够实现多机房、多集群的统一资源管理，并采用虚拟化的技术提高稳定性、提升资源利用率和资源交付的效率。

➢ 集群管理。Kubernetes和Mesos在2014年年底时还不够成熟，当时OpenStack作为云计算领域最著名的开源项目，已经有了超过60%!的(MISSING)市场占有率。它的功能完备，设计理念先进，具有很好的可扩展性，方便二次开发。因此，我们选择了OpenStack+Docker的方案实现第一阶段的容器化。

美联建设基于 Kubernetes 的容器云平台，目标就是希望能够提高研发人员从代码构建到业务上线的整体效率。这个过程是一个完整的CI/CD流程，包括从开发人员提交代码，进行代码 Review，到代码编译、单元测试、集成测试，最后构建出业务镜像，在不同的环境中部署业务，上下线服务等各个环节，形成统一的应用生命周期管理。

左侧是持续集成框架，其中由应用基线管理、代码编译、单元测试、发布系统等模块组成。


业务的最上层是由 SLB 软负载统一管理 DNS、Nginx、LVS 等组件，将业务流量转发给不同Kubernetes集群中的业务容器。基于Kubernetes的CaaS由多集群管理、镜像管理、权限管理、服务发现等子模块组成。

应用标准化的目标，是让开发人员能更容易地实现自助式的开发、部署、上线工作，业务方越来越不需要关注应用是如何部署、服务是如何被发现和接入等细节。整个环节都是自动化的，运维人员尽量少介入，甚至不介入。

应用在开发、预发、线上等环境中的配置信息

针对硬件最有效的监控方式是 syslog 日志监控。比较常用的硬盘坏道、内存故障，都可以通过监控/var/log/messages中的内核关键字提前发现问题。
我们在日志监控中增加了很多syslog内核关键字的监控，比如hung_task、out of memory、segfault、Machine Check Exception。后续更好的做法是设置/etc/rsyslog.conf，把内核日志写入专门的文件中，比如/var/log/kernel.log中，监控该文件即可，如图3-19所示。

TCP 重传率反映了一台主机的网络处理繁忙情况，通过该指标可以快速地定位很多与网络有关的疑难问题，比如网络大面积抖动、应用无故RT升高等。

应用oom事件告警。我们会在计算节点部署监控的agent，通过捕获Docker的事件监听oom 的发生，然后给相应应用负责人发送告警。Docker 底层会定期监控/sys/fs/cgroup/memory/docker/docker pid/memory.oom_control中的under_oom字段，发生oom时，内核会自动将under_oom标志从0设成1。

通过命令行docker events-f event=oom，也能观察到oom事件的发生。

健康巡检脚本每天会自动核对Kubernetes和CMDB中的容器信息，统计搜集剩余可用计算、网络、存储资源、节点上不正常容器等信息，这样日常值班的同事可以第一时间处理，避免潜在问题的发生。

要避免这些线上事故的发生，必须要约束和加强操作流程的规范化和体系化，为此制订了专门的线上操作规范，明确了操作的红线。尤其重要的是自身发布的标准化，既能提高生产效率，又能降低线上变更导致的事故发生概率。

除了稳定性，效率的提升也是很重要的一方面。效率可以包含机器使用的资源效率，也就是成本，也可以包含人员投入的效率，这里主要的投入是指定位和解决线上疑难问题的工作。除不断地增强基础监控的能力，完善主动通知能力以外，我们还做了以下事情：

2）建立业务答疑群。充分利用 IM 工具的能力，通过创建答疑账号和答疑群，可以更高效地与业务方沟通，快速解决问题。

3）形成问题排查指南。在帮助业务方从系统层面定位各种业务问题的过程中，我们总结了不少经验和方法，把这些思路分门别类地整理汇总，最终形成了一个《系统问题定位排查指南》。

我们面向全公司发布了这一指南，最终是希望业务方能够自助式地快速定位系统问题。同时，通过建设一套系统问题分析定位平台，来沉淀问题定位排查工具

降低机器成本和运维成本是一项长期而艰苦的工作。在业务高速发展的过程中，很可能会忽视机器成本给公司运营带来的压力。一旦开始降低成本，会发现有很多降低的空间，在这个过程中，我们也总结了几条降低成本的最佳实践和经验。

因此，需要购买大量的机器应对这些峰值，峰值压力过后，机器资源有很大的闲置和浪费。因此，在大促期间动态地租用部分公有云资源，在平时只需保留满足日常流量的机器池即可。

### 3.2 “自我突破”：关键技术方案和创新点

对于容器来说，内核的选型是非常关键的，因为大量容器的隔离性和稳定性的提升都依赖内核。
美联的内核从CentOS6的2.6.32，演进到CentOS7自带的v3.10，又演进到了4.4.95。之所以选择4.4.95版本，是因为它对OverlayFS的支持更加完善，支持pid_max的隔离，也是社区长期支持的稳定版本。

Kube-controller-manager 周期性检查节点状态，每当节点状态为 NotReady，并且超出podEvictionTimeout 时间后，就把该节点上的 Pod 全部驱逐到其他节点，其中具体驱逐速度还受驱逐速度参数、集群大小等的影响。最常用的2个参数如下。

以内存资源为例，当内存资源低于阈值时，驱逐的优先级大体为 BestEffort＞Burstable＞Guaranteed，具体的顺序可能因实际使用量有所调整。当发生驱逐时，Kubelet 支持 soft 和hard 两种模式，soft 模式表示缓期一段时间后驱逐，hard 模式表示立刻驱逐。

对于Kubelet发起的驱逐，往往是资源不足导致，它优先驱逐 BestEffort 类型的容器，这些容器多为离线批处理类业务，对可靠性要求低。驱逐后释放资源，减缓节点压力，弃卒保帅，保护了该节点的其他容器。无论设计方面，还是实际使用情况方面，该特性都非常好。

Kubernetes自身并不包含网络模块，业界常见的网络方案有Flannel/Calico 等。

通过合理的分层来提高部署效率。将通用性强、公用广泛的组件放到下层，将经常变化的组件放到上层。下层是容器间共享的，更好的复用下层是提升部署效率的关键。另外，由于Docker镜像中心的技术实现，下载每层镜像都会进行 MD5 校验，因此镜像层数不宜过多，否则多次MD5校验会影响下载镜像的效率。

逻辑上又可以细分为两层。目前只提供CentOS7的系统，最下层为CentOS 7.2自带的系统包，上面是公司需要的基础运维工具，例如ldap、curl、tsar，以及DNS配置、内部账户等。

编程语言依赖的基础镜像层，每种语言有不同的依赖环境，例如 Java、C++、PHP、Go等。每种语言也可能对应不同的依赖，例如Java JDK 1.7、Java JDK 1.8。
应用按照编程语言分类，找到对应的language_base层的依赖。

应用的特殊第三方依赖，以及公司内部的常用依赖组件，例如监控Agent、运维Agent、安全Agent等。

由于美联研发的时间比较早，并没有采用业界比较流行的Harbor方案，而是用Go 语言开发了自研的镜像管理中心，我们叫它Lens。

Lens主要支持以下功能：与应用配置中心对接，获取应用配置信息，自动生成Dockerfile；自动构建应用Docker镜像，并推送至不同环境的Docker Registry；提供面向用户的Docker镜像管理UI，提供镜像的快速搜索、详情展示、查询构建日志、删除等功能；支持两个跨机房的Docker Registry之间进行镜像同步，比如开发环境的镜像同步至生产环境，支持定时同步；监听Docker Registry的拉取或推送事件，对接基于LDAP的权限管理模块，对镜像的增删改查进行控制。

➢ 镜像跨机房同步
要能实现Docker Registry之间的同步，必须了解Docker Registry v2存储镜像的格式。Registry使用两个对象记录一个镜像：manifest用于描述镜像的信息；blob是镜像的数据块，也就是镜像真正的内容，保存在文件系统中。

镜像在Registry之间的同步，实际上是manifest和blob的同步。镜像同步的步骤如下：获取源端镜像的 manifest；获取源端镜像的 blob 列表；在目的端 Registry 检查是否存在这些blob，已存在的blob不需要同步；从源端同步目的端不存在的blob；同步manifest；验证镜像是否同步成功。

Kubernetes 原生提供 readiness/liveness probe 的能力，为保证业务的可靠性，美联使用readiness probe，提供了针对业务四层（TCP）及七层（HTTP）的健康检查规则，用户可以针对自身的需求，指定检测的端口的可用性，来保证对应Pod的IP是否可以健康地注册进对应的Kubernetes Service内，继而被Radar发现并注册进SLB。

7. 弹性能力
2018年1月，美联旗下所有业务迁移至腾讯云黑石机房，租用腾讯云物理机。美联在黑石机房维持一个基础资源池，应对日常流量。在大促场景下，按需租用腾讯公有云云主机资源，将业务水平扩容到公有云环境。
公有云按需收费，弹性能力强。为了最大化节约成本，势必要求快速地从腾讯公有云获取海量资源并完成节点初始化，快速完成业务的部署，这要求：

### 3.3 总结

无论是容器还是 PaaS，始终服务的是公司的业务，因此一切要以服务好业务为工作的出发点，这也是衡量平台成功与否的核心指标。

对业务来说，稳定性是第一位的，而改造业务意味着风险，所以在推广的过程中，不能追求大而全，要逐步推广。要采取“农村包围城市”的策略，寻找对容器感兴趣的业务方，找到合适的切入点，最好是他们想解决的痛点问题。然后以业务驱动的方式设定工作目标，层层推进。只有在帮助业务方真正解决了问题后，平台才会有生命力，后续的推广和平台建设也会水到渠成。

在实际的生产环境上，容器的隔离性是遇到最多的问题，比如磁盘异步I/O默认不支持隔离，最大进程数pid_max不支持隔离等，很多其实是内核层面由于历史原因，没有考虑过应用的容器化导致的。可以预见更强的容器隔离性，更好的稳定性是社区努力的方向，也确实诞生了一些有代表的开源项目。

Docker镜像一般是用Dockerfile进行构建和管理的，但这种镜像构建方式在业界也有不同的声音，认为它有一系列的问题，比如：
1）同一份Dockerfile在不同时间点打包出来的镜像可能是不同的，比如yum install的软件版本不同，二进制数文件是不同的。
2）镜像之间的依赖性问题。基础镜像如果更新了，所有依赖的镜像都需要重新构建，如果要替换线上的实例，需要重新发布所有的实例。
3）镜像里的操作系统和应用是静态绑定的关系。容易让用户把Container镜像变成“虚拟机”。
4）镜像内容的可视化不足。一旦打完了镜像，很难知道里面到底打包了什么内容。

➢ tcpretrans：实时跟踪并展示发生TCP重传的TCP连接。当本机的重传率上升时，它能很方便地展示出对方服务器的IP地址，从而快速定位问题。
➢ iosnoop：跟踪进程（含应用名和PID）的磁盘I/O请求量及I/O响应时间。
➢ iolatency：图形化展示磁盘I/O时间消耗分布。
➢ killsnoop：跟踪Linux内核进程间发送的信号，在进程异常退出时，可以找到哪个进程发出了什么信号导致它退出。

### 4.1 架构挑战与应对方案

由于业务流量与复杂性的双重爆发性增长，最早的单体架构已经难以为继，酷家乐开始了全站的服务化改造。此后一年半内，随着服务化改造的逐步深化，服务粒度不断变小、数目不断膨胀。

Kubernetes解决了三个层面上的问题。
1）容器化：将应用的交付模式从传统的二进制包转换到标准镜像，如Docker Image。这使得应用的运行环境和应用本身绑定，大大简化了应用的交付与运维。
2）调度：解决了如何为容器分配运行时资源的问题。轻量级的虚拟化技术外加统一资源调度，大大提升了部署密度，优化了资源利用率。同时，对于DevOps的支持更加彻底。
3）服务抽象：Kubernetes通过Deployment、Service等概念构建了一个完善的微服务体系。同时，用声明式API将部署、扩容、缩容等操作标准化，让CI/CD与监控报警等周边系统的整合变得非常简单。

这一阶段的目标是用Kubernetes解决资源利用率和DevOps两大问题。

### 第5章 个推基于Docker和Kubernetes的微服务实践

基于OpenResty和Node.js搭建了微服务框架，提高了开发效率。在微服务的基础上，结合Docker实现了容器化，并采用Consul进行服务注册及发现。后来面对日渐增多的微服务和配置，采用Kubernetes实现了容器编排。

### 5.1 微服务

传统的单体架构的缺点为开发成本高、可维护性差、技术选型不灵活、伸缩性差。

微服务架构强调把因相同原因而变化的东西聚合在一起，把因不同原因变化的东西分离开来。微服务架构如图5-2所示，特点是：小，专注，一个服务负责一项业务；自治，服务独立部署、升级、扩展；技术异构，服务独立技术选型和开发；服务间松耦合，服务内高内聚；可与组织结构相匹配。

一般来说，合理的微服务架构，团队应该可以在两周内开发完成。这样的微服务更适合小团队开发，也可以避免代码库过大。

在特定功能块出现运算、存储等负载瓶颈的时候，相比传统的单体架构，微服务架构可以独立地进行实例扩展，而不是整体扩容。微服务对语言选择友好，这允许开发者用更低的成本来尝试新的技术。微服务不强制必须采用特定的开发语言，不同的服务可以选择不同的语言实现，比如作为Web后端的业务系统可以选择Node.js进行开发，更后端可以选择Java语言实现支持更高并发或者计算服务。

在微服务化过程中，需要将一个产品服务拆分成多个独立微服务，首先要解决如何服务注册和发现的问题

服务提供方仍然向注册中心注册服务，服务调用方必须通过网关才能访问微服务开放的接口。对

外部调用方必须通过网关才能调用服务，而内部微服务之间直接通过注册中心发现服务并进行调用。

随着前端逐渐转向SPA，团队在Java语言和前端之间增加了一层很薄的Node.js层实现鉴权、协议转换、路由、接口组合等功能。

### 5.2 容器化

个推选择了Docker进行容器化实践，首先搭建Docker集群环境。选择了Calico作为网络插件，Calico是一套基于路由（BGP）的SDN，通过路由转发的方式实现容器的跨主机通信。Calico有如下的特点：
➢ 纯三层的数据中心网络方案；
➢ 利用Linux Kernel实现了一个高效的vRouter来负责数据转发；
➢ 可以直接利用数据中心的网络结构，不需要额外的NAT、隧道或者Overlay Network；
➢ 提供丰富而灵活的网络Policy（基于iptables）；
➢ 支持很细致的ACL控制；
➢ 性能高、可控性高、隔离性好。

Kubernetes提供了一个插件Kube-DNS可以将Service注册到DNS，直接通过Service的名字即可访问Service，推荐安装。

基于Kubernetes的Service和Kube-DNS，个推重新设计了产品服务的注册与发现机制。所有的服务通过Kube-DNS注册到DNS上，不再注册到Consul上，服务之间的调用直接通过Service名字访问，负载均衡直接由Service提供，不再由API网关实现。由于Service的域名只在Kubernetes集群内部有效，在集群外部并不能直接访问，为了使Apiate能够对外暴露服务，Apigate的服务注册和发现还是通过Consul实现，前置Nginx直接从Consul上获取Apigate的上暴露的IP和端口，映射到主机端口上，实现服务的对外暴露。

未引入 Kubernetes 之前，容器和集群的监控和日志处理分析是一件很难的事情，一般需要在每个容器中部署一个Agent收集监控指标和日志，然后统一汇报给监控或日志中心进行处理。对于一个没有运维经验的开发者来说，部署这样一套监控或日志处理系统是一件很麻烦的事。

Kubernetes 中常用的日志处理系统是 EFK 即 Elasticsearch、Fluentd、Kibana，如图 5-11所示。其中，Elasticsearch存储日志并提供搜索，Fluentd负责收集日志，需要部署到应用Pod中，但是不会侵入应用容器，Kibana负责查询和展示日志。

### 5.4 总结

Kubernetes也有ConfigMap实现配置中心的功能。一方面Kubernetes提供的插件，很好地满足了服务监控、日志处理等需求；另一方面，Kubernetes还提供了强大的功能进行容器的管理，使得容器调度、扩容缩容、故障恢复都能很方便地实现。

### 6.1 Prometheus简介与使用

➢ push网关：用来支持短期的任务。
➢ Alertmanager：处理告警。

Prometheus从被Client Library构造的任务获取Metrics，可以直接使用Pull方式，或者对于短期的任务可以通过Pushgateway。Prometheus将抓取来的sample存储到本地，然后通过rules聚合及存储新的数据，或者产生告警。Grafana或其他一些客户端可以通过调用Prometheus API图形化地显示这些数据。

➢ rule_files：用于指定Prometheus需要加载哪些文件，当前设置为空。只加载一次，执行计算的频率是由上面的evaluation_interval指定，每个rule里面也可以配置进行覆盖。

本地存储方式不建议用作长期存储。
第三方存储系统：考虑到本地存储的可扩展性和可靠性，Prometheus没有开发长期存储系统，而是用了一系列接口和第三方存储集成。

如果想调优每秒采样速度，可以减少抓取量，减少target或者每个target的时序。或者提高抓取频率的值，间隔更久。当然，减少时序是更有效的方法。

应避免很慢及很费力的查询，如果一个查询需要大量的数据，则对其画图就需要占用Server和浏览器资源。如果确实需要很多计算，则建议用Recording rule。如果需要很多数据进行聚合，即使产生少量的新时序也会占据Server资源。

### 7.1 为何选择Docker

微服务天然需要自动化工具来降低发布部署的出错率，提高产品迭代的效率

### 9.1 现有维护中的瓶颈

运维人员需要通过排查故障积累更多的知识与经验，而维护本身又希望故障越少越好，两者有根本性的矛盾。现在的智能运维也需要大量的故障处理实例进行学习，所以故障模拟环境是一种很好的方式。

### 9.3 环境的搭建与CI/CD

使用kubeadm初始化Kubernetes集群。
￼
这时，kubeadm 会初始化各种环境配置，包括部署证书（private ca）和密钥，然后调用docker pull相应的镜像。

Kubernetes环境下的CI/CD整体架构，其具体流程如下：
1）开发人员向GitLab提交代码，代码中必须包含Dockerfile。
2）Jenkins的CI流水线自动地从GitLab中拉取代码。
3）交给Maven编译代码并打包。

4）Jenkins把打包后的Docker镜像推送到Harbor镜像仓库。
5）Jenkins的CI流水线中包括了自定义脚本，根据已准备好的Kubernetes的YAML模板，将其中的变量从QConf中拉取替换选项。
6）将Jenkins发布版本到Kubernetes cluster中，会用到生成的Kubernetes YAML配置文件和项目Docker镜像。
7）Kubernetes cluster会更新Skydns和Ingress/traefik的配置，根据新部署的应用名称，在ingress/traefik的配置文件中增加一条路由信息。
8）Jenkins调用外部的DNS服务，更换DNS纪录，以解决外部的访问。

### 9.4 存储引擎的选择

容器是在镜像基础上加一层可写容器层运行的，所以选择合适的存储引擎关系着容器运行的稳定性和性能。

➢ AUFS、Overlay和Overlay2文件级别的存储，更有效的利用内存，但容器的Writable Layer再写I/O压力大时增加较快，Layer增加读写性能会下降。

### 9.5 Kubernetes日志收集

应用的日志可用于排查线上问题、记录访问信息、进一步统计和分析

➢ 服务还是把日志写入磁盘，由外部程序完成收集入库，类似于Flume NG+Kafka+ELK的方案。
➢ 服务直接把日志写入远端，如Elasticsearch、Kafka、MongoDB。

常见的情况有日志存储端异常时，服务系统大量写入日志报错，频繁操作 I/O，业务性能下降；日志存储端异常时，大量的日志对象占满内存，导致oom；日志存储端异常时，耗尽连接池，堵塞或影响服务请求。

最后选择了Flume NG+Kafka+ELK方案，架构如图9-2所示。

直接一个Pod绑定一个收集程序又太重了。

### 第10章 沃趣科技的容器化RDS之路

“你不是不够好，你只是过时了。”这句话用在IT行业特别合适，每隔一段时间都会有新的技术出现，让程序员们应接不暇。

结合用户对于数据库运维自动化的要求越来越高，数据库即服务（DBaaS or RDS）的需求越来越大，与大家分享沃趣在对容器化技术的引入及应用后，对下一代数据库运维架构的理解和目前正在做的工作。

### 10.1 容器化RDS：计算存储分离架构下的“Split-Brain”

本文尝试结合 Kubernetes、Docker、MySQL和计算存储分离架构，分享遇到的诸多问题之一“Split-Brain”。
当前业界的数据库技术发展趋势是：数据库容器化作为下一代数据库基础架构，基于编排架构管理容器化数据库，采用计算存储分离架构。

这套架构也是更加简单、通用、高效的High Availability方案。当集群中某个Node不可用时，借助Kubernetes的原生组件Node Controller、Scheduler和原生API StatefulSet即可将数据库实例调度到其他可用节点，以实现数据库实例的高可用

➢ Kubelet借助API Server定期（node-status-update-frequency）更新etcd中对应节点的心跳信息。
➢ Controller Manager中的Node Controller组件定期（node-monitor-period）轮询etcd中节点的心跳信息。
➢ 如果在周期（node-monitor-grace-period）内心跳更新丢失，该节点标记为 Unknown （ConditionUnknown）。

➢ 如果在周期（pod-eviction-timeout）内心跳更新持续丢失，Node Controller 将会触发集群层面的驱逐机制。
➢ Scheduler将Unknown节点上的所有数据库实例调度到其他健康（Ready）节点。

无法判定节点真实状态。心跳更新是判断节点是否可用的依据，但心跳更新丢失是无法判定节点真实状态的（Kubernetes中将节点标记为Condition Unknown也说明了这点）。Node可能仅仅是网络问题，CPU繁忙、“假死”、Kubelet Bug等原因导致心跳更新丢失，但节点上的数据库实例还在运行中。

➢ 通过sysbench对该实例制造极高的负载，“k8s-node3”load飙升，导致“k8s-node3”上的Kubelet无法与API Server通信，并开始报错。
➢ Node Controller启动驱逐。
➢ StatefulSet发起重建。
➢ Scheduler将MySQL实例调度到“k8s-node1”上。
➢ 新旧MySQL实例访问同一个Volume。
➢ 数据文件被写坏，旧MySQL实例都报错，并无法启动。

两个数据库实例写同一份数据文件，data corruption，两个节点都无法启动。

### 10.2 容器化RDS：计算存储分离架构下的I/O优化

存储层由分布式文件系统组成，以Provisoner的方式集成到Kubernetes。在我们看来，计算存储分离的最大优势在于：
将有状态的数据下沉到存储层，这使得RDS在调度时无须感知计算节点的存储介质，只需调度到满足计算资源要求的Node。数据库实例启动时，只需在分布式文件系统挂载mapping的volume即可，可以显著地提高数据库实例的部署密度和计算资源利用率。

相比本地存储，网络开销会成为I/O开销的一部分，会带来两个很明显的问题：
➢ 数据库是Latency Sensitive型应用，网络延时会极大地影响数据库能力（QPS，TPS）。
➢ 在高密度部署的场景，网络带宽会成为瓶颈，可能导致计算资源和存储资源利用不充分

在计算存储分离架构下，当集群发生脑裂，并触发Node Controller和Kubelet的驱逐机制时，可能会出现多个数据库实例同时访问一份数据文件，导致数据坏块的情况，数据的损失对用户而言是不可估量也是不可忍受的。