## Nginx应用与运维实战
> 王小东

### 前言

同时基于其开源版本衍生出来的OpenResty和淘宝的Tengine等软件也根据自身需求提供了优秀的扩展功能，满足了云计算、微服务等各种技术的应用需求，并在实际生产环境中得到了广泛应用。

作为一款Web服务器软件，Nginx实现了Web服务器的基本功能，用户通过简单的配置指令就可以快速完成Web服务器的搭建。它还是网络通信协议处理软件，支持TCP/UDP、HTTP、HTTP/2、gRPC、FastCGI、SCGI、uWSGI、WebDAV等协议的处理，并实现了相应通信协议的请求解析、长连接、代理转发、负载均衡、会话保持等互联网架构中常见的应用功能。

本书分别从Nginx介绍、应用实战、运维管理及与Kubernetes和微服务的应用集成4个部分来介绍Nginx的特点及运维管理实战经验，力求给从事互联网技术工作的读者带来帮助。

全书所涉及的软件部署均采用了Docker化的部署方法，不仅充分利用了Docker容器的便捷部署方式，还满足了目前容器化运维管理工作的技术需求。

### 第1章 Nginx概述

2019年3月，著名硬件负载均衡厂商F5宣布收购Nginx，Nginx成为F5的一部分。F5表示，将加强对开源和Nginx应用平台的投资，致力于Nginx开源技术、开发人员和社区的发展，更大的投资将为开放源码计划注入新的活力，会主办更多的开放源码活动，并产生更多的开放源码内容。

### 1.1 Nginx的不同版本

目前国内流行的Nginx主要有两个开源版本，分别是由淘宝网技术团队维护的Tengine项目和由章亦春发起的OpenResty项目。

### 1.1.1 开源版Nginx

Nginx自推出以来，一直专注于低资源消耗、高稳定、高性能的并发处理能力，除了提供Web服务器的功能外，还实现了访问代理、负载均衡、内容缓存、访问安全及带宽控制等功能。其基于模块化的代码架构及可与其他开发语言（如Perl、JavaScript和Lua）有效集成的可编程特性，使其具有强大的扩展能力。


部署和优化具有高效率、高性能并发请求处理能力的应用架构是应用架构师一直追求的目标，在应用架构技术的迭代中，各种分离式思想成为主流，比如将访问入口和Web服务器分离、将Web服务器和动态脚本解析器分开、将Web功能不断拆分、微服务等。Nginx不仅提供了Web服务器的功能，还极大满足了这一主流架构的需求并提供了如下应用特性

Nginx可以通过访问路径、URL关键字、客户端IP、灰度分流等多种手段实现访问路由分配。

反向代理功能而言，Nginx本身并不产生响应数据，只是应用自身的异步非阻塞事件驱动架构，高效、稳定地将请求反向代理给后端的目标应用服务器，并把响应数据返回给客户端。其不仅可以代理HTTP协议，还支持HTTPS、HTTP/2、FastCGI、uWSGI、SCGI、gRPC及TCP/UDP等目前大部分协议的反向代理。

### 1.1.2 商业版Nginx Plus

（2）动态管理
·支持通过API清除内容缓存。
·可通过API动态管理上游的后端服务器列表。
（3）安全控制
·基于API和OpenID连接协议单点登录（SSO）的JWT（JSON Web Token）认证支持。
·Nginx WAF动态模块。

（4）状态监控
·超过90个状态指标的扩展状态监控。
·内置实时图形监控面板。
·集成可用于自定义监控工具的JSON和HTML输出功能支持。

### 1.1.3 分支版本Tengine

Tengine的最新版本是2.3.2，在继承Nginx 1.17.3版本的所有功能的同时，也保持了自有的对Nginx的优化和增强，其增强特性如下。


### 1.1.4 扩展版本OpenResty

OpenResty®是一个基于Nginx与Lua的高性能Web平台，其内部集成了大量精良的Lua库、第三方模块以及大多数依赖项，以便搭建能够处理超高并发、扩展性极高的动态Web应用、Web服务和动态网关

### 1.2 Nginx源码架构浅析

处理能力，来源于其优秀的代码架构。它采用了多进程模型，使自身具有低资源消耗的特性。以事件驱动的异步非阻塞多进程请求处理模型，使Nginx的工作进程通过异步非阻塞的事件处理机制，实现了高性能的并发处理能力，让每个连接的请求均可在Nginx进程中以工作流的方式得到快速处理。

### 1.2.1 多进程模型

进程是操作系统资源分配的最小单位，由于CPU数量有限，多个进程间通过被分配的时间片来获得CPU的使用权，系统在进行内核管理和进程调度时，要执行保存当前进程上下文、更新控制信息、选择另一就绪进程、恢复就绪进程上下文等一系列操作，而频繁切换进程会造成资源消耗。

Nginx采用的是固定数量的多进程模型（见图1-2），由一个主进程（Master Process）和数量与主机CPU核数相同的工作进程协同处理各种事件。主管理进程负责工作进程的配置加载、启停等操作，工作进程负责处理具体请求。进程间的资源都是独立的，每个工作进程处理多个连接，每个连接由一个工作进程全权处理，不需要进行进程切换，也就不会产生由进程切换引起的资源消耗问题

Nginx主进程负责监听外部控制信号，通过频道机制将相关信号操作传递给工作进程，多个工作进程间通过共享内存来共享数据和信息。

信号（signal）又称软中断信号，可通过调用系统命令kill来发送信号实现进程通信。在Nginx系统中，主进程负责监听外部信号，实现对进程的热加载、平滑重启及安全关闭等操作的响应。

Nginx的多个进程间就是通过共享内存的方式共享数据的，主进程启动时创建共享内存，工作进程创建（fork方式）完成后，所有的进程都开始使用共享内存。

接字是应用程序与TCP/IP协议通信的中间抽象层，也是一种特殊的文件，应用程序以文件描述符的方式对其进行读/写（I/O）、打开或关闭操作。每次对socket进行读操作都需要等待数据准备（数据被读取到内核缓冲区），然后再将数据从内核缓冲区复制到用户空间。

作为Web服务器，Nginx的基本功能是处理网络事件，快速从网络接口读写数据。Nginx结合操作系统的特点，基于I/O多路复用模型的事件驱动程序设计，采用了异步非阻塞的事件循环方法响应处理套接字上的accept事件，使其在调用accept时不会长时间占用进程

### 1.2.3 模块化

Nginx一直秉持模块化的理念，其模块化的架构中，除了少量的主流程代码，都是模块。模块化的设计为Nginx提供了高度的可配置、可扩展、可定制特性。

·HTTP模块（http）。该模块提供HTTP处理的核心功能和部分功能模块，HTTP核心功能维护了HTTP多个阶段的工作流，并实现了对各种HTTP功能模块的管理和调用。

·Stream模块（stream）。该模块提供TCP/UDP会话的代理和负载相关功能。

### 2.1 编译环境准备

（2）DNS缓存
编辑/etc/resolv.conf配置DNS服务器，打开NSCD服务，缓存DNS，提高域名解析响应速度。

（3）修改文件打开数限制
操作系统默认单进程最大打开文件数为1024，要想实现高并发，可以把单进程的文件打开数调整为65536。


### 2.1.2 Linux内核参数

可以通过改变/proc/sys目录下文件中的值对内核参数进行修改

内核为了不让某个进程消耗掉所有的文件资源，也会对单个进程最大打开文件数做默认值处理，称之为用户级限制，默认值一般是1024，使用ulimit -n命令可以查看用户级文件描述符的最大打开数。文件相关内核参数可参见Linux相关图书。

·SYN：建立连接标识。
·ACK：确认接收标识。
·FIN：关闭连接标识。

数据传输完毕后，TCP关闭连接流程如图2-2所示，具体说明如下。
1）发起端（图2-2中①）主动将连接关闭报文（FIN=1，编号seq=u）发送给响应端，将自己的状态更改为FIN_WAIT_1。

2）响应端（图2-2中②）返回确认报文（ACK=1，确认号ack=u+1，编号seq=v）给发起端，将自己的状态更改为CLOSE_WAIT。


5）发起端（图2-2中④）收到连接释放报文后，发送确认报文（ACK=1，seq=u+1，ack=w+1）给响应端，将自己的状态更改为TIME_WAIT，系统会在等待2倍MSL（Maximum Segment Lifetime）时间后关闭连接，释放资源。

### 2.5.2 命令行参数

·-s参数：发送信号给Nginx主进程，信号可以为以下4个。
·stop：快速关闭。
·quit：正常关闭。
·reopen：重新打开日志文件。
·reload：重新加载配置文件，启动一个加载新配置文件的Worker Process，正常关闭一个加载旧配置文件的Worker Process。


### 2.6 Nginx的Docker容器化部署

Docker是一款基于Go语言开发的开源应用容器引擎，Docker可以让用户将需要运行的应用服务和依赖环境打包在一个小体积的应用容器中，被打包的容器可以移植到任意可运行Docker环境的操作系统中，极大地缩短了应用服务编译和部署所需的时间。Docker的虚拟化机制也使得在不同操作系统环境下编译的应用服务都可运行在同一Docker宿主机中

Docker中有两个基本概念：镜像（Image）和容器（Container）。Docker使用AUFS文件系统进行文件管理，这种文件系统的文件是分层叠加存储的，镜像是存储在只读层的文件，而运行的容器则是镜像运行的实例，它的实例文件存储在可写层中，所以通常需要先通过Docker命令制作镜像，然后再通过Docker编排命令将镜像运行成容器。

### 2.6.3 Dockerfile常用命令及编写

Dockerfile是按照Docker Build语法约定的顺序结构规则脚本文件。通过Dockerfile的编写可以实现Docker镜像的自动化制作

5）ENV设置Container启动后的环境变量，使用方法如下。

6）EXPOSE设置Container启动后对外开放的端口，它只相当于一个防火墙开放端口的概念，与实际运行的服务无关，使用方法如下。

7）RUN用于在制作Image时执行指定的脚本或shell命令，使用方法如下。

RUM yum -y install net-tools

8）USER设置运行Image或Container的系统用户，使用方法如下。

USER nginx:nginx

9）VOLUME定义Image挂载点，该挂载点可被其他Container使用，且目录中的内容是共享的，将会同步更新，使用方法如下。

VOLUME ["/data1","/data2"]



11）CMD命令是设定于Container启动后执行的命令，可被外部docker run命令参数覆盖，使用方法如下。

12）ENTRYPOINT命令是设定于Container启动后执行的命令，不可被外部docker run命令参数覆盖。

### 2.6.4 Nginx Docker运行

Docker容器如果被移除，所有的修改文件同样会被删除，为了把变更的配置保存下来，需要把配置文件目录复制出来进行持久化，所以需要通过卷挂载的方式实现配置的使用和维护，脚本如下：

mkdir -p /opt/data/apps/nginx/￼

在使用docker run命令时，每次都需要使用很多参数，为了便于维护，可以用Docker-Compose工具进行容器编排，Docker-Compose是使用基于YAML语法的脚本配置文件来实现容器的运行管理的

### 3.1.2 配置文件结构

编写Nginx配置文件时，为了便于维护，也会把一些指令或指令域写在外部文件中，再通过include指令引入nginx.conf主配置文件中。例如，配置文件中把写有types指令域的mime.types文件引用到http指令域中。此处使用的是nginx.conf文件的相对路径。

### 3.2.2 进程调优

worker_priority指令值的取值范围是-20～19，数值越小，优先级越高，获得的CPU时间就越多。

### 3.3.1 初始化服务

保持连接机制可以使同一客户端的多个HTTP请求复用TCP连接，减少TCP握手次数和并发连接数，从而降低服务器资源消耗。


keepalive_timeout的设定需要根据具体的场景来考虑，最重要的是要理解保持连接的工作方式与场景需求的匹配情况。


·指令值参数valid：用于设置缓存解析结果的时间。
·指令值参数ipv6：默认配置下，Nginx将在解析域名的同时查找IPv4和IPv6地址。设置参数ipv6=off，可以关闭IPv6地址的查找。
·指令值参数status_zone：设置收集指定区域请求和响应的DNS服务器统计信息，仅商业版本有效。

### 3.3.2 HTTP请求处理

·1××（消息）：表示服务端已经接收到请求，正在进行处理。
·2××（处理成功）：表示服务端已经正确处理完客户端的HTTP请求。
·3××（重定向）：服务端接收到HTTP请求，并将其HTTP请求重定向到客户本地或其他服务器进行处理。

·4××（客户端请求有误）：客户端提交的请求不符合规范或未被授权、禁止访问等。
·5××（服务端处理出错）：服务端无法正常完成请求操作，如超时等。

### 4.3.2 图片处理

ngx_http_image_filter_module
该模块可以对JPEG、GIF、PNG或WebP格式的图片文件进行动态旋转、比例缩放及裁剪。

### 4.3.3 响应处理

模块：ngx_http_headers_module
该模块允许用户在HTTP响应头中添加Expires、Cache-Control及自定义属性字段。

### 4.3.5 gzip压缩

模块：ngx_http_gzip_module
为提高用户获取响应数据的速度，Nginx服务器可以将响应数据进行gzip压缩，在减小响应数据的大小后再发送给用户端浏览器，相对于使用户浏览Web页面，上述方式显示速度更快。要想启用响应数据gzip压缩功能，需要用户浏览器也支持gzip解压功能，目前大多数浏览器都支持gzip压缩数据的显示。

Nginx服务器接收客户端浏览器发送的请求后，通过请求头中的属性字段Accept-Encoding判断浏览器是否支持gzip压缩，对支持gzip压缩的浏览器将发送gzip压缩的响应数据。

### 第5章 Nginx Web服务应用实战

Nginx的一个主要功能是作为Web服务器提供HTTP服务，支持静态页面、动态脚本页面、多媒体等文件的响应和处理。本章的内容如下：

### 5.1 静态文件服务器的搭建

静态文件服务器是指提供HTML文件访问或客户端可直接从中下载文件的Web服务器。对于图片、JavaScript或CSS文件等渲染页面外观的、不会动态改变内容的文件，大多数网站会单独提供以静态文件服务器的方式对其进行访问，实现动静分离的架构。

### 5.1.1 静态Web服务器

在以上配置中，每个server指令域等同于一个虚拟服务器，每个location指令域等同于一个虚拟目录。

### 5.1.2 文件下载服务器

利用Nginx的诸多内置指令可实现自动生成下载文件列表页、限制下载带宽等功能。配置样例如下：

### 5.2 HTTPS安全服务器的搭建

TLS 1.0就是SSL 3.1版本。HTTPS（HyperText Transfer Protocol Secure，超文本传输安全协议）是在HTTP的基础上增加了SSL协议，为数据传输提供了身份验证和加密功能。使用HTTPS协议可验证用户客户端和服务器的身份，确保数据可以在正确的用户客户端和服务器间传输。因为HTTPS协议的数据传输是加密的，所以在传输过程中可以有效防止数据被窃取和修改，从而保障网络信息的安全。

### 5.2.2 HTTPS基本配置

HTTPS协议数据的传输是基于SSL层加密的数据，其简单模型是服务端获得客户请求后，将用私钥加密的协商数据发送给客户端。客户端先使用服务端提供的公钥解密协商数据并读取真实的内容，再用公钥加密返回协商数据并发送给服务端，完成彼此间的密钥协商。密钥协商完毕后，服务端和客户端通过协商后的密钥进行通信数据的加解密传输。私钥只存放在服务端，公钥则由所有的客户端持有。

在实际使用过程中，为提高公钥的使用安全性、防止公钥被替换，使用第三方CA机构的证书实现对服务器身份的认证和网站公钥的安全传递。HTTPS先通过非对称加密方式交换密钥，建立连接后再通过协商后的密钥与加密算法进行对称加密数据传输

服务端获得客户端HTTPS请求后，将包含网站信息及网站证书公钥的证书、服务端随机数（Server Random）及随机选择的客户端支持加密套件返回给客户端，若需要验证客户端身份，也会在此时发送相关信息给客户端。

4）客户端通过操作系统中的CA公钥解密证书获取网站证书公钥并进行网站证书的合法性、有效期和是否被吊销的验证。5）客户端用网站证书公钥将新生成的客户端随机数加密后发送给服务端，同时使用3个随机数生成会话密钥。

### 5.2.4 HTTPS会话缓存

HTTPS建立连接时传递证书及协商会话密钥会占用一定资源，为加快HTTPS建立连接的速度，提升性能，TLS协议使用了会话缓存机制。会话缓存机制可以使已经断开连接的HTTPS会话重用之前的协商会话密钥继续HTTPS数据传输。会话缓存机制有两种实现方式：会话编号（Session ID）和会话凭证（Session Ticket）。

（1）会话编号
服务端在与客户端进行数据传输时，会为每次会话生成一个会话编号，并存储该会话编号与会话协商数据。HTTPS会话中断需要重新连接时，客户端将最后一次会话的会话编号发送给服务端，服务端检查存储中该编号是否存在，如果存在就与客户端使用原有的会话密钥进行数据传输。

·服务端会存储会话编号和会话协商数据，相对会消耗服务器资源。
·当Nginx服务器为多台时，无法实现会话共享。

### 5.2.5 HTTPS双向认证配置

通常网站的HTTPS访问，都是客户端通过证书验证所访问服务器的身份，而服务器对来访的客户端并不做身份验证，也称单向认证。在一些场景中，也会增加客户端身份验证以提高数据传输的安全性，这就是双向认证。配置样例如下：

### 5.3 PHP网站搭建

PHP网站搭建是Nginx与PHP-FPM组合实现的，由于Nginx不支持对PHP动态脚本程序的直接调用或解析，所有的动态脚本程序解析都是通过调用FastCGI接口服务器实现的。FastCGI是Web服务器和动态脚本程序间的一个高速、可伸缩的接口，它采用C/S架构，将Web服务器和动态脚本解析器分离，同时启动一个或多个脚本解析器守护进程接收Web服务器的动态脚本解析请求。

### 5.3.4 FastCGI集群负载及缓存

Nginx支持后端多个FastCGI服务器的负载均衡，负载均衡有两种方式：一种是通过域名解析多个FastCGI服务器，该方式通过所有域名地址轮询（round-robin）的方式实现负载；另一种是通过配置Nginx的upstream模块实现负载。

### 5.4 Python网站的搭建

（1）CGI（Common Gateway Interface，通用网关接口）
CGI是一种通用网关接口规范，该规范详细描述了Web服务器和请求处理程序（脚本解析器）在获取及返回数据过程中传输数据的标准，如HTTP协议的参数名称等。大多数Web程序以脚本形式接收并处理请求，然后返回响应数据，如脚本程序PHP、JSP、Python等

FastCGI是CGI的增强版本，其将请求处理程序独立于Web服务器之外，并通过减少系统为创建进程而产生的系统开销，使Web服务器可以处理更多的Web请求。FastCGI与CGI的区别在于，FastCGI不像CGI那样对Web服务器的每个请求均建立一个进程进行请求处理，而是由FastCGI服务进程接收Web服务器的请求后，由自己的进程自行创建线程完成请求处理

WSGI是为Python语言中定义的Web服务器与Python应用程序或框架间的通用通信接口，可以使Python应用程序或框架与支持这一协议的不同Web服务器进行通信。常见的Python Web框架都实现了该协议的封装。

### 5.6 伪流媒体服务器的搭建

Nginx支持伪流媒体播放功能，其可以和客户端的Flash播放器结合，对以.flv、.f4f、.mp4、.m4v、.m4a为扩展名的文件实现流媒体的播放功能。

### 5.6.2 伪流媒体配置样例

伪流媒体配置样例是利用Nginx的自动索引功能生成XML格式的目录列表，通过XSLT生成前端页面，使用jQuery插件video.js的Flash播放器播放FLV及MP4格式的流媒体文件。页面效果如图5-5所示。


### 5.7 HTTP增强协议服务器的搭建

HTTP/2是HTTP协议的2.0版本，该协议通过多路复用、请求优化、HTTP头压缩等功能提升网络传输速度、优化用户体验。HTTP/2使用二进制分帧层将传输的数据分割为更小的数据和帧，并对它们进行二进制格式编码处理，以实现在不改变HTTP现有语义等标准的基础上提升传输性能，从而降低响应延迟、提高请求吞吐的能力。

可以并行发送多个请求，以提高带宽的利用率。HTTP/2是基于SPDY协议设计的，是SPDY的演进版本，但其不强制使用HTTPS协议，仍可支持HTTP明文传输

HTTP2服务器推送可以实现将多个资源文件（CSS、JS、图片等）同时发送到客户端，如下页面中包含style.css和nginx.png两个资源文件。

在没有服务器推送的情况下，客户端通过3个GET方法获取该页面的所有资源。在启用服务器推送后客户端只需通过一个GET方法，就可以获取到该页面的所有资源。配置样例如下：

### 5.7.2 WebDAV协议服务

WebDAV（Web-based Distributed Authoring and Versioning）是基于HTTP/1.1的增强协议。该协议使用户可以直接对Web服务器进行文件读写，并支持对文件的版本控制和写文件的加锁及解锁等操作

### 第6章 Nginx代理服务应用实战

代理是客户端请求数据处理的中间角色，它本身并不产生响应数据，只是将客户端的请求转发给目标应用服务器，然后目标应用服务器再将响应数据通过代理返回客户端。Nginx不仅可以实现HTTP协议的代理，还支持TCP/UDP及基于HTTP/2的gRPC代理。

### 6.1 HTTP代理

正向代理是客户端设置代理地址后，以代理服务器的IP作为源IP访问互联网应用服务的代理方式；反向代理则是客户端直接访问代理服务器，代理服务器再根据客户端请求的主机名、端口号及URI路径等条件判断后，将客户端请求转发到应用服务器获取响应数据的代理方式。

### 6.1.2 正向代理

正向代理是客户端设置代理地址后，通过将代理服务器的IP作为源IP访问互联网应用服务的代理方式。通过对正向代理访问设置，可以实现限制客户端的访问行为、下载速度、访问记录统计、隐藏客户端信息等目的。

### 6.1.3 HTTP的反向代理

反向代理是用户客户端访问代理服务器后，被反向代理服务器软件按照一定的规则从一个或多个被代理服务器中获取响应资源并返回给客户端的代理模式，客户端只知道代理服务器的IP，并不知道后端服务器的IP，原因是代理服务器隐藏了被代理服务器的信息。

为方便反向代理的配置，此处把通用的代理配置写在proxy.conf文件中。在使用时，通过主配置文件nginx.conf用include指令引入。文件proxy.conf的内容如下：



### 6.1.4 HTTPS的反向代理

在部署有Nginx代理集群的HTTPS站点，通常会把SSL证书部署在Nginx的服务器上，然后把请求代理到后端的上游服务器。这种部署方式由Nginx服务器负责SSL请求的运算，相对减轻了后端上游服务器的CPU运算量，这种方式也被称为SSL终止（SSL Termination）。

### 6.2 TCP/UDP代理

Nginx通过stream模块提供了对TCP/UDP代理的支持，stream模块是在Nginx 1.9.0版本上开始添加的，该模块在Nginx配置文件配置中增加了stream指令域，通过在stream指令域中对指令的配置，实现TCP/UDP协议的代理功能。

### 6.2.2 stream辅助模块

（3）ngx_stream_geoip_module
该模块的功能首先是根据客户端的IP地址与MaxMind数据库中的城市地址信息做比对，然后再将对应的城市地址信息赋值给内置变量。

### 6.2.3 TCP/UDP代理

Nginx并不直接提供TCP/UDP的应用响应，Nginx Stream模块的核心功能是将客户端的TCP/UDP连接反向代理给后端的被代理服务器。

UDP协议通常被用在单向传输无须返回响应及信息分发的场景，如日志收集或在屏幕上的航班信息、股票行情等多媒体场景。



### 6.3.1 gRPC介绍

gRPC是一个开源的基于HTTP/2协议的高性能、跨语言的远程过程调用（RPC）框架。它提供了双向流、流控、头部压缩、单TCP连接上的多复用请求等功能，这些功能使其在移动设备上可更节省空间和降低电量消耗。而且gRPC相对于REST的数据调用方式，提供了一个更加适合服务间调用数据的通信方案。基于gRPC的客户端应用可以像调用本地对象方法一样直接调用gRPC服务端提供的方法，使其更适合分布式应用和服务场景。

### 6.3.3 gRPC反向代理配置

Nginx可以通过HTTP协议的SSL证书，对外提供安全的gRPC代理转发，部署方式为客户端→Nginx服务器（HTTPS）→被代理服务器（SSL gRPC）。

### 第7章 Nginx缓存服务应用实战

反向代理缓存技术是位于用户和网站应用服务器之间的一种加速技术，其按需保存了所有经其转发给客户端的内容，当用户请求的内容已经存在于缓存中时，该内容将立即被返回给用户，这时并不需要与网站应用服务器通信，极大地提高了请求响应速度。

反向代理缓存技术也被以“CDN产品”的形式部署得更接近终端用户。在使用CDN技术后，终端用户可无须考虑时间、地点、运营商等因素而快速打开网站

### 7.1 Web缓存

Web缓存可节约网络带宽，有效提高用户打开网站的速度。由于应用服务器被请求次数的降低，也相对使它的稳定性得到了提升。

### 7.1.1 客户端缓存

客户端通过内容缓存有效期的本地校验和由Web服务端提供的服务端校验两种方式共同校验内容缓存是否有效，这两种方式都是通过HTTP消息头中的相应字段进行判断或与服务端交互的。

### 7.1.2 正向代理缓存

·通常是多个客户端共享一台正向代理缓存服务器，当一台客户端访问某个网站后，其他客户端均会共享这个网站的缓存，无须再向网站服务器发起访问请求，提升内容响应速度。
·通过共享正向代理缓存服务器，不仅减少了外网的访问次数，也降低了网络带宽的需求。

### 7.1.3 内容分发网络

CDN的各缓存服务器节点是通过HTTP响应头的Cache-Control来控制本地内容缓存有效期的。当客户端的请求被分配到CDN缓存服务器节点时，CDN缓存服务器会先判断内容缓存是否过期，若内容缓存在有效期内，则直接返回客户端，否则将向源站点发出回源请求，并从源站点获取最新的数据，在更新本地缓存后将响应数据返回客户端。

### 7.1.4 反向代理缓存

反向代理缓存提高了网站内容的加载速度，降低了被代理服务器的负载，并可以在被代理服务器发生故障时通过缓存的内容作为备份来提高网站的可用性。

### 7.2 Nginx缓存模块

除Nginx自身的缓存处理方案外，Nginx还提供了通过Memcached代理模块的缓存应用方案

### 7.2.1 代理缓存模块

Nginx的缓存功能是集成在代理模块中的，当启用缓存功能时，Nginx将请求返回的响应数据持久化在服务器磁盘中，响应数据缓存的相关元数据、有效期及缓存内容等信息将被存储在定义的共享内存中。当收到客户端请求时，Nginx会在共享内存中搜索缓存信息，并把查询到的缓存数据从磁盘中快速交换到操作系统的页面缓存（Page Cache）中，整个过程的速度非常快。

### 7.3 Nginx缓存应用

Nginx的反向代理缓存有以下几个功能特点。
·故障降级。如果源服务器因故障停机，即便缓存过期，也可以被返回给用户使用，这就避免了页面无法打开的故障信息传递，从而实现有效容错降级。

·缓存锁。使多个相同的请求只有一个可以访问被代理服务器，其他的请求则等待缓存生成后，从缓存中获取响应数据，从而有效地提升缓存利用率，降低被代理服务器的负载。

### 7.3.2 镜像缓存应用

Nginx服务器在配置proxy_store缓存方式下，可以按照URL的路径将从后端获取的静态文件保存在本地磁盘中，因为其内容缓存永远不会过期且没有自动缓存管理机制，所以从严格意义上讲它只能称为内容镜像。该方式可以十分方便地将后端服务器的静态文件资源在Nginx本地生成镜像，相对静态文件资源变动较少的应用场景可以很快地实现动静分离分布式应用架构，进而提升应用负载性能。

### 7.3.4 客户端缓存控制

Nginx作为Web服务器时，会对静态资源自动添加响应头字段Last-Modified，字段值为静态资源文件的最后编辑时间（last_modified_time）。

### 第8章 Nginx负载均衡应用实战

HTTP协议是建立在TCP协议之上的一种应用，是把TCP作为底层的传输协议，由于工作在第七层——应用层，因此它也被称为“七层负载均衡”。

TCP负载均衡是基于TCP协议的负载均衡应用。TCP协议是网络传输的基础协议，工作在网络层和传输层，因此也被称为“四层负载”。

### 8.1 Nginx负载均衡模块

Nginx负载均衡是由代理模块和上游（upstream）模块共同实现的，Nginx通过代理模块的反向代理功能将用户请求转发到上游服务器组，上游模块通过指定的负载均衡策略及相关的参数配置将用户请求转发到目标服务器上。

### 8.1.2 负载均衡策略指令

Nginx支持多种负载均衡策略，如轮询（Round Robin）、一致性哈希（Consistent Hash）、IP哈希（IP Hash）、最少连接（least_conn）等。Nginx的默认负载均衡策略为轮询策略，不需要配置指令，轮询策略通过server的权重参数可实现手动分配的加权轮询策略。

### 8.2.2 一致性哈希

Nginx启用哈希的负载均衡策略，是用hash指令来设置的。哈希策略方法可以针对客户端访问的URL计算哈希值，对相同的URL请求，Nginx可以因相同的哈希值而将其分配到同一后端服务器。

一致性哈希的缺点是，当上游服务器组中的节点数量发生变化时，将导致所有绑定被代理服务器的哈希值重新计算，影响整个集群的绑定关系，产生大量回源请求。

### 8.2.4 最少连接

默认配置下轮询算法是把客户端的请求平均分配给每个被代理服务器，每个被代理服务器的负载大致相同，该场景有个前提就是每个被代理服务器的请求处理能力是相当的。

### 8.3 负载均衡配置

当客户端通过浏览器访问HTTP服务器时，HTTP请求会通过TCP协议与HTTP服务器建立一条访问通道，当本次访问数据传输完毕后，该TCP连接会立即被断开，由于这个连接存在的时间很短，所以HTTP连接也被称为短连接。在HTTP/1.1版本中默认开启Connection:keep-alive，实现了HTTP协议的长连接，可以在一个TCP连接中传输多个HTTP请求和响应，减少了建立和关闭TCP连接的消耗和延迟，提高了传输效率。

在高并发的场景下，Nginx频繁与被代理服务器建立和关闭连接会消耗大量资源。Nginx的upstream_keepalive模块提供与被代理服务器间建立长连接的管理支持，该模块建立了一个长连接缓存，用于管理和存储与被代理服务器建立的连接。

### 8.3.2 upstream的容错机制

Nginx在upstream模块中默认的检测机制是通过用户的真实请求去检查被代理服务器的可用性，这是一种被动的检测机制，通过upstream模块中server指令的指令值参数max_fails及fail_timeout实现对被代理服务器的检测和熔断。

### 8.4 TCP/UDP负载均衡

Nginx的TCP负载均衡与LVS都是四层负载均衡的应用，所不同的是，LVS是被置于Linux内核中的，而Nginx是运行于用户层的，基于Nginx的TCP负载可以实现更灵活的用户访问管理和控制。

### 8.4.2 TCP/UDP负载均衡的容错机制

·proxy_next_upstream是Nginx下提高连接成功率的机制，当被代理服务器返回错误或超时时，将尝试转发给下一个可用的被代理服务器。

TCP连接在接收到关闭连接通知前将一直保持连接，当Nginx与被代理服务器的两个连续成功的读或写操作的最大间隔时间超过proxy_timeout指令配置的时间时，连接将会被关闭。在TCP长连接的场景中，应适当调整proxy_timeout的设置，同时关注系统内核SO_KEEPALIVE选项的配置，可以防止过早地断开连接

### 9.1 Nginx日志配置

Nginx的日志输出位置及内容格式是通过access_log及error_log指令配置实现的。Nginx日志默认是文本格式，通过Nginx提供的log_format可以输出为Json格式，并支持自定义日志输出的内容。

### 9.1.1 访问日志

·参数gzip，设置缓冲区数据的压缩级别，缓冲区数据会被压缩后再写出到磁盘文件。压缩级别范围1～9，级别越高压缩比越高，系统资源消耗也最大，默认级别为1。



### 9.1.3 日志归档Logrotate

日常使用中为方便维护，通常需要将日志文件按日期进行归档。虽然Nginx本身并没有这一功能，但实现日志归档的方法仍有很多，此处推荐使用Logrotate实现日志归档管理。Logrotate是CentOS操作系统内置日志管理工具，该工具可对系统中生成的大量日志文件进行归档管理，其允许对日志文件实行压缩、删除或邮寄等操作。

### 9.2.1 ELK简介

ELK访问逻辑如图9-1所示，是由Elasticsearch、Logstash、Kibana这三款软件和数据采集客户端（如Filebeat）等实现日志采集、储存、搜索分析等操作。

（2）Logstash是基于C/S架构，对日志进行收集、过滤、转发的日志收集引擎，它可以同时从多个源获取数据，动态地将客户端采集的数据进行分拣、过滤，并转发到不同存储服务器。Logstash是以pipeline方式处理每条日志信息的，在每个pipeline中都有输入（INPUTS）、过滤（FILTERS）、输出（OUTPUTS）3个处理动作。每个处理动作可由一个或多个插件实现复杂的功能。输入处理是获取日志数据；过滤处理可以对日志进行分拣、修改；输出处理则是将日志数据发送给目标存储服务器。

（3）Kibana是Elasticsearch的Web管理工具，它提供了友好的界面化操作方式和统计分析的Dashboard工具，让使用者只需简单点击就可完成基本的数据搜索、分析等工作

### 9.2.2 ELK安装

docker-compose是功能非常强的容器运行编排工具，内部含有很多配置指令可以完成容器的资源配置、运行、服务依赖、网络配置等运行时的编排配置

（3）数据持久化
Docker的镜像（Image）文件存放在一个只读层，而容器（Container）的文件则是存放在可写层，当容器删除或重建时，该容器运行时变更的文件将会丢失，所以需要通过外挂卷的方式将变更的配置和文件保存到主机系统中。

### 9.2.3 Nginx日志分析

借助ELK的高性能处理能力，可以实时地将数据分析结果展现给服务器的维护人员及应用的开发人员，进而不断提高业务的可用性及产品的用户体验。Nginx的日志分析可以分为安全分析、性能分析、可用性分析及访问统计分析4个方面。

对请求响应时间的分析，可以通过ELK对访问日志$request_time字段的时间做排名，对时间值比较大的URL从SQL、代码、架构等多方面分析原因。

### 10.1.3 基于Zabbix的连接状态监控

Zabbix Agent脚本可以通过Nginx本机8080端口的status路径获取Nginx连接状态的数据。因连接状态在不同状态下的数据都在一个页面中展示，所以采集脚本需要通过不同的外部参数获取对应的数据。Zabbix Agent数据采集脚本如下：

### 10.3 TCP/UDP主机状态监控

Nginx服务器支持TCP/UDP的代理，但Nginx只在商业版中提供了TCP/UDP服务的状态监控功能，对于开源版本，可以使用第三方模块nginx-module-stream-sts实现TCP/UDP服务的状态监控。由于Nginx stream模块的特性，TCP/UDP服务的状态数据仅在日志处理阶段才会被统计计算。

### 10.3.2 模块配置指令

Nginx stream主机连接状态：会对当前Nginx服务器配置的每个stream主机端口的请求数（Requests Total）、请求时间（Requests Time）、请求频率（Requests Req/s）、响应（Responses）状态码、响应总数（Responses Total）、流量（Traffic）进行统计

### 第11章 Nginx集群负载与配置管理

在实际使用中，入口的负载均衡设备仅负责在传输层（网络分层模型的第四层）实现数据包的快速转发，被转发的数据包继续由多组Nginx负载集群进行应用层（OSI七层模型的第七层）负载、路由及管控，以此实现对客户请求多层负载均衡设备转发的负载架构。

本章将推荐一个无须编写代码，通过对现有的开源软件Jenkins、GitLab和Ansible进行组合，快速搭建一套Web化的Nginx集群配置管理框架的方法。该管理框架通过Jenkins的Web化管理界面实现了权限管理、前端配置、发布记录等功能。它结合GitLab的版本控制功能对每次的变更进行归档，并可随时实现配置回滚。通过对Ansible剧本的调用，自动化地实现了Nginx集群的配置修改、加载、灰度发布等操作。

·多层负载均衡架构。
·基于LVS与Keepalived的高可用Nginx集群负载的搭建。
·基于Jenkins的Web化Nginx集群配置管理框架的搭建。

### 11.1 Nginx集群负载

在常见的传输层负载均衡软件中，LVS集成在Linux内核中，工作在系统内核空间，在DR转发模式下，数据包在网络传输层仅被快速分发，返回的数据包路径并不经过LVS，所以其在网络传输层负载均衡软件里负载性能最强。

### 11.1.1 多层负载均衡架构

多层负载均衡架构是将网络数据在传输层与应用层分开进行负载的网络架构，在传输层使用专用的负载均衡设备或软件仅做网络分发，在应用层由Nginx进行流量路由、过滤、转发等操作。这种网络架构极大地发挥了Nginx对HTTP、HTTPS等七层协议请求的处理优势，同时提高了传输层负载均衡的效率，增加了负载集群的横向扩展能力。

·传输层负载由处理逻辑较少的传输层负载均衡设备或软件组成，通常传输层负载均衡会使用高性能的硬件F5、Radware等，也可以使用LVS自建服务器实现。在云环境中，传输层负载均衡通常由云服务商自建的负载均衡集群实现。

### 11.1.2 LVS简介

LVS（Linux Virtual Server）是一个开源的负载均衡项目，是国内最早出现的开源项目之一，目前已被集成到Linux内核模块中。该项目在Linux内核中实现了基于TCP层的IP数据负载均衡分发，其工作在内核空间且仅做负载均衡分发处理，所以稳定性相对较好，性能相对较强，对内存及CPU资源的消耗也最低。

DS（Director Server）：控制器服务器，部署LVS软件的服务器。

·VIP（Virtual IP）：虚拟IP，对外提供用户访问的IP地址。

·IPVS（IP Virtual Server）：LVS的核心代码，工作于内核空间，主要有IP包处理、负载均衡算法、系统配置管理及网络链表处理等功能。
·ipvsadm：IPVS的管理器，工作于用户空间，负责IPVS运行规则的配置。

IPVS是基于Linux的Netfilter框架实现的，其以数据包的网络检测链为挂载点完成数据的负载均衡及转发处理。

·客户访问虚拟IP（VIP）时，数据包先在主机内核空间被PREROUTING链检测，根据数据包的目标地址进行路由判断，若目标地址是本地，则交由INPUT链进行处理。
·IPVS工作于INPUT链，当数据包到达INPUT链时，会先由IPVS进行检查，并根据负载均衡算法选出真实服务器IP。

·DR，该模式是将客户端的请求包通过修改MAC地址为真实服务器的MAC地址后将数据包分发给真实服务器，客户端的返回包则由真实服务器的本地路由自行处理，源IP地址还是VIP地址（真实服务器需要在本地回环接口配置VIP）。因DS只负责请求包转发，且与真实服务器间进行基于二层的数据分发，所以处理性能最高，但要求DS与真实服务器在同一MAC广播域内

### 11.1.3 Keepalived简介

Keepalived是一款用C语言编写的开源路由软件，目前仍处于活跃开发的状态，其主要目标是基于Linux系统提供一款配置简单且功能强大的负载均衡和高可用的软件应用。负载均衡是基于LVS（IPVS）实现的，Keepalived在LVS的基础上增加了多种主动健康检测机制，可以根据后端真实服务器的运行状态，自动对虚拟服务器负载的真实服务器进行维护和管理。

·HTTP检测。通过HTTP GET方法访问指定的URL并对返回结果进行MD5算法求值，如果与配置文件中的预设值不匹配，则确认为检测失败，并将该真实服务器从服务池中移除。该机制支持同一服务器的多URL获取检测。

### 11.1.4 Nginx集群负载搭建

Keepalived通过文件配置的方式实现LVS的运行管理，并通过VRRP机制实现传输层负载的高可用，为Nginx集群提供高性能、高可用的负载应用。

·Keepalived通过VRRP协议组播通告状态信息，确保两台LVS服务器的高可用。
·当处于MASTER状态的Keepalived发生故障时，处于BACKUP状态的Keepalived切换为MASTER状态，负责与接入路由对接，把数据包转发给后端的Nginx服务器。
·Keepalived通过健康检测机制检测Nginx集群内每台Nginx服务器的健康状态。

至此，高可用的LVS负载均衡就配置完成了。当主LVS服务器出现故障时，备份LVS服务器可以快速接管传输层网络数据的负载均衡工作，将数据包分发给后端的Nginx服务器集群。

### 11.2.1 Nginx集群配置管理规划

Nginx的配置是以文件形式存在的，配置指令会在启动时一次性加载并生效，采用这种方式除upstream的配置可动态变更（商业版本支持API变更，开源版本依赖第三方模块动态修改）外，其他配置的修改均需要重启或热加载Nginx进程才可生效。为实现便捷的Nginx配置变更管理，需要从以下几个方面进行规划。

·conf.d为自建目录，是存放虚拟主机配置文件的目录。

所以Nginx进行配置变更时要及时做好归档和版本控制，因为Nginx配置是以文件方式存在的，所以可以将每次修改的文件以Git标签的方式在Git仓库中进行存档和版本控制。

4.配置发布
Ansible虽然提供了命令行的操作能力，但是用户权限、操作日志及快速回滚等操作仍不够便捷。Jenkins是一款Web化的持续集成发布工具，被广泛应用于业务应用的发布，拥有超过1000个插件，用户无须额外开发就可快速完成代码从代码仓库到运行部署的整个流程，同时还支持用户权限、操作日志及快速回滚等操作。

·Jenkins通过GitLab获取Git仓库中的Nginx文件。
·Ansible根据Jenkins Web界面输入的参数与对应配置模板生成配置文件，更新本地的Nginx配置文件。

·Ansible将更新后的配置文件同步到Nginx集群的所有Nginx服务器，并对Nginx进程执行reload操作，以加载更新后的配置。
·Jenkins将更新后的Nginx配置文件以Git标签的方式进行归档。
·用户可以通过Jenkins获取对应Git仓库的所有Git标签，并根据需求选择对应的Git标签代码执行回滚操作。

### 11.2.3 配置修改工具Ansible

Ansible是一款自动化的运维工具，是基于Python开发的。Ansible提供了一种自动化执行框架，其可以按照用户设计的剧本自动化执行相关操作。Ansible是基于模块工作的，其可以实现使用各种模块，并按照设计的剧本，批量对多个目标执行相同的操作。Ansible集合了众多运维工具的优点，配置更加简单方便。

·处理器（Handlers），当剧本任务条件满足时，触发执行的任务步骤

ignore_errors: True               # 如果当前动作执行出错，忽略错误继续执行

### 11.2.4 配置发布工具Jenkins

通过Jenkins的Web化管理界面，依赖各种强大的插件功能，可以使Nginx的配置变更管理变得更加便捷和安全。

Jenkins是以任务（Job）为管理单元的，常用的任务类型有自由风格、Maven项目、文件夹和流水线（pipeline）四种，本样例中仅使用自由风格任务类型。自由风格及流水线任务按照工作流程被划分为多个阶段，Jenkins负责维护和管理任务在每个阶段的执行，并通过工作流的状态，按照任务的设定推动任务工作流的完成。自由风格任务的6个阶段配置如表11-10所示。

### 11.2.5 Nginx配置管理实例

例如，可以通过jQuery插件对Jenkins的操作界面进行自定义修改，增加根据选择项动态实现参数选项的显示和隐藏，或者增加自定义按钮实现配置预览等功能

### 第12章 Nginx在Kubernetes中的应用

Kubernetes简称k8s，是Google开源的分布式容器管理系统，它的核心功能是如何自动化部署、扩展和管理运行于容器中的应用软件，实现对容器的部署、网络管理、负载调度、节点集群和资源的扩缩容等自动化管理功能。

Kubernetes支持Docker、Rocket和Hyper-v容器引擎，其中Docker容器引擎是基于Go语言开发的，它基于宿主机中操作系统上的进程级别虚拟化技术，直接利用宿主机的系统资源，比虚拟系统级别的虚拟化技术少了虚拟系统的中间层调用，具有资源占用低、镜像体积小、加载速度快等优点。


### 12.1 Kubernetes简介

·Kubernetes集群中被操作控制的资源称为资源对象，如Pod、Node、Service都被看作资源对象。

·kubectl是Kubernetes的命令行管理工具，该工具与Web UI一样，可以通过Kuber-netes主节点的接口服务（API Server）查看及进行创建、删除或更新资源对象等操作。

一个Kubernetes集群可包括多个Node节点，每个Node节点上运行了网络、代理及管理组件。

·kube-proxy负责为Pod提供网络代理和负载均衡等功能。
·Kubernetes并未提供专门的网络组件实现网络功能，目前常用的是flannel，它通过CNI（Container Network Interface，容器网络接口）方式与Kubernetes集成，提供网络功能。

kubelet是运行在每个节点上的节点代理服务，可以实现每个节点上的Pod管理及监控，并接收主节点组件下发的各种管理任务。
·Pod是Kubernetes中最基本的管理单位，是在Docker容器上的一层封装，一个Pod可包含多个容器，可以实现内部运行容器的资源共享。
·CRI（Container Runtime Interface，容器运行时接口）是Kubernetes中用来与底层容器（如Docker）进行通信的接口，可对容器执行启停等操作。

### 12.1.2 Kubernetes相关术语

每个Pod可包含多个容器，每个运行的Pod由一个名为pause的沙盒容器（Sandbox Container，也称基础容器）与一个或多个应用容器组成。

基础容器为Pod中的应用容器提供如下功能。
·共享PID命名空间，Pod中的应用程序可以查看彼此的进程ID。
·共享网络命名空间，Pod中的不同容器共同使用一个IP和端口范围。
·共享IPC命名空间，Pod中的不同容器的应用可以使用SystemV IPC或POSIX消息队列进行通信。
·共享UTS命名空间，Pod中的所有容器共享同一个主机名及共享Pod级别定义的存储卷（Volume）。

Node是指Kubernetes集群中运行Pod的宿主机，每个Node都会运行节点代理服务（kubelet），负责各Node运行Pod容器的管理、监控，并向主节点汇报运行容器的状态，同时接收并执行主节点下发的任务。通过管理工具可对Node资源进行添加、删除及隔离等操作。

标签键值（value）的长度最多63个字符，必须以字母或数字为首字符，也可以为空。每个资源实例与标签是多对多的关系，可以在资源实例初始时定义标签，也可以动态添加或删除。

服务（Service）定义了由多个具有相同服务名称标签的Pod组成的虚拟网络集群，其负责虚拟集群内Pod的负载均衡和自动发现，每个服务会被分配一个全局唯一固定的虚拟IP（Cluster IP），Kubernetes集群内的所有应用都可以通过Cluster IP与这个服务实现TCP通信。

资源对象端点（Endpoint）表示一个由Pod IP和端口组成的可被访问的网络访问点，是构成Service的基础单位，每个Service负责实现端点列表中端点的负载均衡和网络请求转发。

配置映射（ConfigMap）提供了一种类似于配置中心的配置使用方法。应用容器的内容是在制作镜像时打包好的，如果要修改容器的内容，通常都需要重新制作镜像。日常使用中，会遇到很多只简单修改应用配置文件的需求，通过配置映射将配置变量或文件存储在存储服务etcd中，应用容器可以在运行的系统中以环境变量或挂载文件的方式使用这些配置变量。

网络策略需要网络插件的支持，如Flannel并没有提供网络策略的支持，所以无法实现网络隔离。

副本集（Replica Set，RS）在RC原有功能的基础上提供了更多的增强工具，它主要被部署控制器（Deployment Controller）作为协调Pod创建、删除和更新使用。RS也被称为下一代副本控制器，官方已经推荐使用部署控制器管理RS（而不是RC）。

端点控制器（Endpoint Controller）负责与服务对应端点列表的生成和维护，监听服务及其对应Pod的变化。服务被创建或修改时，端点控制器根据服务信息获得其所有Pod的IP和端口信息，并创建或更新同名的端点对象列表。当服务被删除时，同名的端点列表也会被删除。kube-proxy服务通过获取每个服务对应的端点列表，实现服务的负载均衡和数据转发配置。

可使用如下命令查看当前Kubernetes集群接口服务支持的接口版本。

kubectl api-versions

（3）元数据
元数据用以对当前操作的资源实例进行标识，元数据可以包括实例名称（name）、实例所在命名空间（namespace）、实例标签（label）、实例注解（Annotation）等信息

（3）应用部署工具Helm
Helm并非官方提供的工具，而是Deis公司（已被微软收购）开发的用于Kubernetes下应用部署、更新、卸载的管理工具。Helm类似于Linux操作系统中的包管理工具，如CentOS下使用的yum。Helm让Kubernetes的用户可以像安装软件包一样，轻松查找、部署、升级或卸载各种应用。

### 12.1.3 Kubernetes集群部署

kubernetes-dashboard是Kubernetes社区中一个很受欢迎的项目，它为Kubernetes用户提供了一个可视化的Web前端，通过Web前端可以查看当前集群的各种信息，为用户管理维护Kubernetes集群提供帮助。

### 12.1.4 Kubernetes网络通信

计算机间的信息和数据在网络中必须按照数据传输的顺序、数据的格式内容等方面的约定或规则进行传输，这种约定或规则称作协议。

网络分层分为OSI七层模型和TCP/IP五层模型两种。TCP/IP五层模型分别是应用层、传输层、网络层、链路层和物理层，其中应用层对应于OSI七层模型中的会话层、表示层、应用层，这也是二者的区别。计算机网络数据是按照协议规范，采用分层的结构由发送端自上而下流动到物理层，再从物理层在网络分层中自下而上流动到接收端的应用层完成数据通信。

Docker容器有如下4种常见的网络模式。
·主机模式（host）。该模式下，因为容器与宿主机共享网络命名空间（network name-space，netns），所以该容器中可以共享使用宿主机的所有网卡设备。使用者可以通过访问宿主机IP，访问容器中运行应用的所有网络端口。主机模式下网络传输效率最高，但宿主机上已经存在的网络端口无法被容器使用。


·无网卡模式（none）。该模式下，容器中只有环回（Lookback，lo）接口，运行在容器内的应用仅能使用环回接口实现网络层的数据传输。
·桥接模式（bridge）。该模式下，容器内会被创建Veth（Virtual ETHernet）设备并接入宿主机的桥接网络，通过宿主机的桥接网络，容器内部应用可与宿主机及宿主机中接入同一桥接设备的其他容器应用进行通信。


4.跨主机的Pod间数据通信
由CoreOS使用Go语言开发的Flannel实现了一种基于Vxlan（Virtual eXtensible Local Area Network）封装的覆盖网络（Overlay Network），将TCP数据封装在另一种网络包中进行路由转发和通信。

Vxlan协议是一种隧道协议，基于UDP协议传输数据。Flannel的Vxlan虚拟网络比较简单，在每个Kubernetes的Node上只有1个VTEP（Vxlan Tunnel Endpoint）设备（默认为flannel.1）。Kubernetes集群中整个Flannel网络默认配置网段为10.244.0.0/16，每个节点都分配了唯一的24位子网，Flannel在Kubernetes集群中类似于传统网络中的一个三层交换设备，每个Node节点的桥接设备通过VTEP设备接口互联，使运行在不同Node节点中不同子网IP的容器实现跨Node互通。

Service为Pod应用提供了固定的虚拟IP和端口实现固定访问，使得集群内其他Pod应用可以访问这个服务。

Service是四层（TCP/UDP over IP）概念，其构建了一个有固定ClusterIP（集群虚拟IP，Virtual IP）和Port的虚拟集群，每个节点上运行的kube-proxy进程通过主节点的接口服务监听资源对象Service和Endpoint内Pod列表的变化。kube-proxy默认使用iptables代理模式，其通过对每个Service配置对应的iptables规则，在集群中的Node主机上捕获到达该Service的ClusterIP和Port的请求，当捕获到请求时，会将访问请求按比例随机分配给Service中的一个Pod，

·kube-proxy根据集群中Service和Endpoint资源对象的状态初始化所在节点的iptables规则。
·kube-proxy通过接口服务监听集群中Service和Endpoint资源对象的变化并更新本地的iptables规则。

·iptables规则监听所有请求，将对应ClusterIP和Port的请求使用随机负载均衡算法负载到后端Pod。

·kube-proxy首先是建立filter表的INPUT规则链和nat表的PREROUTING规则链，将访问节点的流量全部跳转到KUBE-SERVICES规则链进行处理。

·KUBE-SEP每个Pod有两条KUBE-SEP规则，一条是将请求数据DNAT到Pod IP，另一条用来将Pod返回数据交由KUBE-POSTROUTING规则链实现SNAT。

Service的负载均衡是由iptables的statistic模块实现的。statistic模块的random模式可以将被设定目标的请求数在参数probability设定的概率范围内分配，参数设定值在0.0～1.0之间，当参数设定值为0.5时，表示该目标有50%!的(MISSING)概率分配到请求

kube-proxy遍历Service中的Pod列表时，按照公式1.0/float64(n-i)为每个Pod计算概率值，n是Pod的总数量，i是当前计数。当有3个Pod时，计算值分别为33%!、(MISSING)50%!、(MISSING)100%!，(MISSING)3个Pod的总流量负载分配分别为33%!、(MISSING)35%!、(MISSING)32%!。(MISSING)

Service也支持会话保持功能，是应用iptables的recent模块实现的。recent允许动态创建源地址列表，并对源地址列表中匹配的来源IP执行相应的iptables动作

Service实现了Pod访问的固定IP和端口，但ClusterIP并不是绑定在网络设备上的，它只是kube-proxy进程设定的iptables本地监听转发规则，只能在Kubernetes集群内的节点上进行访问

hostNetwork方式相当于创建Docker容器时以主机模式为网络模式的Pod运行方式，该方式运行的容器与所在Node主机共享网络命名空间，属于资源对象Pod的运行方式，不支持多个Pod的Service负载均衡等功能。资源配置如下：

NodePort方式是在集群中每个节点监听固定端口（NodePort）的访问，外部用户对任意Node主机IP和NodePort的访问，都会被Service负载到后端的Pod，全局NodePort的默认可用范围为30000～32767

（4）LoadBalancer方式
LoadBalancer方式是一种Kubernetes自动对外发布的解决方案，该方案是将外部负载均衡器作为上层负载，在创建Service时自动与外部负载均衡器互动，完成对Kubernetes Service负载均衡创建的操作，将Service按照外部负载均衡器的负载策略对外提供服务

·不同的外部负载均衡器需要有对应的负载均衡控制器（Loadbalancer Controller）。

Pod间的关系可通过podAntiAffinity的配置尽量把同一Service下的Pod分配到不同的Node上，提高自身的高可用性，也可以把互相影响的不同Service的Pod分散到不同的集群Node上。对于Pod间访问比较频繁的应用，可以使用podAffinity配置，尽量把被配置的Pod部署到同一Node服务器上。

Service的流量调度策略有两种，分别是Cluster和Local。Cluster是默认调度策略，依据iptables的随机负载算法，将用户请求负载均衡分配给Pod，但该方式会隐藏客户端的源IP。Local策略则会将请求只分配给请求IP主机中该Service的Pod，而不会转发给Service中部署在其他Node中的Pod，这样就保留了最初的源IP地址。但该方式不会对Service的Pod进行负载均衡，同时被访问IP的Node主机上如果没有该Service的Pod，则会报错。Local策略仅适用于NodePort和LoadBalancer类型的Service。

### 12.2 Nginx Ingress

Kubernetes通过kube-proxy服务实现了Service的对外发布及负载均衡，它的各种方式都是基于传输层实现的。在实际的互联网应用场景中，不仅要实现单纯的转发，还有更加细致的策略需求，如果使用真正的负载均衡器更会增加操作的灵活性和转发性能。基于以上需求，Kubernetes引入了资源对象Ingress，

Ingress为Service提供了可直接被集群外部访问的虚拟主机、负载均衡、SSL代理、HTTP路由等应用层转发功能。Kubernetes官方发布了基于GCE和Nginx的Ingress控制器，Nginx Ingress控制器能根据Service中Pod的变化动态地调整配置，结合Nginx的高稳定性、高性能、高并发处理能力等特点，使Kubernetes对集群中运行于容器的应用程序具有了更加灵活的应用层管理能力。


Nginx Plus版则提供了诸多完善的商业功能，其支持Nginx原生配置指令、JWT验证、Pod变化的动态配置及主动健康检查等功能

Kubernetes社区版是基于Nginx的扩展版OpenResty及诸多第三方模块构建的，其基于OpenResty的Lua嵌入式编程能力，扩展了Nginx的功能，

### 12.2.1 Nginx Ingress原理

Nginx Ingress由资源对象Ingress、Ingress控制器、Nginx三部分组成，Ingress控制器用以将Ingress资源实例组装成Nginx配置文件（nginx.conf），并重新加载Nginx使变更的配置生效。当它监听到Service中Pod变化时通过动态变更的方式实现Nginx上游服务器组配置的变更，无须重新加载Nginx进程

·Ingress，一组基于域名或URL把请求转发到指定Service实例的访问规则，是Kubernetes的一种资源对象，Ingress实例被存储在对象存储服务etcd中，通过接口服务被实现增、删、改、查的操作。

·Ingress控制器（Ingress controller），用以实时监控资源对象Ingress、Service、End-point、Secret（主要是TLS证书和Key）、Node、ConfigMap的变化，自动对Nginx进行相应的操作。

Ingress控制器通过同步循环机制实时监控接口服务Ingress等资源对象的变化，当相关Service对应的端点列表有变化时，会通过HTTP POST请求将变化信息发送到Nginx内部运行的Lua程序进行处理，实现对Nginx Upstream中后端Pod IP变化的动态修改。每个后端Pod的IP及targetPort信息都存储在Nginx的共享内存区域

Pod IP变化及资源对象Ingress对upstream指令域相关注解（annotation）的变化无须执行Nginx的reload操作

当Ingress控制器监控的其他资源对象变化时，会对当前变化的内容创建Nginx配置模型，如果新的配置模型与当前运行的Nginx配置模型不一致，则将新的配置模型按照模板生成新的Nginx配置，并对Nginx执行reload操作。Nginx配置模型避免了Nginx的无效reload操作。

Ingress控制器为Nginx的配置内容提供了冲突检测及合并机制，Ingress控制器使用了准入控制插件（Validating Admission Webhook）做验证Ingress配置语法的准入控制，验证通过的Ingress资源对象才会被保存在存储服务etcd中，并被Ingress控制器生成确保没有语法错误的Nginx配置文件

### 12.2.2 集成的第三方模块

OpenResty的最大特点是集成了Lua脚本的嵌入式编程功能，基于Nginx的优化，使Nginx具有更强的扩展能力。Nginx Ingress通过Lua脚本编程，利用OpenResty的balancer_by_lua模块，可通过nginx-ingress控制器动态地修改Nginx上游服务器组的配置，无须Nginx进程的热加载，有效地解决了因Pod调度带来的Pod IP变化的问题。

InfluxDB输出模块可以使Nginx的每次请求记录以InfluxDB作为后端进行存储，其以非阻塞的方式对每个请求进行过滤，并使用UDP协议将处理后的数据发送到InfluxDB服务器。可以通过该模块实时监控Nginx的所有请求，获得每个请求的连接类型、请求状态，并通过InfluxDB实现相关故障状态的报警

（3）GeoIP2数据库模块（ngx_http_geoip2）
MaxMind的GeoIP数据库已经升级到第二代，GeoIP2数据库提供了准确的IP信息，包括IP地址的位置（国家、城市、经纬度）等数据。该模块增加了GeoIP2数据的支持。

分布式跟踪有助于查明故障发生的位置以及导致性能低下的原因。该模块是将Nginx的请求提供给OpenTracing项目的分布式跟踪系统用于应用的请求分析和监控。

### 12.2.3 安装部署

·Helm安装的应用名称为nginx-ingress，命名空间为nginx-ingress。·以默认的Deployment方式部署，设置Pod副本数为2，并以Service的NodePort方式对外发布服务，设置流量调度策略为Local。·Kubernetes将为nginx-ingress Service随机创建范围在30000～32767之间的Node-Port端口。

·在该部署方式下，Nginx Pod需要使用Local的流量调度策略，获取客户端的真实IP。

（2）Pod的hostNetwork方式
主机网络（hostNetwork）方式可以使Pod与宿主机共享网络命名空间，外网传输效率最高。因Pod直接暴露外网，虽然存在一定的安全问题，但不存在客户端源IP隐藏的问题，部署拓扑如图12-10所示。


（3）SSL终止（SSL Termination）和SSL透传（SSL Passthrough）
SSL终止模式下，客户端的TLS数据会在代理服务器Nginx中解密，解密的数据由代理服务器直接或再次TLS加密后传递给被代理服务器，这种模式下，相对增加代理服务器的计算负担，但方便了SSL证书的统一管理。

SSL透传模式下，Nginx不会对客户端的HTTPS请求进行解密，加密的请求会被直接转发到后端的被代理服务器，这种方式常被应用到后端的HTTPS服务器需要对客户端进行客户端证书验证的场景，相对也会降低Nginx对TLS证书加解密的负担。由于请求数据是保持加密传输的，HTTP消息头将无法修改，所以消息头字段X-forwarded-*的客户端IP无法被添加。Nginx Ingress默认部署方式没有开启SSL透传的支持，需要在部署时使用参数--enable-ssl-passthrough进行开启。



Nginx Ingress提供了基于kubectl工具的管理插件ingress-nginx，用于Nginx Ingress的日常维护。插件ingress-nginx安装方法如下：

### 12.2.4 日志管理

Nginx Ingress是以Pod方式运行的，在默认配置下，Nginx的日志输出到stdout及stderr。Kubernetes下有很多日志收集解决方案，此处推荐使用Filebeat进行容器日志收集，并将容器日志实时发送到ELK集群，ELK环境部署可参见9.2节，日志收集方案逻辑如图12-11所示。

·Docker的默认日志驱动是json-driver，每个容器的日志输出到stdout及stderr中时，Docker的日志驱动会将容器日志以*-json.log的命名方式保存在/var/lib/docker/containers/目录下。

·Filebeat采集的日志可以直接发送给Logstash服务器，也可以发送给Kafka后由Logstash服务器进行异步获取。
·所有日志被Logstash转到Elasticsearch集群进行存储。
·使用者通过Kibana进行日志查看和分析。

### 12.2.5 监控管理

Nginx Ingress中已经集成了Nginx的Prometheus Exporter，可以直接使用Prometheus或Zabbix获取监控数据。Nginx监控支持可以在部署的时候使用部署参数controller.metrics.enabled=true启用。Prometheus及Zabbix的部署和使用可参见10.4节。



### 12.3 Nginx Ingress配置

Nginx Ingress提供了3种方法实现Nginx Ingress配置的修改，分别是配置映射（ConfigMap）、注解（Annotations）和自定义模板

### 12.3.2 注解Annotations

Nginx Ingress注解使用在Ingress资源实例中，用以设置当前Ingress资源实例中Nginx虚拟主机的相关配置，对应配置的是Nginx当前虚拟主机的server指令域内容。在与Nginx Ingress配置映射具有相同功能配置时，将按照所在指令域层级遵循Nginx配置规则覆盖。

支持在注解中添加Nginx原生配置指令。

·Peak EWMA负载均衡算法，是对每个Pod请求的往返延时（Round-Trip Time，RTT）计算移动平均值，并用该Pod的未完成请求数对这个平均值加权计算，计算值最小的Pod端点将被分配新的请求。

设置基于cookie的会话亲缘关系，也就是会话保持功能。启用基于cookie的会话保持功能时，可以使同一客户端的请求始终转发给同一后端服务器。Nginx Ingress对启用会话保持功能的Service集群使用一致性哈希负载算法，即使后端Pod数量变化，也不会对会话保持功能产生太大的影响。

Nginx-ingress在用户没有提供证书的情况下会提供一个内置的默认TLS证书，如果secretName参数没有配置或配置错误，Nginx会使用系统默认的证书，所以配置后仍需检查确认。

（10）“金丝雀”发布
“金丝雀”发布又称为灰度发布，灰度发布功能可以将用户请求按照指定的策略进行分割，并转发到不同的代理服务器组，通过不同的代理服务器部署应用不同版本可进行对照比较，因该方式对于新版本而言类似于使用“金丝雀”的方式进行测试，所以也叫“金丝雀”发布。Nginx Ingress支持Header、cookie和权重3种方式，可单独使用，也可以组合使用。

“金丝雀”路由规则同时存在时的优先顺序是canary-by-header、canary-by-cookie、canary-weight。

（11）lua-resty-waf模块
lua-resty-waf是一个基于OpenResty的高性能Web应用防火墙，对当前虚拟主机的访问可以按照相关防火墙规则进行访问过滤。模块配置说明如表12-26所示。


通过使用Nginx Influxdb模块，可以用UDP协议将请求记录实时发送到后端的Influxdb服务器。

### 第13章 Nginx在微服务架构中的应用

近几年，微服务（Microservices）技术迅猛发展，以Spring Cloud为架构方案、Kubernetes为支撑平台成为微服务架构的主流实践方案。微服务架构中，把一个大系统中的单体应用按业务功能分解成多个功能单一的服务化小系统，并以API的方式使这些小系统相互协作，组合成这个大系统的功能应用。以往大型的复杂单体应用被拆解后的多个微服务所代替，系统中的每个微服务被独立部署，彼此之间是松耦合的。

微服务是以多个独立的个体部署的，因此微服务架构给运维工作带来了诸多挑战，相对于单体应用，微服务的业务调用链更长、调用关系更复杂、故障点更多。运维人员需要考虑更多运行可用性、连续性、容量伸缩及响应速度方面的工作。

在微服务架构体系中，Nginx也基于其自身优势不仅在Kubernetes系统中以Ingress的方式提供服务入口应用，更在微服务网关等核心组件中发挥着重要的作用。

### 13.1 认识微服务

软件生产以程序设计为生产方式的表现形式发展经历了3个时期，分别是程序设计时代、软件系统时代和软件工程时代。

程序设计时代的软件生产仅是程序设计，主要是按照需求编写用来计算的数学程序，编写者和使用者通常为同一人或同一组人，是一种自给自足、个体手工劳动的生产活动。编程语言主要以早期的命令式程序设计语言为表现方式。

结构化程序设计的典型代表莫过于C语言，每个程序由多个源文件构成，每个源文件就是一个模块，不同的源文件被入口文件main.c引入后，通过控制结构实现模块功能的调用。

软件工程时代的软件生产引入了软件工程的概念，软件生产被定义了生命周期，程序开发也被要求遵守系统化、规范化、数量化的工程原则。随着社会的发展，人们对软件的需求量剧增，软件的复杂度也越来越高，大规模软件常常由数百万行代码组成，参与程序开发的人员数以百计，结构化程序设计的问题日益凸显。

随着软件系统规模的变大及处理的场景越发复杂，软件生产进入软件工程时代，整个软件系统的架构设计和规范变得越来越重要，数据库、网络、分布式等应用架构技术也成为软件设计的重要组成部分。软件生产方式的表现形式逐渐由内在的程序设计向外在的应用架构转变，曾被使用的应用架构有垂直应用架构和面向服务架构。

·垂直应用架构。最早的LAMP（Linux、Apache、MySQL、PHP）是一种非常原始的垂直架构模式，由于早期互联网公司的业务规模小，LAMP在很长一段时间内十分流行。随着互联网应用规模的增长，分层模式的垂直应用架构得到了广泛应用。分层模式的最典型模型就是MVC（Model-View-Controller）模型

·面向服务架构。面向服务架构（Service Oriented Architecture，SOA）是一种松耦合、粗粒度的以服务为中心的架构，以服务为基本的业务功能单元，由平台接口契约来定义，将业务系统服务化。

首先，我们要认识到这种需求不明确存在的客观性，因为它不只存在于软件生产活动中，还是一种客观的社会环境特点。这一社会环境特点被称为VUCA，VUCA是易变性（Volatility）、不确定性（Uncertainty）、复杂性（Complexity）、模糊性（Ambiguity）的缩写。VUCA是如今整个社会环境的特点，尤其是信息科技方面，随着科技进步及互联网的快速发展，社会正处于信息化爆炸的年代，大数据、云计算、物联网、人工智能快速发展，人们对未来充满未知和疑惑，各种需求更加模糊、复杂且具有极大的不确定性。

微服务应用架构下，构成服务的粒度较小，代码逻辑简单、易维护、可替换性强。每个微服务都是独立运行的，每个产品功能由一个或多个微服务共同实现，对功能需求的变更只需增加或修改对应的微服务即可，其完全满足了敏捷式开发的需求，成为软件开发过程的必然选择。

### 13.1.2 微服务的技术特点

服务是独立运行的、可被访问的服务单元。微服务架构是一种应用架构，架构中每个微服务可以独立部署，彼此之间是松耦合的。它集成了面向服务架构的诸多优点，且更注重以服务为单元的低复杂度、小体积形态，每个微服务代表一个较小的业务能力，多个不同的微服务可以被组织成可实现更复杂功能的集合。

Spring Cloud是一套完整的微服务架构实践方案，它利用Java语言Spring Boot框架的开发便利性，使开发人员的开发项目与其提供的各微服务组件可以很方便地进行集成，使微服务架构的项目可以快速实施。

（1）服务注册发现
微服务架构中，为确保每个服务的高可用性，每个微服务都由多个部署相同代码的节点构成，每个微服务都会把自己的所有节点注册到注册中心。对于服务调用方，可以通过注册中心查询并发现期望调用服务的节点调用地址，以实现服务访问通信。注册中心会提供相应的检测机制，以确保被发现的节点地址是可用的。常用的服务注册与发现组件有Spring Cloud Euraka、ZooKeeper、etcd、Consul。

为了便于客户端的访问及访问管理，在客户端和服务端之间增加了服务网关组件。服务网关为所有的微服务提供了一个唯一的入口，通过不同的路径将客户端的请求路由到不同的内部服务。通过服务网关还可以提供统一的用户鉴权、跨域访问、流量管控及数据整形等功能，既方便了微服务之间的访问，又减轻了开发工程师的工作量。常见的服务网关组件有Spring Cloud Zuul、Kong（基于OpenResty）及Gravitee。

为了方便配置的修改，配置中心提供了一种配置文件与应用代码分离、集中修改的方法实现配置修改操作。每个服务将配置存储在配置中心，在每次启动时按需读取配置内容，完成配置加载的需求。常见的组件有Spring Cloud Config、Apollo及Disconf。

在单体应用拆分为多个可独立运行服务的微服务架构中，服务节点不断增加，服务间的调用关系变得越发复杂。通常一个客户端请求会引发多个及多层级服务的调用，期间除了需要对容错保护机制进行监控，还需要对因调用关系而引发的链路性能进行分析监控。

分布式链路跟踪会对客户端访问的每个请求创建一个唯一的跟踪标识，当请求在访问链路中流转时，跟踪系统将根据该跟踪标识实现对每个请求链路的监控。这些监控信息可以包括访问路径中的服务名称、请求耗时、方法错误等。常见的分布式链路跟踪组件有Spring Cloud Sleuth、Jaeger和Zipkin。

（6）微服务进程间的通信
微服务是通过网络实现通信的，服务的相互调用是进程间的通信调用。对于进程间的通信在通信机制上有两种，一种是IPC（Inter-Process Communication）机制，其以REST风格为代表，并完全通过HTTP协议实现，相对更加通用、规范；另一种是RPC（Remote Procedure Call）机制，典型应用是Google开源的gRPC框架，它基于HTTP/2协议，使不同服务间的进程可以像调用本地方法一样调用远程方法。

在通信模式上有同步和异步之分，在同步模式下，服务间调用需要被调用方即时响应，在高并发场景下会出现阻塞；在异步模式下，服务间通过消息组件实现间接通信，可有效避免阻塞，同时还支持一对多的通信实现，常用的消息组件有RabbitMQ、Kafka。

（7）支撑平台
碎片化是微服务的主要特征，因而微服务及微服务架构的运维变得更加复杂。容器化技术以进程级别虚拟化使每个微服务运行在传统物理机上，基于容器的管理系统Kubernetes为微服务提供了自动化的管理解决方案。Kubernetes提供了包括自动化部署、运维、监控、负载均衡、灰度访问等功能，有效解决了碎片化微服务的运维管理问题

### 13.1.3 微服务的进化

服务网格（Service Mesh）是一种微服务架构形式，它将微服务独立运行时所依赖的服务组件功能与业务进程分离，使其作为一种可配置的基础设施层存在，每个微服务都包含一个基础设施，并在微服务间为业务进程提供快速、可靠、安全的通信保障。

服务网格起源于开源项目Linkerd，并因Google联合IBM、Lyft发起的Istio项目得到广泛推广。

Istio的Sidecar可实现自动注入Pod，并使集群内服务间的通信完全可被Istio监控。
·Istio分为控制面板（Control Plane）和数据面板（Data Plane）

·数据面板由每个微服务的基础设施（Sidecar）组成，其负责与控制面板间的通信及具体微服务进程间的通信基础功能的实现，Istio的Sidecar是通过Envoy实现的

·在Kubernetes中部署Istio后，Service间的通信将不再通过Kube-proxy，而是被Istio通过iptables规则转由Sidecar接管。

服务网格将Spring Cloud微服务架构中诸多组件通过基础设施层利用Kubernetes系统的特点注入微服务每个节点的Pod中，该方式对业务代码无侵入性，使开发工程师可以更专注于业务功能的实现，极大地减轻了进行软件开发的工作量，提高了软件生产效率

无服务器化（Serverless）并不代表没有服务器，服务器作为底层资源仍是软件运行的基础，它并不是不需要服务器，而是共享服务器资源。每个用户只需要考虑自己业务应用所需要的计算资源，而不需要关心其运行在什么样的服务器上。

开源无服务器化应用Kubeless是基于Kubernetes系统实现的，它支持Python、Node.js、Ruby、PHP、Go、.NET等语言的运行时（runtime），也支持自定义运行时的方法

无服务器化架构方式更细粒度地拆解了微服务，它使每个函数都可成为一个微服务的最小功能单元，极大减少了开发工程师所需考虑的非业务类额外因素，更包括代码可复用的公共组件等，使开发工程师们更专注于业务功能的实现，可以更快速地完成开发任务。

以公司为实体范围的内部用户将共享每个微服务提供的功能，用户通过服务中心检索每个分类的微服务，并按照自己的需求组装更复杂的功能。当网络中不存在符合功能的微服务时，工程师们可以根据需求添加新的微服务或对相似的微服务进行升级。服务中心管理着每个微服务的版本，并根据智能算法和微服务提供的测试声明确保其化学属性的可用性

### 13.2.1 Nginx产品组件

Nginx控制器（Nginx Controller）是Nginx Plus的Web集中监控和管理平台，提供了丰富的监控图表，使用户可以轻松地监视应用程序的运行状况和性能。

Nginx将更注重成为其所代理后端的应用交付网关实现，作为各种应用的统一入口，实现访问入口路由、应用防火墙、内容缓存、负载均衡等功能。例如，成为微服务架构中的API网关或作为Kubernetes架构中的Ingress组件。

### 13.2.2 开源微服务网关Kong

Kong是一款开源的API平台，它是基于Nginx扩展版OpenResty的Lua应用，其将Nginx的配置解构成多个Lua应用模块，通过Lua应用实现了Nginx中各请求阶段的操作。Kong把Nginx操作的配置存储在外部数据库中，并提供了REST风格的管理接口，用户可以通过管理接口实现Kong所有功能的动态操作。

Kong提供了基于Lua脚本实现的多种功能插件，在将用户请求转发给后端服务之前，用户可使用这些插件实现用户请求的认证、访问限流、链路跟踪、日志处理等各种操作。Kong是一个微服务网关平台，它作为微服务API的统一入口对外提供服务，为方便API的管理，定义了如下术语。

Kong为方便实现Nginx配置的动态管理，定义了多个操作对象和对象参数，通过管理接口对不同的操作对象按照该对象的对象参数进行配置，可以非常快速地完成Kong的管理操作。

路由（Route）对象用于表示Nginx配置中虚拟主机的配置，对应Nginx的指令域Server及其包含的location配置。

### 13.2.3 安装部署

Kong将Nginx的配置存储在外部数据库，可以通过自带的数据库初始化命令自动完成数据库表结构的创建和初始数据的添加，为方便一次性创建，该脚本会启动独立的容器kong-migrations来完成此项操作。

### 13.2.4 微服务网关应用

Kong提供了基本认证、密钥认证、OAuth2认证、HMAC认证、JWT认证、LDAP认证等多种方式的认证插件，此处列举常见的密钥认证方式配置。