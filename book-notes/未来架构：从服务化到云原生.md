## 未来架构：从服务化到云原生
> 张亮 吴晟 敖小剑 宋净超

### 内容简介

互联网架构不断演化，经历了从集中式架构到分布式架构，再到云原生架构的过程。云原生因能解决传统应用升级缓慢、架构臃肿、无法快速迭代等问题而成了未来云端应用的目标。

### 作者简介

资深码农，微服务专家，Cloud Native拥护者，敏捷实践者，ServiceMesh布道师，

### 推荐序1

5G的出现将带来什么？也许世界会进一步线上化、数字化、虚拟化，然而这一切都需要底层基础设施做支撑

### 第1章 云原生

信息技术从出现伊始到渐成主流，其发展历程经历了软件、开源、云三个阶段。从软件到开源，再到云，这也是信息技术的发展趋势。

互联网越来越成为当今社会关注的焦点，互联网的基石之一——软件，正在迅速地改变着这个世界。

随着开源文化越来越被认可，以及社区文化越来越成熟，使用优秀的开源产品作为基础构架来快速搭建系统以实现市场战略，成了当今最优的资源配比方案。

### 1.1 互联网架构变迁

互联网应用对于用户量的预估远远没有企业级应用那么准确，在业务发展迅速的情况下，用户量的增长是爆发性且没有上限的。

敏捷地探知市场需求并将其实现，是互联网行业的立命之本。产品快速升级必然会推动开发、测试、交付甚至系统迅速迭代。

在Web 2.0时代刚刚流行的时候，互联网应用与企业级应用并没有本质的区别，集中式架构分为标准的三层：数据访问层、服务层和Web层。

分布式系统的引入，虽然解决了整个应用的吞吐量上限问题，但它并不是能够解决一切问题的“银色子弹”。分布式系统在带来便利的同时，也带来了额外的复杂度。

监控自动化工具可以对服务器的 CPU、内存、磁盘 I/O、网络 I/O 等重要配置进行主动探测监控，一旦指标超过或接近阈值则自动通过邮件、短信等方式通知相关责任人。使用Nagios、Zabbix等系统监控工具可以有效实现这一点。

流程自动化工具主要对服务器进行维护，同时实现应用上线部署等日常操作的自动化和标准化。Puppet、Chef、Ansible、SaltStack 等自动化运维管理工具的出现，快速地将运维工作推向自动化，让一名运维工程师可以很容易地维护成千上万台服务器。

分布式架构解决了互联网应用吞吐量的瓶颈；越来越成熟的分布式中间件也屏蔽了分布式系统的复杂度，提升了开发工程师的工作效率；自动化运维工具则提升了运维工程师的工作效率。但是，由于目标不同，在固有的将开发和运维划分为不同部门的组织结构中，部门之间的配合并不总是很默契的。

开发部门的驱动力通常是频繁交付新特性，而运维部门则更关注服务的可靠性。

DevOps是可以帮助开发工程师和运维工程师在实现各自目标的前提下，向最终用户交付价值最大化、质量最高的成果的一系列基本原则。DevOps在软件开发和交付流程中强调“在产品管理、软件开发以及运维之间进行沟通与协作”。

DevOps是一种公司文化的变迁，它代表了开发、运维和测试等环节之间的协作，因此多种工具可以组成一个完整的DevOps工具链，

随着虚拟化技术的成熟和分布式架构的普及，用来部署、管理和运行应用的云平台被越来越多地提及。IaaS、PaaS和SaaS是云计算的三种基本服务类型，分别表示关注硬件基础设施的基础设施即服务、关注软件和中间件平台的平台即服务，以及关注业务应用的软件即服务。容器的出现，使原有的基于虚拟机的云主机应用，彻底转变为更加灵活和轻量的“容器+编排调度”的云平台应用。

Docker通过集装箱式的封装方式，让开发工程师和运维工程师都能够以Docker所提供的“镜像+分发”的标准化方式发布应用，使得异构语言不再是捆绑团队的枷锁。

容器单元越来越散落使得管理成本逐渐上升，大家对容器编排工具的需求前所未有的强烈， Kubernetes、Mesos、Swarm等为云原生应用提供了强有力的编排和调度能力，它们是云平台上的分布式操作系统。

微服务是一种架构风格，它将一个复杂的单体应用分解成多个独立部署的微型服务，每个服务运行在自己的进程中，服务间的通信采用轻量级通信机制，如RESTful API。服务可以使用不同的开发语言和数据存储技术。通过服务拆分，系统可以更加自由地将资源分配到所需的应用中，而无须直接扩展整个应用。

采用微服务架构风格的团队将围绕业务组织团队，而不是围绕技术组织团队，这一点和DevOps有异曲同工之妙。

对于集中式架构而言，拆分大型应用通常需要在技术层面上设立UI团队、后端开发团队、数据库团队。在这种团队划分方式下，即使进行简单更改也会导致协作团队垮掉。

配置管理。相比于集中式架构的属性文件配置方式，微服务架构更加倾向于使用集中化的配置中心来存储配置数据。

配置修改要想及时生效，配置中心必须有推送配置变更事件的能力。

服务发现。单体应用的服务是可数且可人工运维的，而对于基于微服务架构的应用而言，其服务数非常多，数不胜数。因此，微服务框架要具有服务发现的能力。一般情况下，服务发现是通过向注册中心注册服务实例的运行时标识以及对其进行监听并反向通知其状态变化来实现的。

依靠一套完善的调用链追踪系统来实现，包括确定服务当前的运行状况，以及在出现状况时迅速定位相应的问题点。

日志中心。在微服务架构中，散落在应用节点上的日志不易排查，而且随着应用实例的销毁，日志也会丢失，因此需要将日志发送至日志中心统一进行存储和排查。

### 1.2 什么是云原生

云原生（Cloud Native）最初是由 Pivotal公司的Matt Stine于2013年提出的

云一般指的是一个提供资源的平台，云计算的本质是按需分配资源和弹性计算。

原生是一种设计模式，它要求云原生应用具备可用性和伸缩性，以及自

动化部署和管理的能力，可随处运行，并且能够通过持续集成、持续交付工具提升研发、测试与发布的效率。

将开发环境与生产环境的差异降至最低，便于实施持续交付和敏捷开发。

企业一般都使用GitLab来搭建自己的软件配置管理系统。

持续集成是指自动且持续不断地构建和测试软件项目并监控其结果是否正确。有了持续集成工具的支持，项目可以频繁地将代码集成到主干位置，进而使得错误能够快速被发现。它的目的是让产品在快速迭代的同时还能保持高质量。持续集成并不能消除 Bug，但是它能让 Bug被快速发现并且容易被改正。
持续交付是指频繁地将应用的新迭代版本交付给测试团队或最终用户以供评审，如果评审通过，则自动部署至生产环境。持续交付的中心思想是，无论应用如何更新，它都可以随时随地交付并自动化部署。

Kubernetes使用Service和Ingress处理服务发现和负载均衡。

常见的私有云有OpenStack、VMware等。

监控往往通过“采集、存储、分析、报警（展现）”的流程自动将系统状态通知给系统责任

由于云原生应用是无状态的，因此不应该将日志写入本地磁盘，而是应该写入日志中心。用于采集标准输出并将日志输入其他流的工具主要有Fluentd、Flume、FileBeat、Logstash等，然后这些工具会将日志通过各种缓冲的管道进行处理，写入日志中心，日志中心的存储介质可以是Elasticsearch、HBase等。Elastic公司提供的由搜索引擎Elasticsearch、日志收集工具Logstash和图形界面Kibana所组成的日志中心套件（简称ELK）是一站式的开源解决方案，也有如Splunk这样的一体化商业日志解决方案。

### 第2章 远程通信

远程通信的技术重点是通信方式、序列化协议和透明化RPC框架

### 2.1 通信方式

传输层的作用是使源端和目的端的计算机以对等的方式进行会话，实现端到端的传输。位于传输层的通信协议有TCP和UDP。采用TCP作为应用程序间的远程通信传输方案十分常见， UDP也有其特定的使用场景

TCP是Transmission Control Protocol的缩写，中文译法为传输控制协议。它是一种面向连接的协议，提供可靠的双向字节流。TCP通过三次握手的连接创建机制确保连接的可靠性。一个简明的三次握手流程如图2-2所示，具体描述如下。

TCP通过显式方式确认连接的创建和终止，因此也被称为面向连接的协议。通信时存在必要的创建连接开销，TCP的开销高于UDP，性能低于UDP。但使用TCP可以保证数据的正确性、顺序性和不可重复性，对于业务应用间的通信而言，TCP是更合适的选择。

虽然UDP可能产生网络丢包，并且无法保证传输的原有顺序，但在性能方面更占优势，更加适用于允许数据被部分丢弃的业务场景，如系统调用追踪日志、视频会议流等。

在HTTP/1.1时代，由于无法在同一个连接上并发请求，浏览器需要花费大量的时间等待每一个资源被响应，因此浏览器通常需要开启多个连接来加速请求资源的过程。但开启过多连接的代价是十分昂贵的，所以现代浏览器通常都会被限制最多开启6～8个HTTP/1.1连接。也正因为如此，才会产生各种CSS、JavaScript以及图片合并技术，用于将众多小文件合并成一个完整的大文件，减少文件的个数，提升浏览器加载文件的性能。但不幸的是，单一的大文件会阻塞后续的请求，极度影响用户体验。总之，连接的限制逐渐成了整个Web 系统的性能瓶颈。直到2015年，HTTP才进行了首次重大升级，由HTTP/1.1变为HTTP/2。HTTP/2 的目标是，在与 HTTP/1.1语义完全兼容的前提下进一步减少网络延迟。也就是说，HTTP/2 是在不改变原有 Web 体系的同时提升性能的，是通过多路复用机制实现的。HTTP/2 的多路复用机制允许通过单一的连接同时发起多个请求和响应消息，这极大地提升了网络传输的性能。图2-5清晰展示了HTTP/1.1和HTTP/2的差别。

渲染每个页面时都需要加载一个页面的HTML、CSS和JavaScript文件，这些文件是同步等待的，虽然可以通过开启多个连接来加速加载，但会增加服务端的负荷。并且在每次请求结束之后，浏览器和服务器之间的连接便会关闭，下次请求还需要进行握手并建立连接。HTTP/2将展现一个页面的过程进行了很大的优化，只包含七个步骤，具体如下。浏览器和服务器创建连接。由于 HTTP/2支持长连接，因此如果之前创建的连接仍然存在，则此步骤可以省略。客户端通过GET方法请求index.html来获取页面内容。因为必须先获取HTML的内容才能知道该页面中还包含哪些需要加载的资源，因此获取页面内容是同步的。

通过一个连接的多路复用可以同时请求多个文件。服务器通过连接的多路复用返回style.css和script.js的内容。浏览器加载完毕，开始渲染页面。保留连接，以便下次请求时使用。可以通过设置连接保留时间和最大连接限制以避免用户离开网站以及服务端持有连接过多的问题。

HTTP/2 通过数据流（stream）的方式支持连接的多路复用。一个连接可以包含多个数据流，多个数据流发送的数据互不影响，将请求和响应在同一个连接中分成不同的数据流可以进一步提升交互的性能。HTTP/2将每次的请求和响应以帧（frame）为单位进行了更细粒度的划分，所有的帧都在数据流上进行传输，数据流会确定好帧的发送顺序，另一端会按照接收的顺序来处理。除了多路复用，HTTP/2还提供服务器推送和请求头压缩等功能。随着服务化的发展，HTTP 不再仅被用于浏览器或移动端与后端服务的交互，而是越来越多地被用于后端应用之间的交互。与微服务配套使用的“HTTP/1.1+RESTful API”组合已经非常成熟，由Google开源的基于HTTP/2的异构语言高性能RPC框架gRPC也受到了广泛的关注。

长连接是指客户端与服务端长期保持连接，连接不会在一次业务操作结束后断开，连接一旦创建成功便可以最大限度地复用，以降低资源开销、提升性能。长连接的维护成本较高，需要实时监控检查，以保持连接的连通性。短连接是指客户端和服务端在处理完一次请求之后即断开连接，下次处理请求时则需要重新建立连接。虽然每次建立连接的消耗都比较大，但短连接无须维护连接的状态，相比长连接，其实现复杂度大幅降低。

长连接更加适用于端对端的频繁通信。每个基于TCP的连接都需要经过三次握手，高频率的通信如果将时间都浪费在连接的建立上，就很不划算了。但是，由于维护连接会产生消耗，因此连接的数量不能无限制增加。综上所述，长连接更加适用于面向后端的系统之间的交互。例如，应用系统之间的交互，数据库访问服务与数据库的交互等。它们的共同特点是，交互频率高且连接个数有限。

基于 B/S 模式的浏览器与服务器交互的情况，更加适合使用短连接。HTTP 是无状态的，浏览器和服务器每进行一次交互便会建立一次连接，任务结束后便直接关闭连接。面向互联网海量用户的网站为每一个用户维持一个连接，这是无法承受的成本，而且相对于服务之间的交互，人为操作的频率与之完全不是一个数量级。除了面向用户的连接，面向服务的后端场景也有可能使用短连接，由于基于HTTP的短连接实现起来非常便捷，因此如果服务间交互的性能不是系统瓶颈，那么使用短连接也是可以的。

但I/O的性能发展明显落后于 CPU。对于高性能、高并发的应用系统来说，如何回避I/O瓶颈从而提升性能，这一点是至关重要的。

### 2.2 序列化

序列化协议是一种结构化数据格式，主要用于数据存储和消息传输。序列化是将对象转化为这种格式的方法。

Protobuf的全称是Protocol Buffers，是Google开源的跨平台、跨语言的轻便高效的序列化协议。它是Google公司内部广泛使用的异构语言数据标准，支持反序列化后的对象向前兼容。

### 2.3 远程调用

跨语言的RPC解决方案中，RESTful API是热门的选择。RESTful API大多采用JSON或XML的格式传输信息，虽然绝大多数的编程语言都支持JSON和XML解析，但需要应用开发者自行选择编码方式和服务器架构。使用文本格式序列化性能较差，而搭建一个高性能且容错性强的通信架构也并非易事，因此，RESTful API未必是互联网高并发场景下的合理选择。

gRPC是Google开源的一款对语言和平台均中立的高性能RPC框架，它使用HTTP/2进行网络通信，并将Protobuf作为其序列化工具。gRPC支持多种异构语言，提供了支持Java语言、Go语言和C语言的三个版本。

gRPC是面向服务端和移动端设计的RPC框架，基于 HTTP/2，具有双向流、请求压缩、单连接多路复用等功能，因此在移动设备上表现更好，能够进一步节省移动端的耗电量和网络流量。

### 3.1 本地配置

在集中式系统架构的单机应用时代，配置大多通过属性文件的形式存储，以Key=Value的形态出现。当然也有使用XML或YAML等更加复杂的方式进行配置的（

在单机应用时代，配置文件是完全够用的。运维工程师若要修改配置，只需要登录生产服务器，用vim等文本编辑工具进行修改，然后再重启应用，或者等待定时任务重新加载配置即可。

### 3.2 配置集中化

由于服务器数量增加导致运维工作量增加，因此分布式系统很难继续采用本地配置的方式。提升运维生产力的核心方法是集中化，即将散落在每台服务器的运维操作集中于一点统一处理，然后由程序通过远程通信或异步消息自动分发至各个服务器。对系统配置进行修改是运维工程师线上操作的重要工作之一，因此对配置进行集中统一管理已是大势所趋。 配置中心，顾名思义，就是集中管理各个系统配置的服务。

配置修改的工作量大。大量线上服务器需要运维工程师逐台操作，效率低下。配置修改可能发生遗漏而导致环境不一致。同一组分布式服务分别运行在不同的配置下，状况难以预料。

配置修改的工作量减少。运维工程师只需要修改配置中心即可达到“单点修改，全局生效”的效果。配置修改不可能遗漏。应用程序均通过配置中心远程读取配置，杜绝了遗漏修改的可能性。各节点配置不一致，时间差基本一致。由配置中心统一分发，不存在人工操作带来的时间差。

配置信息可以像业务数据一样被持久化保存，能够快速搭建环境、恢复业务。多个系统配合上线时，配置检查、沟通协调变得更加容易。业务应用系统将配置信息放置于应用程序之外，更容易保持应用的无状态化，为容器化、微服务等部署方案提供了强有力的支持。

### 3.3 配置中心和注册中心

注册中心用于分布式系统的服务治理，多用于管理运行在当前集群中的服务的状态，需要随时进行动态更新。而配置中心则不然，它关注的是配置本身，相比于状态，配置是更加静态和具象的事物。

### 3.4 读性能

通过远程通信方式读取数据的性能显然远远不及通过本地内存读取数据的性能。读性能大幅下降的同时，还带来了配置中心与日俱增的访问压力。一旦配置中心因为访问压力大而瘫痪，整个集群也会受到明显的影响。那么应该使用什么方式提升读性能并降低配置中心的压力呢？答案是使用缓存。

配置中心具备数据持久化的能力，所有的配置数据必须落盘以保证数据的完整性。前面提到过，配置信息是变更稀疏的，读取操作远远多于写入操作。如果每次访问都直接从磁盘中读取，难免会影响性能。将配置信息全量加载至系统内存，每次仅通过内存读取，则可以大幅提升访问效率。集中式缓存的优点是，每次客户端进行访问时都可以获取到最新的数据，缺点是并未缓解配置中心的访问压力。

### 3.7 数据一致性

配置中心的出现有效地解决了互联网分布式环境的配置问题。

### 第4章 服务治理

服务化的关键是服务治理。在微服务大行其道的今天，服务的粒度被拆分得非常细，随之而来的是服务数量的迅速增长。在云原生的浪潮中，服务治理更多情况下与容器调度平台结合，共同形成一站式的自动化调度治理平台。

服务治理主要包括服务发现、负载均衡、限流、熔断、超时、重试、服务追踪等。

### 4.1 服务发现

一致性指的是所有节点都能够在同一时间返回同一份最新的数据副本；可用性指的是每次请求都能够返回非错误的响应；分区容错性指的是服务器间的通信即使在一定时间内无法保持畅通也不会影响系统继续运行。对于分布式系统来说，分区容错性是必须满足的。因此，必须要在一致性和可用性之间进行取舍，这就是所谓的“选择AP还是选择CP”。传统的关系型数据库选择的是CA，即缺乏分布式的能力。

ZooKeeper是如何保证多个服务端的数据一致的呢？ZooKeeper 通过消息传递保持分布式节点之间的数据一致性。Zab 是 ZooKeeper Atomic Broadcast的缩写，是专门为ZooKeeper而设计的支持崩溃恢复的原子广播协议。

### 4.2 负载均衡

负载均衡的本质是通过合理的算法将请求分摊到多个服务节点。对于由服务承载能力对等的节点组成的服务集群来说，实现负载均衡的关键在于均匀分发请求。DNS 可以说是最早出现的负载均衡使用案例。在 DNS 服务器中为同一个主机名称配置多个IP地址，在应答查询时，DNS服务器对每个查询都将以轮询的方式返回不同的主机IP地址，将客户端访问引导至不同的服务器，从而达到负载均衡的目的。小规模系统可以使用DNS作为负载均衡的手段，但DNS缺乏对服务发现的应对能力，一旦服务节点的启动和销毁变得更加频繁，DNS就会无法应对，它的记录和传播速度无法跟上服务节点的变化节奏。

服务端负载均衡又分为硬件负载均衡和软件负载均衡。硬件负载均衡需要在服务器节点之间安装专用的负载均衡设备，常见的有F5等设备。软件负载均衡的解决方案有很多，常见的有LVS、Nginx等。

采用服务端负载均衡方案时，负载均衡器会维护一个可用的应用服务器列表，并通过心跳检测将发生故障而无法及时响应心跳的服务器移出列表。当负载均衡器接收到客户端的请求时，将通过轮询、权重或流量负载等负载均衡算法将请求转发至相应的服务器。

负载均衡的网络消息转发一般集中在传输层和应用层。由于传输层在七层模型的第四层，因此通常简称为四层负载均衡，它是基于IP地址和端口号进行负载均衡的。而应用层在七层模型的第七层，因此集中在应用层的负载均衡是基于URL和请求头等应用层信息进行负载均衡的。也有基于MAC地址的二层负载均衡和基于IP地址的三层负载均衡。

四层负载均衡通过修改报文中的目标地址和端口，将请求转发至合适的应用服务器。以TCP为例，负载均衡服务器在接收到第一个来自客户端的 SYN 请求后，即可通过负载均衡算法选择合适的应用服务器，然后将报文中的目标IP地址修改为真实的后端应用服务器的IP地址，并将请求直接转发至该服务器。因此TCP通过三次握手建立的连接是由客户端与真实应用服务器端直接建立的，负载均衡服务器在这种情况下仅作为路由器进行转发。

客户端负载均衡的优点是，由客户端内部程序实现，无须额外部署负载均衡器，而且客户端和服务端是直接连接的，无须通过服务端负载均衡器进行二次转发，无网络间传输带来的损耗。

采用Java开发的应用级服务框架都采用客户端负载均衡的方式，如Dubbo和Spring Cloud，由于它们是嵌入应用的框架，因此也可以将这类框架称为侵入式服务治理方案。虽然各种服务治理方案中都提供客户端负载均衡模块，但目前单独提供客户端负载均衡功能的组件还比较少，目前常见的是Netflix公司的Ribbon。

由Eureka客户端向Ribbon提供当前可用的服务列表，再由Ribbon通过应用配置的负载均衡策略将请求发送至相关的后端服务。

客户端负载均衡的解决方案对客户端是不透明的，客户端需要知道服务器端的服务列表，并且自行决定需要发送请求的目标地址。负载均衡策略和服务列表的时效性是由客户端维护的，因此，客户端负载均衡并不适用于直接面向外网的场景，它更适合系统内部的微服务架构模型使用。

### 4.3 限流

限流又被称为流量整形（Traffic Shaping），它能够平滑网络上的突发流量，并将突发流量整形为一个稳定的流量供网络使用。限流的主要目的是保护后端的服务节点不被突然到来的流量洪峰冲垮。

拒绝服务：直接定向到错误页面，或告知用户当前资源已经没有了。

排队等待：将客户端请求放入队列，在服务端有资源处理时再从队列中获取请求处理，处理完毕再返回客户端。

应用降级：提供默认行为或数据，如默认显示无任何评论、库存有货等。

### 4.4 熔断

在流量过载的情况下禁止客户端对服务端进行访问，是熔断的目的所在。限流和熔断在某种意义上来说，是类似的两个概念，限流是允许部分流量通过，而熔断则是完全禁止某客户端访问后端服务，它们的目的都是防止流量洪峰压垮整个集群。

熔断的原理与电路熔断的原理相同，若一条线路上的电压过高，保险丝会自动熔断以防止火灾的发生。同理，若某目标服务质量低于临界点（如发生大量响应超时），熔断对该服务的调用便能快速释放资源，防止目标服务因持续超负荷运转而宕机，在目标服务状况恢复正常时再恢复调用。

### 5.2 核心概念

日志（Logging）、指标（Metrics）和追踪（Tracing）是紧密相关的三个核心概念

CNCF生态中的Prometheus监控系统正是基于指标的典型系统，它通过定义和收集不同的指标数据，以及提供基于时间维度的查询能力，为分布式服务指标提供基础数据保障。

追踪在监控领域通常被称为分布式追踪，是指在单次请求范围内处理信息。任何的数据和元数据信息都被绑定到了系统中的单个事务上

日志、指标、追踪可以看作功能全集中的元素，一些商业级别的 APM（Application PerformanceManagement，应用性能管理）系统便采用“追踪+指标”的方式提供一体化的解决方案。

### 5.4 应用性能管理与可观察性平台

APM系统为单体式应用和分布式应用提供了全面的可视化展现建议、性能分析建议、性能诊断和优化建议，为开发团队、运维团队提供了常规监控体系之外的保障。

### 第6章 侵入式服务治理方案

侵入式服务治理方案指的是，在应用端使用框架提供的API开发程序并提供服务治理方案。Java提供了很多一站式服务化框架，可以有效地与应用系统深度配合，形成完善的服务治理体系

### 6.1 Dubbo

互联网架构的演进过程分为四个阶段，每个阶段对应一种架构模式，具体如下。单体应用架构在系统访问流量不大时，应用所需的所有功能都在开发和部署时被集中在一起。单体应用架构获取数据的主要途径是与数据库进行交互。由于关系型数据库与面向对象的阻抗不匹配，因此，开发出能够简化增删改查工作的数据库访问（ORM）框架是重中之重。

因此，我们要将核心业务抽取出来形成独立的后端服务，再对前端进行进一步抽离，使其能够更加快速地响应市场需求。此时，前端与后端的交互以及后端服务之间的交互，若采用基于RESTful API的Web MVC显然并不适合，因此RPC成了获取数据的重要方式。

后端服务的增多，使得服务的治理成本越来越高，手动进行服务发现、负载均衡、连接管理、限流保护等工作已经变得不现实，

Dubbo所关注的重点在于第三点和第四点的前半部分。对于分布式应用间的RPC交互而言， Dubbo采用透明化的方式，让使用者无须关心方法的调用是本地的还是远程的。Dubbo采用以ZooKeeper为主的注册中心和治理中心来提供服务治理，并未提供调度中心的实现方案。

### 6.2 Spring Cloud

Spring Cloud是基于Spring Boot的嵌入式服务治理框架，

Spring Boot提供内嵌的Web服务器并简化了Spring MVC配置，提供对数据库、NoSQL、缓存、消息中间件、REST 访问、邮件发送等第三方服务的高度整合，仅使用几行代码就用开发出一个简单的Web应用，而使用Spring原生开发和配置的方式远达不到相同效果。启动Spring Boot的入口应用仅需要一个注解和一行代码，

Spring Boot会自动将其加入Spring容器，并发布至内嵌的Web容器。一个简单的Spring MVC的示例代码如下。

侵入式服务框架会或多或少地改变业务应用的开发方式，例如，在开发阶段需要引入注册中心、负载均衡策略等概念，这样做会增加应用开发的复杂度。开发框架的成本非常高，因此在对新方案进行推广和升级的时候，往往会受到来自业务开发部门的阻力。

### 第7章 云原生生态的基石Kubernetes

Kubernetes是CNCF托管的所有项目中第一个成功毕业的项目，整个CNCF技术栈都是围绕它而建立的。Kubernetes 是云原生项目中最重要的组件，其目标不仅仅是成为一个编排系统，更是为用户提供一个规范，让用户可以描述集群的架构，定义服务的最终状态，并让系统自动维持在这个状态。

### 7.3 设计哲学

在云原生应用程序中，与任何事物进行通信都是通过网络实现的，用户编写YAML格式的应用程序编排文件，然后Kubernetes将其转换为JSON格式，并通过RESTful HTTP的方式与API Server进行通信。Kubernetes组件本身则通过gRPC进行通信。

Kubernetes的控制器一直监听API Server。一旦发现有应用状态变更，就会声明一个新的状态，并相信API Server和kubelet会进行必要的操作来调整应用状态，直至与声明的状态一致为止。

### 7.4 Kubernetes中的原语

每个Kubernetes对象中包含两个嵌套字段：spec和status。spec描述了对象的期望状态，即用户希望对象所具有的特征，由用户通过配置提供。status 描述了对象的实际状态，由Kubernetes负责提供和更新。

Service：直接使用Service提供集群内部的负载均衡，并借助云供应商提供的负载均衡器将集群服务暴露出来供外部客户端访问。

Ingress：依然使用Service提供集群内部的负载均衡，但是要自定义负载均衡器让外部客户端访问集群的服务，常见的有Traefik Ingress Controller、Nginx Ingress Controller等。

网络隔离：需要使用基于CNI协议的网络插件，如Flannel、Calico。

资源隔离：Kubernetes原生支持资源隔离，Pod就是资源隔离和调度的最小单位，同时， Kubernetes还能够使用Namespace限制用户空间和资源限额。

我们可以启动一个旁侧容器来辅助非功能需求，例如，拦截所有应用程序的流量出入来进行审计、权限认证和流量管理等。将应用程序的功能划分为单独的进程，这种方式被称为Sidecar模式。

### 7.5 应用Kubernetes

Kubernetes 作为云上的操作系统，能够保证用户的应用在不同的云环境中使用相同的描述语言。从理论上来讲，所有可以容器化、支持弹性伸缩的分布式应用都可以部署到 Kubernetes集群中。关于Kubernetes的详细配置，各位读者可以参考网络上的资源。

1.将原有的应用拆解为服务。2.定义服务接口/API的通信方式。3.编写启动脚本作为容器进程的入口。4.准备应用的配置文件。5.将应用容器化并制作容器镜像。6.准备Kubernetes所使用的YAML文件。7.如果有外置的配置文件，需要创建ConfigMap或Secret存储。

为每个用户划分独立的Namespace，并且创建独立的Service Account和kubeconfig文件。隔离不同Namespace之间的资源，但不进行网络隔离，使不同Namespace间的服务可以互相访问。

在kubectl命令上再封装一层，增加用户身份设置和环境初始化操作，简化kubectl命令和常用功能。

将所有的应用日志统一收集到Elasticsearch中，统一日志访问入口。

可以通过Grafana查看所有Namespace中的应用的状态和Kubernetes集群本身的状态。

### 7.6 Kubernetes与云原生生态

Kubernetes 最大的作用是形成了事实上的“云操作系统”标准，以及定义了云应用的规范模型，所有的云原生应用必须遵循它的定义才能够部署和实施。

虽然Kubernetes已经足够强大，但仍然缺失一个重要能力——微服务治理能力。相比于单体式应用，微服务在带来更加轻量级且纯粹的应用系统的同时，也增加了部署和运维的成本，并且对微服务的可观察性也有更高的要求。其中，如Istio这样的Service Mesh技术便成了解决云原生中的连接、保护、控制和观察服务的重要基础设施。

引入服务网格（Service Mesh）：在Kubernetes上实现微服务架构以及进行服务治理时， Service Mesh是必需的组件。

落地无服务器架构（Serverless）：以FaaS为代表的无服务器架构将会逐渐流行。

简化应用部署与运维：包括简化云应用的监控、日志收集分析等。

现今的IaaS（Infrastructure as a Service，基础设施即服务）运营商主要提供基础架构服务，如虚拟机、存储设备、数据库等，这些基础架构服务仍然会使用现有的工具（如Chef、Terraform、Ansible等）来进行管理。Kubernetes则能够直接在裸机上运行，并结合CI/CD成为DevOps的得力工具，进而成为工程师部署应用的首选。Kubernetes也将成为PaaS（Platform as a Service，平台即服务）的重要组成部分，为开发者提供简单的应用程序部署方法。但是，开发者可能不会直接与Kubernetes或PaaS交互，实际的应用部署流程将在自动化持续集成工具（如Jenkins）中完成。

### 8.1 Service Mesh概述

Service Mesh 是新兴的微服务架构，被誉为下一代微服务，同时也是云原生技术栈的代表技术之一。

2016年9月29日，在SF Microservices大会上，“Service Mesh”这个词第一次在公开场合被使用，这标志着Service Mesh这个术语正式从Buoyant公司走向社区。

2018年7月，CNCF社区正式发布了云原生定义1.0版本，非常明确地指出，云原生代表技术包括容器、服务网格（Service Mesh）、微服务、不可变基础设施和声明式API，至此Service Mesh技术被放到了一个前所未有的高度上。

当发起一个请求时，作为请求发起者的客户端应用实例会首先通过简单方式将请求发送到本地的Service Mesh代理实例。注意此时应用实例和代理实例是两个独立的进程，它们之间的通信方式是远程调用，而不是代码层面的方法调用。

Service Mesh 的代理实例会完成完整的服务间通信的调用流程，如服务发现、负载均衡等基本功能，熔断、限流、重试等容错功能，以及各种高级路由功能，安全方面的认证、授权、鉴权、加密等，最后将请求发送给目标服务。最终表现为Sidecar模式，实现和传统类库类似甚至比传统类库更完备的功能。

Sidecar这个词译为“边车”或者“车斗”。Sidecar 模式早在Service Mesh出现前就在软件开发领域中被使用了，它的灵感来源于实物，通过在原有的两轮摩托的一侧增加一个边车来实现对原有功能的扩展

当多个服务依次调用时，Service Mesh表现为一个单独的通信层。在服务实例之下，ServiceMesh 接管整个网络，负责所有服务间的请求转发，从而让服务只需简单地发送请求和处理请求，不必再负责传递请求的具体逻辑。中间服务间通信的环节被剥离出来，变为一个抽象层，称为服务间通信专用基础设施层。

当系统中存在大量服务时，服务间的调用关系就会表现为网状。如图8-5所示，在每个“格子”中左边的是应用程序，右边的是Service Mesh的Sidecar,Sidecar之间的线条表示服务之间的调用。可以看到，Sidecar之间的服务调用关系形成了一个网络，这也就是Service Mesh（服务网格）名字的由来。

部署：Service Mesh在部署上体现为轻量级网络代理，以Sidecar模式和应用程序一对一部署，两者之间的通信是远程调用的，但是要通过Localhost。

透明：Service Mesh 对应用程序是透明的，其功能实现完全独立于应用程序。应用程序无须关注Service Mesh的具体实现细节，甚至对Service Mesh的存在也可以无感知。这样带来的一个巨大优势是，Service Mesh可以独立部署升级、扩展功能、修复缺陷，而不必改动应用程序。

对Service Mesh的定义给出了概括和总结，需要注意的是，如果把应用程序去掉，只呈现出 Sidecar 和Sidecar 之间的调用关系，这个时候 Service Mesh 的概念就会特别清晰：Sidecar和调用关系形成完整的网络，代表服务间复杂的调用关系，承载着系统内的所有应用。

### 8.2 Service Mesh演进历程

在第一代网络计算机系统中，最初一代的开发人员便需要在应用代码里处理网络通信的细节问题，比如数据包顺序、流量控制等问题，这种编码方式导致网络通信逻辑和业务逻辑混杂在一起。为了解决这个问题，TCP/IP技术出现了，流量控制问题得到了解决，网络通信功能变成了通用功能。

TCP/IP出现后，实际功能没发生变化：所有的功能都在，相应的代码也在。但是，最重要的流程控制已经从应用程序里面被剥离出来。剥离出来的这些功能被进一步标准化和通用化，最终统一成了操作系统网络层的一部分，这就是TCP/IP协议栈。

这个改动大幅降低了应用程序的复杂度，将业务逻辑和底层网络通信细节解耦，应用程序开发人员得以解脱并将精力集中在应用程序的业务逻辑实现上。

为了简化开发，避免代码重复，我们选择使用类库，如经典的Netflix OSS套件。这样一来，重复编码问题就解决了，只需要写少量代码，就可以借助类库实现各种功能。

以Spring Cloud或Dubbo为代表的传统微服务框架，是以类库的形式存在的，通过重用类库来实现功能，避免代码重复。但以运行时操作系统进程的角度来看，这些类库还是渗透进了打包部署之后的业务应用程序，和业务应用程序运行在同一进程内。所谓的“侵入式框架”便是这样的。

业务应用的核心价值在于业务实现，微服务是手段而不是目标，若在学习和掌握框架上投入太多精力，在业务逻辑实现上的投入必然会受到影响。业务团队往往承受着极大的业务压力，时间、人力永远不足。

侵入式框架无法跨语言，这个问题非常棘手，为了解决它，通常有两个思路。统一编程语言，整个系统只使用一种编程语言，只为这一种语言提供框架和类库。为要使用的每种编程语言提供一套解决方案，即为每种语言开发一份框架和类库代码。

TCP/IP 解决了什么问题？答案是网络通信逻辑代码和业务逻辑代码混杂在一起的问题。TCP/IP 是如何解决这些问题的？方法是将网络通信逻辑代码从业务应用中剥离出来，进行标准化后下沉到底层。

在第一代Service Mesh产品（Linkerd、Envoy）刚刚发展成熟并正要开始逐渐被推向市场时，以 Istio 为代表的第二代 Service Mesh 产品就登场了，并且直接改变了市场格局。

第二代Service Mesh和第一代Service Mesh的差异在于是否有控制平面，具体如下。第一代Service Mesh只有数据平面（Sidecar），所有的功能都在Sidecar中实现。第二代Service Mesh增加了控制平面，带来了远超第一代的控制力，功能也更加丰富。图8-18展示了Istio的控制面板。

以Sidecar方式部署的Service Mesh控制了服务间所有的流量。
Istio增加了控制面板来控制系统中所有的Sidecar。
Istio能够控制所有的流量，即控制系统中所有请求的发送。
Istio出现之后，Linkerd陷入困境，Envoy则作为Istio的数据平面和Istio一起发展。随后Buoyant公司推出了全新的Conduit来应对Istio的强力竞争，我们将在后面的内容中详细讲述Service Mesh的开源产品和市场竞争情况。

控制力不够强大。第二代Service Mesh（Istio）通过增加控制平面来加强控制，带来了更强大的功能，目前第二代的Service Mesh还在继续完善中。

### 8.3 Service Mesh市场竞争

直到2017年年底，当非侵入式的Service Mesh技术终于从萌芽走向成熟，当Istio终于横空出世后，人们才惊觉：原来微服务并非只有侵入式一种“玩法”！

Istio通过“收服”和Linkerd同为第一代Service Mesh的Envoy，直接拥有了一个功能和稳定性与Linkerd同一水平的数据平面。基于C++的Envoy在性能和资源消耗上本来就强过基于Scala/JVM的Linkerd。

在功能方面，由于Envoy的定位是数据平面，因此无须考虑太多，很多工作在Istio的控制平面中完成即可，Envoy从此只需专心将数据平面做好，完善各种细节。

确立Google和IBM在微服务市场的统治地位。为Google和IBM的公有云打造“杀手级”的特性。在Kubernetes的基础上延续Google的战略布局。Google 在企业市场的战略布局是从底层开始一步一步向上靠近应用。刚刚大获全胜的Kubernetes为Istio提供了一个非常好的基石，而Istio的历史使命就是继Kubernetes“拿下”容器领域之后，更进一步地“拿下”微服务领域！

大幅降低微服务开发的入门门槛，让更多的企业和开发人员可以将微服务落地。统一微服务的开发流程，使开发/运维方式标准化。

2018年7月，令整个社区期待已久的1.0.0版本发布。
从目前的发展态势来看，Istio正在慢慢完善产品，相对于2017年表现得更加成熟，虽然还存在诸多问题，虽然还缺乏大规模的落地实践，但相信随着时间的推移，Google、IBM和Istio社区会继续将Istio向前推进。

2018年6月，蚂蚁金服对外发布Service Mesh类产品SOFAMesh，这是一个基于Istio的增强扩展版本，并使用基于Go语言开发的SOFAMosn作为数据平面，替代了Envoy。

### 8.4 Istio

Istio的主要功能是连接、保护、控制和观测。

连接：智能控制服务之间的流量和 API 调用，进行一系列测试，并通过红/黑部署逐步升级。保护：通过托管身份验证、授权和服务之间通信加密，自动保护服务。控制：应用策略并确保其执行，使得资源在消费者之间得到公平分配。观测：通过自动跟踪、监控和记录所有服务，了解正在发生的情况。

数据平面是以Sidecar方式部署的智能代理，Istio默认集成的是Envoy。数据平面用来控制微服务之间的网络通信，以及和Mixer模块的通信。控制平面负责管理和配置数据平面，控制数据平面的行为，如代理路由流量、实施策略、收集遥测数据、加密认证等。控制平面包含Pilot、Mixer、Citadel三个主要组件。

Envoy是由Lyft开发并开源的基于C++的高性能代理。Istio中使用的是Envoy的扩展版本，称为Istio Proxy，在GitHub仓库中为istio/proxy，可以理解为在标准版本的Envoy基础上扩展了Istio独有的功能。

在Istio中，Envoy用于调节服务网格中所有服务的所有入站和出站流量，Envoy的大量功能在Istio中被使用。简单来说，Envoy主要提供服务间通信的能力，包括对各种网络通信协议的支持，具体如下。

Envoy还提供了和网络通信直接相关的各种功能，具体如下。服务发现（从Pilot得到服务发现信息）负载均衡健康检查熔断高级路由（路由规则由Polit下发）基于百分比的流量拆分加密和认证故障注入

### 第9章 云原生数据架构

在大数据和微服务大行其道的今天，传统的关系型数据库也将迎来变革。云原生的数据库架构，将越来越受到关注。

### 9.1 关系型数据库尚能饭否

关系型数据库的性能和访问承载能力在面向单一数据节点的企业级应用时代是无可挑剔的。在访问量和数据量急剧增长的今天，关系型数据库已经很难再像以前那样作为大规模系统的底层支撑了，甚至成为了应用系统的瓶颈。

单节点并发访问量受限。由于数据库中存储的数据是有状态的，因此很难像服务一样任意拆分和扩容。单一的数据库节点承载大量服务节点的查询和更新请求，这并非一个对等的架构部署模式。

关系型数据库的不足，归根结底是设计初衷有一定问题。它并非分布式的产物，对分布式系统天生不友好，因此它很难适应互联网的架构模型。面对可以随时弹性扩容的无状态服务，使用关系型数据库已经略显笨重。

### 9.2 未达预期的NoSQL

Redis提供了集群处理的能力，可以将数据分散至不同的节点，有效解决了单一节点的访问量瓶颈。

MongoDB在分布式的表现上也远强于关系型数据库，它可以将数据自动分片，并且能够透明化分片之间的负载均衡和失效转移。MongoDB还内置了GridFS，支持大数据集的存储。

HBase 通过行键和列族来确定一条记录，每个列族中的属性是不固定的，这一点与文档数据库类似。HBase 同样能够自动切分数据，使得数据存储自动具有水平扩展的能力。HBase 的数据存储在HDFS这样的分布式文件系统中，对海量数据的支持是最好的。

### 9.4 云原生数据库中间件的核心功能

从性能方面来说，关系型数据库大多采用B+ Tree类型的索引，在数据量超过阈值的情况下，索引深度的增加也将使磁盘访问的 I/O 次数增加，进而导致查询性能下降。同时，高并发访问请求也使得集中式数据库成为系统的最大瓶颈。

从运维成本方面考虑，当一个数据库实例中的数据量达到阈值时，对于 DBA 的运维压力就会增大。数据备份和恢复的时间成本都将随着数据量的增大而愈发不可控。一般来讲，单一数据库实例的数据阈值在1TB之内是比较合理的。

通过分库和分表进行数据拆分使各个表的数据量保持在阈值以下，以及对流量进行疏导来应对高访问量，是应对高并发和海量数据系统的有效手段。数据分片又分为垂直分片和水平分片两种。为了缓解读写压力，也可以采用读写分离的方式拆分数据库。

按照业务进行拆分的数据分片方式称为垂直分片，又称为纵向拆分，它的核心理念是专库专用。在拆分之前，一个数据库由多个数据表构成，每个表对应着不同的业务。而拆分之后，需要按照业务将表进行归类，分布到不同的数据库中，从而将压力分担至不同的数据库。

### 第10章 分布式数据库中间件生态圈ShardingSphere

ShardingSphere 的初衷是，充分合理地在分布式的场景下利用关系型数据库的计算能力和存储能力，而非实现一个全新的数据库。

### 10.2 核心功能

数据分片是提升互联网应用性能的有效手段，但它也是一把双刃剑，在性能提升的同时，各种复杂度也随之而来。正因有数据分片，才会衍生出分布式事务和数据库治理等需求

流程包括SQL解析、SQL路由、SQL改写、SQL执行、结果归并，

### 10.5 Database Mesh

分片是一个复杂的过程，如果希望对应用透明，业界常见的做法是针对SQL进行解析，并将其精准路由至相应的数据库中执行，最终将执行结果进行归并，以保证数据逻辑在分片的情况下仍然正确。数据分片的核心流程是SQL解析–>SQL路由–>SQL改写–>SQL执行–>结果归并。为